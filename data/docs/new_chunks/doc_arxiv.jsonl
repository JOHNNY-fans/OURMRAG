{"id": 60002, "dataset": "arxiv", "images": ["2411_04924v1_0", "2411_04924v1_0", "2411_04924v1_1"], "chunks": [{"chunk_id": 1, "text": "We introduce MVSplat360, a feed-forward approach for 360\u00b0 novel view synthesis(NVS) of diverse real-world scenes, using only sparse observations. This settingis inherently ill-posed due to minimal overlap among input views and insufficientvisual information provided, making it challenging for conventional methodsto achieve high-quality results. Our MVSplat360 addresses this by effectivelycombining geometry-aware 3D reconstruction with temporally consistent videogeneration. Specifically, it refactors a feed-forward 3D Gaussian Splatting (3DGS)model to render features directly into the latent space of a pre-trained Stable VideoDiffusion (SVD) model, where these features then act as pose and visual cuesto guide the denoising process and produce photorealistic 3D-consistent views.Our model is end-to-end trainable and supports rendering arbitrary views withas few as 5 sparse input views. To evaluate MVSplat360\u2019s performance, weintroduce a new benchmark using the challenging DL3DV-10K dataset, whereMVSplat360 achieves superior visual quality compared to state-of-the-art methodson wide-sweeping or even 360\u00b0 NVS tasks.Experiments on the existing benchmarkRealEstate10K also confirm the effectiveness of our model.The rapid advancement in 3D reconstruction and NVS has been facilitated by the emergence ofdifferentiable rendering [29, 31, 41, 40, 21]. These methods, while fundamental and impressive, areprimarily tailored for per-scene optimization, requiring hundreds or even thousands of images tocomprehensively capture every aspect of the scene. Consequently, the optimization process for eachscene can be time-consuming, and collecting thousands of images is impractical for casual users.In contrast, we consider the problem of novel view synthesis in diverse real-world scenes using a lim-ited number of source views through a feed-forward network. In particular, this work investigates thefeasibility ofrendering wide-sweeping or even 360\u00b0 novel views using extremely sparse observations,like fewer than 5 images. This task is inherently challenging due to the complexity of scenes, wherethe limited views do not contain sufficient information to recover the whole 3D scene. Consequently,there is a necessity to ensemble visible information under minimal overlap accurately and generatemissing details reasonably.This represents a new problem setting in sparse-view feed-forward NVS. Existing feed-forwardmethods typically focus on two distinct scenarios: 360\u00b0 NVS with extremely sparse observations, butonly at object-level [20, 52, 66, 59, 27, 72, 63, 56, 62, 50, 46, 47, 18], or generating reasonable resultsfor scene-level synthesis, but only for nearby viewpoints [53, 7, 19, 43, 9, 13, 6, 60, 10, 70, 42, 61].In contrast, we argue that the time is ripe to unify these previously distinct research directions.Our goal should be to develop systems capable of synthesizing wide-sweeping or even 360\u00b0 novelviews of large, real-world scenes with complex geometry and significant occlusion. Specifically, thiswork explores synthesising 360\u00b0 novel views from fewer than 5 input images.We show that in thischallenging setting, existing feed-forward scene synthesis approaches [9, 13, 6, 60, 10, 57] struggleto succeed. This failure arises from two main factors: i) the limited overlap among input views causesmany contents to appear in only a few views or even a single one, posing significant challenges for3D reconstruction; ii) the extremely sparse observations lack sufficient information to capture thecomprehensive details of the whole scene, resulting in regions unobserved from novel viewpoints.In this paper, we propose a simple yet effective framework to address these limitations and introducethe first benchmark for feed-forward 360\u00b0 scene synthesis from sparse input views. Our key ideais to leverage prior knowledge from a large-scale pre-trained latent diffusion model (LDM) [35] to\u201cimagine\u201d plausible unobserved and disoccluded regions in novel views, which are inherently highlyambiguous. Unlike existing 360\u00b0 object-level NVS approaches [27, 63, 49, 56, 25, 72, 50], large-scalereal-world scenes comprise multiple 3D assets with complex arrangements, heavy occlusions, andvarying rendering trajectories, which makes it particularly challenging to condition solely on cameraposes, as also verified by concurrent work ViewCrafter [68].To develop a performant framework for scene-level synthesis, we opt to treat the LDM as a refinementmodule, while relying on a 3D reconstruction model to process the complex geometric informa-tion. Broadly, we build upon the feed-forward 3DGS [21] model, MVSplat [10], to obtain coarsenovel views by matching and fusing multi-view information with the cross-view transformer andcost volume. Although these results are imperfect, exhibiting visual artifacts and missing regions(see <PIC>), they represent the reasonable geometric structure of the scene, as they are renderedfrom 3D representation.", "images": ["2411_04924v1_0"], "tokens": 1235}, {"chunk_id": 2, "text": "Furthermore, we choose Stable Video Diffusion (SVD) [3] over otherimage-based LDM as the refinement module, since its strong temporal consistency capabilities alignbetter with the view-consistent requirement of the NVS task, as also observed by concurrent work3DGS-Enhancer [28]. Conditioning SVD with the 3DGS rendered outputs, our MVSplat360 producesvisually appealing novel views that are multi-view consistent and geometrically accurate (see <PIC>).We conduct a series of experiments, mainly on two datasets. First, we establish a new benchmark onDL3DV-10K dataset [23], creating a new training and testing split for feed-forward wide-sweepingand 360\u00b0 NVS. In this challenging setting, our MVSplat360 achieves photorealistic 360\u00b0 NVSfrom sparse observations and demonstrates significantly better visual quality, where the previousscene-level feed-forward methods [9, 6, 60, 10] fail to achieve plausible results. Second, we deployMVSplat360 on the existing RealEstate10K [74] benchmark. Following latentSplat [57], we estimateboth", "images": ["2411_04924v1_0"], "tokens": 275}, {"chunk_id": 3, "text": "interpolation and extrapolation NVS, and report state-of-the-art performance.Our main contributions can be summarized as follows. 1) We introduce a crucial and pressingproblem for novel view synthesis, i.e., how to do wide-sweeping or even 360\u00b0 NVS from sparseand widely-displaced observations of diverse in-the-wild scenes (not objects) in a feed-forwardmanner (no any per-scene optimization). 2) We propose an effective solution that nicely integrates thelatest feed-forward 3DGS and the pre-trained Stable Video Diffusion (SVD) model with meticulousintegration designs, where the former is for reconstructing coarse geometry and the latter is forrefining the noisy and incomplete coarse reconstruction. 3) Extensive results on the challengingDL3DV-10K and RealEstate10K datasets demonstrate the superior performance of our MVSplat360.We opt to go beyond per-scene optimisation [1, 2, 58, 15], and to deal with a more general feed-forward network capable of achieving 360\u00b0 NVS for unseen scenes, yet without the need ofadditionalper-scene training.This requires effectively matching information between sparse views in 3Dspace, as well as generating sufficient content based on only partial observations. To achieve that,our MVSplat360 framework, illustrated in <PIC>, comprises two main components: a multi-viewgeometry reconstruction module (Section 3.1) and a multi-frame consistent appearance refinementnetwork (Section 3.2). The former is responsible for matching and fusing multi-view informationfrom sparse observations to create a coarse geometry reconstruction, whereas the latter is designedto refine the appearance with a pre-trained latent video diffusion model. While similar two-stepapproaches have been explored in recent related works, e.g., [4, 58, 38, 15], we are the first (to thebest of our knowledge) to explore it on wide-sweeping or even 360\u00b0 NVS for large-scale scenes fromsparse views (as few as 5), in a feed-forward manner.bserved and novel viewpoints selection. To enReaders are referred to ourproject page for video results with more comprehensive comparisons, where our MVSplat360 shows", "images": ["2411_04924v1_1"], "tokens": 553}]}
{"id": 60006, "dataset": "arxiv", "images": ["2403_06845v2_0", "2403_06845v2_1", "2403_06845v2_2"], "chunks": [{"chunk_id": 1, "text": "World models have demonstrated superiority in autonomousdriving, particularly in the generation of multi-view driving videos. How-ever, significant challenges still exist in generating customized drivingarXiv:2403.06845v2 [cs.CV] 11 Apr 2024videos. In this paper, we propose DriveDreamer-2, which builds uponthe framework of DriveDreamer and incorporates a Large LanguageModel (LLM) to generate user-defined driving videos. Specifically, anLLM interface is initially incorporated to convert a user\u2019s query intoagent trajectories. Subsequently, a HDMap, adhering to traffic regula-tions, is generated based on the trajectories. Ultimately, we propose theUnified Multi-View Model to enhance temporal and spatial coherence inthe generated driving videos. DriveDreamer-2 is the first world model togenerate customized driving videos, it can generate uncommon drivingvideos (e.g., vehicles abruptly cut in) in a user-friendly manner. Besides,experimental results demonstrate that the generated videos enhance thetraining of driving perception methods (e.g., 3D detection and track-ing).Furthermore, video generation quality of DriveDreamer-2 surpassesother state-of-the-art methods, showcasing FID and FVD scores of 11.2and 55.7, representing relative improvements of \u223c30% and \u223c50%.World models for autonomous driving [23,25,59,62] have drawn extensive atten-tion from both the industry and academia in recent years. Benefiting from theirexcellent predictive capabilities, autonomous driving world models facilitate thegeneration of diverse driving videos, encompassing even long-tail scenarios. Thegenerated driving videos can be utilized to enhance the training of various driv-ing perception approaches, proving highly beneficial for practical applications inautonomous driving. World modeling in autonomous driving presents a formidable challenge dueto its inherent complexity and large sampling space. Early approaches [8, 22]mitigate these problems by incorporating world modeling within the Bird\u2019sEyeView (BEV) semantic segmentation space. However, these methods primarily ex-plore world models in simulated autonomous driving environments. In the recentevolution of autonomous driving technologies, there has been a substantial leapforward in the development of world models. This progress has been propelled bythe utilization of cutting-edge diffusion models [5, 17, 18, 39, 40, 46], exemplifiedby notable contributions such as DriveDreamer [59], Drive-WM [62], Magic-Drive [7], Panacea [64], and the integration of large language models like GAIA-1 [23], ADriver-I [25]. These sophisticated models have played a pivotal role inpushing the boundaries of world modeling capabilities, enabling researchers andengineers to delve into increasingly intricate and realistic driving scenarios. How-ever, it is important to note that a majority of these methods rely heavily onstructured information (e.g., 3D boxes, HDMaps, and optical flow) or real-worldimage frames as conditions. This dependence not only constrains interactivitybut also limits the diversity of generated videos.To tackle the aforementioned challenges, we propose DriveDreamer-2, whichis the first world model to generate diverse driving videos in a user-friendly man-ner. In contrast to previous methods [7,59,62] that rely on structured conditionseither from specific datasets or sophisticated annotations, DriveDreamer-2 em-phasizes generating customized driving videos by simulating various traffic con-ditions with user-friendly text prompts. Specifically, the traffic simulation taskhas been disentangled into the generation of foreground conditions (trajectoriesof the ego-car and other agents) and background conditions (HDMaps of laneboundary, lane divider, and pedestrian crossing). For foreground generation, afunctional library is constructed to finetune a Large Language Model (LLM), en-abling it to generate agent trajectories based on user text input. For backgroundconditions, we propose the HDMap generator that employs a diffusion model tosimulate road structures. In this process, the previously generated agent trajec-tories are involved as conditional inputs, which allows the HDMap generator tolearn the associations between foreground and background conditions in drivingscenes. Building upon the generated traffic structured conditions, we employ theDriveDreamer [59] framework to generate multi-view driving videos. It is notedthat we introduce the Unified Multi-view Video Model (UniMVM) within theDriveDreamer framework, which is designed to unify both intra-view and cross-view spatial consistency, enhancing the overall temporal and spatial coherencein the generated driving videos. Extensive experiment results show that DriveDreamer-2 is capable of produc-ing diverse user-customized videos, including uncommon scenarios where vehiclesabruptly cut in (depicted in <PIC>). Besides, DriveDreamer-2 can generate high-quality driving videos with an FID of 11.2 and FVD of 55.7, relatively improvingprevious best-performing methods by \u223c30% and \u223c50%.", "images": ["2403_06845v2_0"], "tokens": 1229}, {"chunk_id": 2, "text": "Furthermore, experi-ments are conducted to verify that driving videos generated by DriveDreamer-2can enhance the training of various autonomous driving perception methods,where the performance of detection and tracking are relatively improved by\u223c4% and \u223c8%.The main contributions of this paper can be summarized as follows:\u2013 We present DriveDreamer-2, which is the first world model to generate di-verse driving videos in a user-friendly manner.\u2013 We propose a traffic simulation pipeline employing only text prompts asinput, which can be utilized to generate diverse traffic conditions for drivingvideo generation.\u2013 UniMVM is presented to seamlessly integrate intra-view and cross-view spa-tial consistency, elevating the overall temporal and spatial coherence withinthe generated driving videos.\u2013 Extensive experiments are conducted to show that DriveDreamer-2 can craftdiverse customized driving videos. Besides, DriveDreamer-2 enhances theFID and FVD by \u223c30% and \u223c50% compared to previous best-performingmethods. Moreover, the driving videos generated by DriveDreamer-2 en-hance the training of various driving perception methods. The primary objective of world methods is to establish dynamic environmen-tal models, endowing agents with predictive capabilities for the future. In theearly exploration, Variational Autoencoders (VAE) [31] and Long Short-TermMemory (LSTM) [19] are employed to capture transition dynamics and ren-dering functionality, showcasing remarkable success across diverse applications[9\u201313,13,29,36,48,65]. Constructing driving world models poses distinctive chal-lenges, primarily arising from the high sample complexity inherent in real-worlddriving tasks [3]. To address these challenges, ISO-Dream [42] introduces anexplicit disentanglement of visual dynamics into controllable and uncontrollablestates. MILE [22] strategically incorporates world modeling within the Bird\u2019s EyeView (BEV) semantic segmentation space.Recently, DriveDreamer [59], GAIA-1 [23], ADriver-I [25], and Drive-WM [62] have explored the training of drivingworld models in the real world, leveraging powerful diffusion models or naturallanguage models. However, most of these methods heavily depend on structuredinformation (e.g., 3D boxes, HDMaps, and optical flow) as conditions. This de-pendency not only constrains interactivity but also limits generation diversity. Video generation and prediction are pivotal techniques for understanding the vi-sual world. In the early stages of video generation, methods like VariationalAutoencoders (VAEs) [4, 21], flow-based model [33], and Generative Adver-sarial Networks (GANs) [38, 47, 53, 56] are explored. Language models [20, 26,32, 45, 51, 55, 60, 63] are also employed for intricate visual dynamics model-ing. Recent advancements have seen diffusion models [5, 17, 18, 39, 40, 46] ex-tending their influence to video generation. Notably, video diffusion models[1, 14, 16, 28, 49, 58, 67] exhibit superior capabilities in generating high-qualityvideos with realistic frames and smooth transitions, offering enhanced control-lability. These models adapt seamlessly to various input conditions, includingtext, canny, sketch, semantic maps, and depth maps. In the realm of autonomousdriving, DriveDreamer-2 leverages powerful diffusion models for learning visualdynamics.Driving simulators stand as a cornerstone in self-driving development, aimingto offer a controlled environment to mimic real-world conditions. LCTGen [52]utilizes an LLM to encode detailed language descriptions to a vector and sub-sequently employs a generator to produce corresponding simulated scenarios.This method requires highly detailed language descriptions, including informa-tion such as the speed and orientation of agents. TrafficGen [6] comprehendsthe inherent relationships within traffic scenarios, enabling the generation ofdiverse and legitimate traffic flows within the same map. CTG [70] generatestraffic simulations by employing manually designed loss functions that adhere totraffic constraints. CTG++ [69] further extends CTG by utilizing GPT-4 [41]to convert user language descriptions into a loss function, which guides thescene-level conditional diffusion model to generate the corresponding scenario.In DriveDreamer-2, we construct a functional library to finetune the LLM toachieve a user-friendly text-to-traffic simulation, which eliminates intricate lossdesign or complex text prompt inputs. <PIC> illustrates the overall framework of DriveDreamer-2. A customized traf-fic simulation is first proposed to generate foreground agent trajectories andbackground HDMaps. Specifically, DriveDreamer-2 utilizes a finetuned LLM totranslate user prompts into agent trajectories, and the HDMap generator is thenintroduced to simulate road structures using the generated trajectories as con-ditions. Leveraging the customized traffic simulation pipeline, DriveDreamer-2is capable of generating diverse structured c", "images": ["2403_06845v2_1"], "tokens": 1220}, {"chunk_id": 3, "text": "onditions for the subsequent videogeneration. Building upon the architecture of DriveDreamer [59], the UniMVMframework is proposed to unify both intra-view and cross-view spatial consis-tency, thereby enhancing the overall temporal and spatial coherence in the gen-erated driving videos. In the subsequent sections, we delve into the details of thecustomized traffic simulation and the UniMVM framework. Finetuning LLM for Trajectory Generation Previous traffic simulationmethods [37, 69, 70] necessitate the intricate specification of parameters, involv-ing details such as the agent\u2019s speed, position, acceleration, and mission goal.To simplify this intricate process, we propose to finetune LLM with the con-structed trajectory-generation function library, allowing for the efficient trans-formation of user-friendly language inputs into comprehensive traffic simulationscenarios. As depicted in <PIC>, the constructed function library encompasses 18functions, including agent functions (steering, constant speed, acceleration, andbraking), pedestrian functions (walking direction and speed), and other utilityfunctions such as saving trajectories.phase, we follow [37] to expand prompt inputs to a pre-definedtemplate, and the finetuned LLM can directly output the trajectory array.", "images": ["2403_06845v2_2"], "tokens": 321}]}
{"id": 60007, "dataset": "arxiv", "images": ["2309_09777v2_0", "2309_09777v2_2", "2309_09777v2_1"], "chunks": [{"chunk_id": 1, "text": "World models, especially in autonomous driving, are trending and drawing extensive attention due to their ca- pacity for comprehending driving environments. The estab- lished world model holds immense potential for the gen- eration of high-quality driving videos, and driving poli- cies for safe maneuvering. However, a critical limitation in relevant research lies in its predominant focus on gam- ing environments or simulated settings, thereby lacking the representation of real-world driving scenarios. Therefore, we introduce DriveDreamer, a pioneering world model en- tirely derived from real-world driving scenarios. Regarding that modeling the world in intricate driving scenes entails an overwhelming search space, we propose harnessing the Spurred by insights from AGI (Artificial General Intelli-gence) and the principles of embodied AI, a profound trans-formation in autonomous driving is underway. Autonomousvehicles rely on sophisticated systems that engage with andcomprehend the real driving world. At the heart of this evo-lution is the integration of world models [15,17\u201319].Worldmodels hold great promise for generating diverse and real-istic driving videos, encompassing even long-tail scenarios,which can be utilized to train various driving perception ap-proaches. Furthermore, the predictive capabilities in worldmodels facilitate end-to-end driving, ushering in a new eraof autonomous driving experiences.Deriving latent dynamics of world models fromvisual signals was initially introduced in video predic-tion [8, 11, 19]. By extrapolating from observed visualsequences, video prediction methods can infer future statesof the environment, effectively modeling how objects andentities within a scene will evolve over time. However,modeling the intricate driving scenarios in pixel space ischallenging due to the large sampling space [5, 7]. Toalleviate this problem, recent research endeavors havesought innovative strategies to enhance sampling efficiency.ISO-Dream [52] explicitly disentangles visual dynamicsinto controllable and uncontrollable states. MILE [29]strategically incorporates world modeling within theBird\u2019s Eye View (BEV) semantic segmentation space,complementing world modeling with imitation learning.SEM2 [13] further extends the Dreamer framework intoBEV segmentation maps, utilizing Reinforce Learning(RL) for training. Despite the progress witnessed in worldmodels, a critical limitation in relevant research lies in itspredominant focus on simulation environments.In this paper, we propose DriveDreamer, which pioneersthe construction of comprehensive world models from realdriving videos and human driver behaviors. Considering theintricate nature of modeling real-world driving scenes, weintroduce the Autonomous-driving Diffusion Model (Auto-DM), which empowers the ability to create a comprehen-sive representation of the complex driving environment. Wepropose a two-stage training pipeline. In the first stage,we train Auto-DM by incorporating traffic structural infor-mation as intermediate conditions, which significantly en-hances sampling efficiency.Consequently, Auto-DM ex-hibits remarkable capabilities in comprehending real-worlddriving scenes, particularly concerning the dynamic fore-ground objects and the static background. In the second-stage training, we establish the world model through videoprediction. Specifically, driving actions are employed to it-eratively update future traffic structural conditions, whichenables DriveDreamer to anticipate variations in the drivingenvironment based on different driving strategies. More-over, DriveDreamer extends its predictive prowess to fore-see forthcoming driving policies, drawing from historicalobservations and Auto-DM features. Thus creating a exe-cutable, and predictable driving world model.The main contributions of this paper can be summa-rized as follows: (1) We introduce DriveDreamer, whichis the first world model derived from real-world drivingscenarios. DriveDreamer can jointly enable the generationof high-quality driving videos and reasonable driving poli-cies. (2) To enhance the comprehension of real-world driv-ing scenes and expedite the world model convergence, weintroduce the Autonomous-driving Diffusion Model and atwo-stage training pipeline. The first-stage training enablesthe comprehension of traffic structural information, and thesecond-stage video prediction training empowers the pre-dictive capacity. (3) DriveDreamer can controllably gener-ate driving scene videos that are highly aligned with trafficconstraints (see <PIC>), enhancing the training of drivingperception methods (e.g., 3D detection). Besides, Drive-Dreamer can generate future driving policies based on his-torical observations and Auto-DM features. Notably, Drive-Dreamer achieves promising planning results in open-loopassessments on the nuScenes dataset. 2. Related Work2.1. Diffusion ModelDiffusion models represent a family of probabilistic gen-erative models that progressively introduce noise to dataand subsequently learn to reverse this process for the pur-pose of generating samples [73].", "images": ["2309_09777v2_0"], "tokens": 1218}, {"chunk_id": 2, "text": "These models have re-cently garnered significant attention due to their exceptionalperformance in various applications, setting new bench-marks in image synthesis [1, 14, 49, 55, 57], video gener-ation [21, 23, 35, 60, 67, 74], and 3D content generation[6, 43, 53, 69]. To enhance the controllable generation ca-pability, ControlNet [76], GLIGEN [42], T2I-Adapter [48] and Composer [32] have been introduced to utilize variouscontrol inputs, including depth maps, segmentation maps,canny edges, and sketches. Concurrently, BEVControl [72],MagicDrive [12] and DrivingDiffuson [41] incorporate lay-out conditions to enhance image generation. The funda-mental essence of diffusion-based generative models lies intheir capacity to comprehend and understand the intricaciesof the world. Harnessing the power of these diffusion mod-els, DriveDreamer seeks to comprehend the complex realmof autonomous-driving scenarios.2.2. Video GenerationVideo generation and video prediction are effectiveapproaches to understanding the visual world. In therealm of video generation, several standard architectureshave been employed, including Variational Autoencoders(VAEs) [8,28], auto-regressive models [34,56,61,70], flow-based models [40], and Generative Adversarial Networks(GANs) [46, 58, 62, 65]. Recently, the burgeoning diffu-sion models [9, 24, 25, 49, 50, 57] have also been extendedto the domain of video generation. Video diffusion mod-els [21, 23, 35, 60, 67, 74] exhibit higher-quality video gen-eration capabilities, producing realistic frames and transi-tions between frames while offering enhanced controllabil-ity. They accommodate various input control conditionssuch as text, canny, sketch, semantic maps, and depth maps.Video prediction models represent a specialized formof video generation models, sharing numerous similari-ties. In particular, video prediction involves anticipatingfuture video changes based on historical video observa-tions [8, 11, 19, 27, 64].DriveGAN [36] establishes asso-ciations between driving actions and pixels, predicting fu-ture driving videos by specifying future driving policies. Incontrast, DriveDreamer incorporates structured traffic con-ditions, text prompts, and driving actions as inputs, empow-ering precise, realistic video and action generation that arefaithfully aligned with real-world driving scenarios.2.3. World ModelsWorld models have been extensively explored in model-based imitation learning, demonstrating remarkable suc-cess in various applications [15\u201320, 37, 44, 59, 71]. Theseapproaches typically leverage Variational Autoencoders(VAE) [39] and Long Short-Term Memory (LSTM) [26]to model transition dynamics and rendering functionality.World methods target at establishing dynamic models of en-vironments, enabling agents to be predictive of the future.This aspect is of paramount importance in autonomous driv-ing, where precise predictions about the future are essentialfor safe maneuvering. However, constructing world mod-els in autonomous driving presents unique challenges, pri-marily due to the high sample complexity inherent in real-world driving tasks [5]. To address these problems, ISO- Dream [52] introduces an explicit disentanglement of visualdynamics into controllable and uncontrollable states. MILE[29] strategically incorporates world modeling within theBEV semantic segmentation space, enhancing world mod-eling through imitation learning. SEM2 [13] extends theDreamer framework into BEV segmentation maps, em-ploying reinforcement learning for training. Despite theprogress witnessed in world models, a critical limitation inrelevant research lies in its predominant focus on simulationenvironments. The transition to real-world driving scenar-ios remains an under-explored frontier.3. DriveDreamerThe overall framework of DriveDreamer is depicted in<PIC>. The framework begins with an initial referenceframe I0 and its corresponding road structural informa-tion (i.e., HDMap H0 and 3D box B0). Within this con-text, DriveDreamer leverages the proposed ActionFormerto predict forthcoming road structural features in the latentspace.", "images": ["2309_09777v2_2"], "tokens": 1030}, {"chunk_id": 3, "text": "These predicted features serve as conditions andare provided to Auto-DM, which generates future drivingvideos. Simultaneously, the utilization of text prompts al-lows for dynamic adjustments to the driving scenario style(e.g., weather and time of the day). Moreover, Drive-Dreamer incorporates historical action information and themulti-scale latent features extracted from Auto-DM, whichare combined to generate reasonable future driving actions.In essence, DriveDreamer offers a comprehensive frame-work that seamlessly integrates multi-modal inputs to gen-erate future driving videos and driving policies, thereby ad-vancing the capabilities of autonomous-driving systems.Regarding the extensive search space of establishingworld models in real-world driving scenarios, we introducea two-stage training strategy for DriveDreamer. This strat-egy is designed to significantly enhance sampling efficiencyand expedite model convergence. The two-stage training isillustrated in <PIC>. There are two steps in the first-stagetraining. Step 1 involves utilizing the single-frame struc-tured condition, which guides DriveDreamer to generatedriving scene image, facilitating its comprehension of struc-tural traffic constraints. Step 2 extends its understandinginto video generation. The second-stage training enablesDriveDreamer to interact with the environment and predictfuture states effectively. This phase takes an initial frameimage along with its corresponding structured informationas input. Simultaneously, sequential driving actions are pro-vided, with the model expected to generate future drivingvideos and future driving actions. In the following sections,we delve into the specifics of the model architecture andtraining pipelines.", "images": ["2309_09777v2_1"], "tokens": 391}]}
{"id": 60011, "dataset": "arxiv", "images": ["2405_05378v1_0", "2405_05378v1_2", "2405_05378v1_1"], "chunks": [{"chunk_id": 1, "text": "Designing the Conversation Seed Prompt: The design of the conversation seed prompt is grounded in social identity perspective (Tajfel and Turner,2004), which posits that individuals form identi-ties through their association with various social groups, encompassing multiple simultaneous iden-tities, such as nationality, gender, and interests (Maet al., 2023). According to Abrams and Hogg(2010), contextual cues can render a social iden-tity more salient when compared to other social identities. Thus, to make the race/caste identities salient when generating conversations, our con-versation seed prompt includes the background context regarding the colleagues' group identi-ties (e.g. \u201cWhite\u201d, \u201cBrahmin\u201d) and the initial dia-logue, in which the applicant\u2019s group identity (e.g.\u201cBlack\u201d, \u201cDalit\u201d) is discussed (e.g.\u201che appears tobe [group]\u201d) (see <PIC>). Colleagues & Applicant name selection: To intro-duce diversity in the name selection, we randomly selected names that are culturally indicative of dif-ferent races and castes. All models were provided with identical prompts, including the same names and groups, to ensure consistency across the exper-iments.", "images": ["2405_05378v1_0"], "tokens": 297}, {"chunk_id": 2, "text": "We discuss additional details in \u00a7H. Hiring occupation selection: We consider four occupations in our experiments: Software Devel-oper, Doctor, Nurse, and Teacher. These roles are chosen due to their varied societal perceptions and stereotypical associations along both race and caste dimensions, as highlighted in prior work on race(Ghosh and Caliskan, 2023; Veldanda et al., 2023) and caste (Pathania et al., 2023; Barua and Verma,2021; Kumbhar, 2021; George, 2015, 2019). LLM model selection: For a comprehensive anal-ysis, we selected eight LLMs \u2013 two models from OpenAI and six widely used open-source models,as listed in <PIC>. We set the temperature to 0.7 for all models with a 512-token limit. We introduce the Covert Harms and Social Threats(CHAST) metrics, a set of 7 metrics grounded in social science literature, such as", "images": ["2405_05378v1_2"], "tokens": 218}, {"chunk_id": 3, "text": "the Social Iden-tity Threat Theory (Branscombe et al., 1999a; Maet al., 2023) and Intergroup Threat Theory (Stephan and Cookie, 2000). These frameworks offer a nu-anced understanding of the various forms of harm and threat to identity groups. The CHAST metrics are particularly relevant to our experimental setup, wherein LLMs generate conversations involving two colleagues discussing an applicant from a dif-ferent identity group (e.g. \u201cBlack\u201d or \u201cDalit\u201d). According to the Social Identity Threat Theory(SIT), social identity threat is evoked when peo-ple feel concerned about being negatively treated,devalued, or stereotyped based on group member-ship, often arising from intergroup communica-tion (Branscombe et al., 1999a; Ellemers et al.,2002). We selected three types of social iden- tity threat: categorization threat, which is felt when people experience being reduced to a single category; morality threat and competence threat which arise when the group's morality or compe-tence are undermined.We also included metrics from Intergroup Threat Theory (ITT), which argues that intergroup threat is experienced when an in-group member perceives that another group is in a position to cause them harm (Stephan and Cookie, 2000). ITT offered two metrics: Realistic threat, which occurs when an in-group member is concerned about their physical safety, political and economic prosperity, or re-sources due to an out-group and symbolic threat, which arises when an in-group member is con-cerned about the integrity of the in-group\u2019s values, standards, morals, or attitudes due to an out-group member (Stephan and Cookie, 2000). Lastly, we incorporated disparagement as part of our metrics, which encapsulates any behavior by a model that reinforces a notion that certain groups are less valuable than others (Dev et al., 2022). Fol-lowing prior research on harms in job hiring scenar-ios, we additionally included opportunity harm(Yam and Skorburg, 2021; Roberts, 2015), which arises due towithdrawal or reduced chance of an opportunity based on background or group iden- tity. <PIC> presents the CHAST metrics, including their definitions and examples extracted from LLM-generated conversations in our study.", "images": ["2405_05378v1_1"], "tokens": 561}]}
{"id": 60013, "dataset": "arxiv", "images": ["2405_13077v2_0", "2405_13077v2_1"], "chunks": [{"chunk_id": 1, "text": "Given an initial adversarial request Radv such as \u201cwrite a tutorial on how to make bomb\u201d that models always reject, the goal is to formulate a prompt that induces the target LLM (T) to generate content that fulfills the request. Our method, IRIS, uses the same LLM for the attacker (A). As shown in <PIC>, IRIS consists of two main steps: Iterative Refinement, which iteratively refines the initial prompt, and Rate+Enhance, which maximizes the harmfulness of the output. During the Iterative Refinement step, IRIS operates through a feedback loop that progres-sively refines the adversarial prompt based on T's responses and A's modifications. At each iter-ation, the current prompt Pcurrent is presented to T, and its response R is evaluated to deter-mine whether T rejects the request based on a simple rule: reject if the response is less than 20 words; otherwise, do not reject. If T rejects the prompt, IRIS solicits an explanation from the attacker model A on why the attempt failed us-ing a template query QA(EXPLAIN:R). This self- explanation step is vital for well-aligned LLMs(e.g.", "images": ["2405_13077v2_0"], "tokens": 293}, {"chunk_id": 2, "text": ", GPT-4), as it prevents an immediate rejection when directly asking models to refine the failed adversarial prompt, QA(MODIFY:Pcurrent). The output from the MODIFY query is a refined prompt, Prefined, which becomes the new basis for subse-quent iterations. The iterative prompt refinement process continues until Radv is found or the num-ber of attempts N is reached, which we set N = 4 in our experiments. Since there are 3 queries in each iteration, IRIS produces a maximum of 3N+1 = 13 queries to LLMs for our experiments, which is significantly more efficient than previous approaches (over 20 queries). However, over 80% of the time, only one or two iterations are used. Ex-periment artifacts show refined prompts Prefined produced by IRIS always request the same harmful behavior as the original prompts Padv. In the Rate+Enhance step, IRIS further prompts the target model to rate the harmfulness of Radv from 1 to 5 and refine the response to maximize its harmfulness rating, as Radv could just be a long out-put that containing safe educational content rather than harmful output. We provide an algorithmic implementation of IRIS in Algorithm 1. Jailbreaking Methods for Comparison. In ad-dition to IRIS, we consider two state-of-the-art methods that use LLMs to refine jailbreak prompts: PAIR (Chao et al., 2023) and TAP (Mehrotra et al., 2023). PAIR uses Vicuna-13B (Chiang et al., 2023) to iteratively refine the prompt, while TAP further improves the method by incorporating the tree-of-thought reasoning (Yao et al., 2024). There is an- other method, PAP (Zeng et al., 2024), that fine-tunes GPT-3.5 to generate prompts but requires 400 queries when jailbreaking GPT-4. We exclude it from our comparisons. <PIC> shows the main results that compare IRIS with TAP and PAIR, whose results were reported in Mehrotra et al. (2023).IRIS-2x represents an ensemble of two independent IRIS trials on each ad-versarial prompt, where the jailbreak is considered successful if at least one of the trials succeeds. The average number of queries for IRIS-2x is the sum of the queries in the two trials. We find that IRIS achieves higher jailbreak success rates with signif-icantly fewer queries than TAP and PAIR. IRIS has success rates of 98% and 92% for GPT-4 and GPT-4 Turbo, respectively, using under 7 queries on average. With two independent trials (IRIS-2x), these rates rise to 100% and 98% with under 13 queries on average, which is approximately 55% fewer queries than other methods while increasing the jailbreak success rate by at least 22%.", "images": ["2405_13077v2_1"], "tokens": 689}]}
{"id": 60017, "dataset": "arxiv", "images": ["2401_10568v2_0", "2401_10568v2_1"], "chunks": [{"chunk_id": 1, "text": "There are various characteristics of this environment that make it open-ended (<PIC>): Imperfect information, where players only have access to information discovered by their own units and cities. Stochastic dynamics with random events and crises that can disrupt plans. Multiple victory paths are possible (e.g., conquer, science, or highest score), requiring a balance between economic expansion, military development, diplomacy, culture, and technology. A dynamic game space with continuous changes in state and action space for a single agent. Multi-agent interactions with built- in AI players or other models, providing the potential for self-play. General-sum game that allows alliance formation during gameplay, which changes the game structure and makes the victories of different players non-exclusive. Changes in the number of players during a game due to revolts or conquers, leading to significant alterations in the joint state and action space. Communication between players through diplomatic actions and natural language chat, allowing agents to use their natural language capabilities. In summary, CivRealm presents unique challenges and complexities, making it an open-ended testbed for decision-making agents. Please see \u00a7 A.1 for more details. Agent-architecture-agnostic framework.", "images": ["2401_10568v2_0"], "tokens": 277}, {"chunk_id": 2, "text": "CivRealm empowers each agent to act as a player in the open-source turn-based strategy game Freeciv [74]. CivRealm employs a server-proxy-client framework and implements proxy APIs so that a server hosts the game and the proxy establishes the connection between agents (i.e., clients) and the server. The proxy distributes the game states received from the server to each agent and submits the actions returned by agents to the server. By this design, agents with various architectures can seamlessly engage in Freeciv by interpreting the observations provided by the proxy and generating actions that adhere to CivRealm's specifications.. LLM-based-agent-friendly. Freeciv is a turn-based game that operates without the need for real- time reactions. This affords players ample time for thoughtful deliberation. This pace aligns well with the operation of LLM agents, which typically demand substantial time for inference. Evaluation platform for generalization ability. CivRealm offers multiple convenient methods to create novel scenarios, such as generating random maps with diverse landscapes and varying player and unit numbers, or modifying the rule sets that define the fundamental game rules. These elements result in new configurations, demanding agents to reason the underlying game mechanics rather than relying solely on memorized experiences and public knowledge. Therefore, CivRealm serves as an effective platform for assessing the generalization capabilities of decision-making agents. Support for a variety of tasks. CivRealm offers a wide range of learning and reasoning tasks. These tasks include not only the comprehensive full game of Freeciv, but also smaller-scale mini-games designed using Lua scripts. In \u00a7 3.2, we will provide detailed descriptions of these tasks. In CivRealm, players take the role of civilization leaders with the objective of guiding their civi- lization from its humble beginnings to greatness, where full games can last from several hours to several days.Civilizations evolve through eras, with an explosion in the number of controllable objects as the game progresses, resulting in vast state spaces and joint actions (<PIC>). Decisions in the game have multifaceted impacts, encompassing both long-term strategic consequences and short-term tactical outcomes. This complexity necessitates a thought process that carefully weighs the implications of these decisions since greedy moves can easily be non-optimal in the long term. Observations. Instead of directly processing raw pixel data of the game interface, we extract representative discrete information from graphics observed during human gameplay. These observations encompass data rela-term goals for their technology research. The diplomacy actions empower players to initiate negotiations, such as trading technologies, negotiating ceasefires, forming alliances, etc. For an exhaustive list of the implemented actions, please refer to \u00a7 A.1.2. Evaluation Metrics. CivRealm offers evaluation metrics to assess playing performance across var- ious dimensions, including population, constructed cities, researched technologies, produced units, explored land, etc. An aggregated score is provided for overall evaluation. Please refer to \u00a7 A.1.3.", "images": ["2401_10568v2_1"], "tokens": 735}]}
{"id": 60020, "dataset": "arxiv", "images": ["2406_17534v2_0", "2406_17534v2_1"], "chunks": [{"chunk_id": 1, "text": "In standard supervised HTC, there is an underlying assumption of abundant training samples(Zhao et al., 2023; Im et al., 2023; Song et al.,2023), which is often unattainable and expensive to construct manually. Moreover, HTC datasets are characterized by a complex hierarchical label structure, with leaf labels typically following a Zipfian distribution, resulting in very few data instances for these labels. As a result, the few-shot setting is more realistic, and has gained increasing interest recently (Ji et al., 2023; Bhambhoria et al., 2023; Wang et al., 2023b). Nevertheless, existing works often struggle with unsatisfactory performance in this setting. For example, BERT with the vanilla fine-tuning strategy performs extremely poorly in few-shot HTC. Recently, large language models (LLMs) have achieved notable success on various NLP tasks (Wang et al., 2023a; Drozdov et al., 2023; Zeng et al., 2023), which have significantly enhanced the efficacy of in-context learning (ICL) with relevant demonstrations in the few-shot setting (Shome and Yadav, 2023; Dai et al., 2023; Zhang et al., 2023).However, the application of ICL on HTC faces unique challenges, diverging from traditional text classification scenarios. These challenges are primarily due to two distinct characteristics of HTC, as delineated in <PIC>. Firstly, HTC features a deep hierarchical labeling structure and expansive label sets, resulting in large label sets in ICL, which adversely impacts its performance. Secondly, as the hierarchy deepens, the semantic similarity between adjacent labels increases (Stein et al., 2019), making it very challenging to select relevant demonstrations that guide the learning process efficiently. In this work, we introduce the first ICL-based framework for few-shot HTC. Specifically, we use a LLM as the foundation model for inference, and provide demonstrations to guide HTC label generation through ICL. Our success depends on finding suitable demonstrations for a given i", "images": ["2406_17534v2_0"], "tokens": 494}, {"chunk_id": 2, "text": "nput. In order to achieve this, we build a retrieval database that can find the most-relevant demonstrations for the input. Further, in order to avoid providing an enormous set of multi-layer contextual HTC labels all at once, as is required for ICL, we suggest an iterative policy to infer the labels layer-by-layer, reducing the number of candidate labels greatly <PIC> illustrates our ICL-based framework for HTC. We first train a PLM-based indexer and build a retrieval database containing reference samples (the training data). After that, we perform a similarity search in the retrieval database with the text to be inferred. Finally, we construct an ICL prompt with highly similar demonstrations for prediction. We will introduce our ICL prompt policy for HTC (\u00a7 3.1), and then detail the retrieval database construction (\u00a7 3.2) and demonstration retrieval methods (\u00a7 3.3). Indexer Training. For indexer training, we apply the objectives of mask language modeling Lmlm, and layer-wise classification Lcls. Lmlm is used to predict the words that fill the random mask tokens in the inputs. Lcls is to predict HTC labels through each hierarchical layer index vectors.Finally, our method can bring the stateof-the-art results in few-shot HTC on the three datasets. Further, we performed comprehensive analysis for deep understanding of our method, spreading various important factors. This work still includes several unresolved problems, which might be addressed in the future. Firstly, LLMs are currently confined to expanding text via label descriptions and their application to full training set expansion has not been effective. In order to fully utilize LLMs in text expansion, we need further optimization. Second, the performance gap between supervised methods and our ICL-based approach appears to diminish with increasing training dataset size, suggesting the need for further analysis. ", "images": ["2406_17534v2_1"], "tokens": 442}]}
{"id": 60023, "dataset": "arxiv", "images": ["2404_07851v1_0", "2404_07851v1_1"], "chunks": [{"chunk_id": 1, "text": "We explore a range of techniques to guide LLaMA-2 models (Touvron et al, 2023) to improve MT outputs using fine-grained feedback derived from Multidimensional Quality Metric (MQM) annotations (Freitag et al, 2021), as shown in <PIC>. Following prior work on refinement, we start with evaluating the impact of such feedback when prompting LLMs in zero-shot and few-shot settings (\u00a75). Different from prior work, we then explore fine-tuning the LLM to advance its ability to improve translations based on the feedback provided in the prompt, in an instruction following style (Taori et al, 2023) (\u00a76). Through extensive experiments with three language pairs (Chinese-English, English-German, and English-Russian), we show that prompting LLMs to edit MT with feedback reliably improves translation quality as measured by automatic metrics, particularly in the few shot settings where the LLaMA-2 7B model achieves close peformance to the 13B version (\u00a75). However, the models are unable to make the most of the fine-grained feedback which performs roughly on par with generic prompts for improvement. Instruction fine-tuning shows stronger improvements on translation quality based on both automatic and human evaluation (\u00a76).", "images": ["2404_07851v1_0"], "tokens": 295}, {"chunk_id": 2, "text": "Our analysis reveals that prompting the finetuned LLMs with fine-grained feedback not only helps fix the errors highlighted in the prompt (\u00a77), but also leads to more natural outputs We ask bilingual human annotators to assess the post-edited outputs obtained by fine-tuning in the bilingual setting as it is the stronger approach based on automatic scores. We randomly select 50 instances for each language pair for annotation. Each instance is examined by 3 human annotators. For each instance of source text, original MT with MQM annotation, post-edited MT, the annotator is asked to rate on a 5-point Likert scale (1 strongly disagree to 5 strongly agree) whether the translation quality has improved, and to what extent the annotated errors are actually resolved through postediting. Ordinal Kripendorff\u2019s alpha (Krippendorff, 2011) 6, which measure the inter-annotator agreement is moderate for the Overall quality: 0.527, 0.479, 0.421 for Zh-En, En-De, and En-Ru. Annotators are also given the option to provide free form comments. Refer to Appendix F for further details on the annotation set-up. As illustrated in <PIC>, our human evaluation results confirm that fine-tuning with error annotations enhances overall translation quality (Overall Quality) and effectively resolves errors in the initial translation (Resolve Errors). While this improvement is notably evident in Zh-En and En-De pair, for the En-Ru pair, approximately 40/150 annotations lean towards the Disagree category. Some of the feedback from En-Ru annotators who choose to Disagree state that there are cases when the output translation from the fine-tuned model is more precise in the target language, but loses some of the nuance in the source text. Further, feedback from the annotators support our own observation that the post-editing via finetuning does not only fix targeted errors in the original translation but rewrites for naturalness in the target language.They comment that the fine-tuning translation \u201cbetter explains the context\u201d and \u201cflows better in the target language\u201d compared to the original translation which seems to be directly translated without consideration of the context. We list further comments in Appendix Table 20.", "images": ["2404_07851v1_1"], "tokens": 531}]}
{"id": 60024, "dataset": "arxiv", "images": ["2305_14282v3_0", "2305_14282v3_1"], "chunks": [{"chunk_id": 1, "text": "How can we devise a fine-grained explanationbased text generation metric capable of pinpointing concrete error locations, identifying error types, assigning severity labels, and justifying the final score\u2014all simultaneously without relying on human-annotated data. In this paper, we propose INSTRUCTSCORE, a method to learn an explainable text generation metric without using human annotated ratings. InstructScore provides both a numerical score and a natural language error explanation. To this end, we first extract latent evaluation knowledge from an instruction-following model, such as GPT-4 (OpenAI, 2023), to construct a synthetic dataset with a predetermined explanation structure. Next, we determine a range of explanation failure modes and devise automated feedback to meta-evaluate error explanations. Finally, we further fine-tune INSTRUCTSCORE model on self-generated outputs that optimize feedback scores, resulting in diagnostic reports that are better aligned with humans. [<PIC>] We leverage GPT-4 to extract representative explainable knowledge that can greatly contribute to the subsequent Exp-Generator learning process.", "images": ["2305_14282v3_0"], "tokens": 260}, {"chunk_id": 2, "text": "Specifically, we collected raw sentences in the target language from diverse domains and topics via GPT-4 (Details are included in Section 5.1 and Appendix A), resulting in data across diverse tasks. This corpus is used as the starting point to inject errors. Then, we prompt GPT-4 to synthesize designated generation errors, as shown in Table 1. For each text, we specify the number of errors, error types, and severity labels, and ask GPT-4 to generate a candidate output with the specified error descriptions and 2) an explanation for this error annotation. If an evaluation task is multi-dimensional, error types will be separately assigned to each dimension (An example is included in the Appendix). Benefiting from the large-scale pre-training process, GPT-4 is able to generate diverse errors and meet the requirements with specified instructions. To avoid the model\u2019s over-reliance on the lexical and structural similarities between the candidate and raw text, we request GPT-4 to rephrase the raw text sentence to construct a pseudo-reference sentence. By specifying error type t, severity label se, and raw text, GPT-4 is able to generate a synthetic error sentence x with annotated error location l and a pseudo reference y with explanation e. How can we devise a fine-grained explanationbased text generation metric capable of pinpointing concrete error locations, identifying error types, assigning severity labels, and justifying the final score\u2014all simultaneously without relying on human-annotated data. In this paper, we propose INSTRUCTSCORE, a method to learn an explainable text generation metric without using human annotated ratings. InstructScore provides both a numerical score and a natural language error explanation. To this end, we first extract latent evaluation knowledge from an instruction-following model, such as GPT-4 (OpenAI, 2023), to construct a synthetic dataset with a predetermined explanation structure. Next, we determine a range of explanation failure modes and devise automated feedback to meta-evaluate error explanations.Finally, we further fine-tune INSTRUCTSCORE model on self-generated outputs that optimize feedback scores, resulting in diagnostic reports that are better aligned with humans. INSTRUCTSCORE assesses the quality of generated texts based on an explainable diagnostic report. Building upon this report, INSTRUCTSCORE provides an intuitive way to comprehend a model\u2019s generation capability, resulting in easier comparison among different models. In particular, we begin by extracting concise yet representative explainable knowledge from a large-scale instructionfollowing model, which is then utilized to train our Exp-Generator. After carefully analyzing the diagnostic reports produced by our Exp-Generator, we summarize common failure modes in diagnostic report and ask GPT-4 to identify them. Then we transform the GPT-4\u2019s feedback into alignment scores using our predefined criteria. Finally, we select diagnostic reports that have the highest alignment scores, and further finetune our Exp-Generator on those self-refined outputs. The overall framework is illustrated in <PIC>. The quality score s for each candidate y is determined based on the number of errors and their severity labels in the diagnostic report. Minor errors are given a score of \u22121 and major errors are given a score of \u22125. These penalties for errors are weighted to calculate the final score. Similar to previous practices (Freitag et al, 2021a), our metric identifies up to five errors per sentence.", "images": ["2305_14282v3_1"], "tokens": 827}]}
{"id": 60025, "dataset": "arxiv", "images": ["2405_19893v1_0", "2405_19893v1_1"], "chunks": [{"chunk_id": 1, "text": "Impact of selective retrieval. When optimizing the utility model, we add an empty string es in the training process to leverage the knowledge of LLMs to achieve selective retrieval. Here, we conduct some case studies to examine this mechanism. As we can see from Table 3, there are example queries that the utility model deems no retrieval and in total there are 20.9% queries which the utility model ranks the empty string es higher than other documents. As we can see from Table 3, many of the listed queries are commonsense knowledge that has been memorized in LLMs\u2019 parameters and we can easily call an LLM for the required answer instead of retrieval, which illustrates that in retrieval-augmented generation, despite the further knowledge external corpus introduces, the inherent knowledge of LLMs when deploying retrievalbased techniques is worth investigation and we believe that our work has provided a straightforward and plausible solution. However, this design still has certain limitations, i.e., the LLM used for end task needs to be the same or stronger than the one In this work, we argue that similarity is not always the \u201cpanacea\u201d for retrieval-augmented generation and totally relying on similarity would sometimes degrade the performance.As is shown in the upper part of <PIC>, when a user types in the query \u201cTell me about author George RR Martin\u201d, a similarity-driven retrieval system would rank the documents in a given corpus according to similarity metrics, i.e., the semantic relevance or TFIDF based metric (Robertson and Zaragoza, 2009). Even though the retrieved documents barely provide useful information, e.g., \u201cGeorge RR Martin is an author\u201d, it would rank higher due to high similarity score and the document that states the publications \u201cThe Song of Ice and Fire\u201d of George RR Martin with higher information gain would rank lower due to inadequate low similarity score. Besides, given the fact that the retrieved documents are often more than one, using them in is", "images": ["2405_19893v1_0"], "tokens": 467}, {"chunk_id": 2, "text": "olation due to the context limitation of LLMs (Shi et al, 2023)or simply aggregating the Top-k document without considering the relationships between them makes it difficult to capture the commonalities and characteristics among them and even confuse LLMs due to excessive text length thus incurring information loss and probably performance degradation (Mallen et al, 2023). Passage window size matters in utility model performance! Since the utility model plays a key role in bring in the supervision of LLMs into passage selection process, we take a closer look at how the passage window size \u2225D\u2022\u2225 8 would influence the final performance by directly incorporating the utility model trained under different settings to the final task. Due to the space limit, we average the metrics in four datasets and present the result in <PIC> and find that the model performance improves as the window size grows, which demonstrates that the growing passage window size endows more LLMs\u2019 powers for distinguishing passage significance among diverse inputs to the utility model thus improving performance on downstream tasks. However, due to the computational burden (the model training time grows linearly with the passage window size) that a large passage window size imposes during training, there is need to balance the trade-off between performance and cost.", "images": ["2405_19893v1_1"], "tokens": 293}]}
{"id": 60026, "dataset": "arxiv", "images": ["2409_09272v1_0", "2409_09272v1_1"], "chunks": [{"chunk_id": 1, "text": "Recent advancements in neural audio codecs such as SpeechTokenizer [107], Encodec [22] and VALL-E [83] have provided evidence of the advantages of multi-layer residual vector quantizers (RVQs) in accurately representing speech with discrete speech tokens for high-quality and efficient audio transmission, regardless of sound type or language. 2. We aim to develop the neural codec architecture into an effective decoupling model that separates mixed speech tokens into standalone semantic and acoustic tokens. As illustrated in <PIC>, our proposed decoupling model based on the codec architecture (CDM) comprises three core components: an encoderdecoder architecture, a HuBERT-equipped RVQs module, and a discriminator. The encoder-decoder\u2019s primary function of precisely reconstructing the original audio compels the encoder to extract the key features from speech signals. The HuBERT-equipped RVQs further decouple these features and hierarchically quantize them into discrete semantic and acoustic tokens. The discriminator enforces that the encoder and RVQs optimize their learned representations,aiming for comprehensive retention of the original audio\u2019s details.", "images": ["2409_09272v1_0"], "tokens": 295}, {"chunk_id": 2, "text": "Through this structure, we can achieve effective decoupling of speech signals. The decoupled semantic and acoustic audio samples can be found on our demo page [1]. The bottleneck layer aims to reduce the dimensions of acoustic tokens A from $R^{7C\u00d7Tn}$ to a more compact space $R^{C\u00d7Tn}$ by using 1D convolution and batch normalization. This layer serves a dual purpose: first, it enhances computational efficiency and reduces trainable parameters, facilitating subsequent layers to operate on a compact representation; second, it acts as a regularizer.By randomly rearranging the elements across the temporal dimension Tn, this layer nullifies speech comprehension that is highly dependent on the temporal order of phonemes and words [51]. We empirically set a shuffling window of 1 second, corresponding to 50 frames, to obscure word-level intelligibility (as each token representation is extracted from a 20ms waveform).In addition to decoupling speech information, the shuffle layer serves to augment content protection by further scrambling the condensed acoustic tokens $A^b$. As shown in <PIC>, By randomly rearranging the elements across the temporal dimension Tn, this layer nullifies speech comprehension that is highly dependent on the temporal order of phonemes and words [51]. We empirically set a shuffling window of 1 second, corresponding to 50 frames, to obscure word-level intelligibility (as each token representation is extracted from a 20ms waveform). Thereby, the likelihood of attackers deciphering and correcting these sequences is extremely low, given the sheer number of possible permutations for a 4-second audio ($50!^{4}$, approximately $8.56 \u00d7 10^{257}$, details are discussed in \u00a78). Our experiments also confirm the dual content protection by decoupling and shuffling, thwarting the advanced ASR techniques and human auditory analysis. ", "images": ["2409_09272v1_1"], "tokens": 476}]}
{"id": 60028, "dataset": "arxiv", "images": ["2405_02132v3_0", "2405_02132v3_1"], "chunks": [{"chunk_id": 1, "text": "As shown in <PIC>, the architecture for investigation is simply a speech encoder with an LLM. For each sample, the given prompt for transcribing (i.e., transcribe the speech), the speech utterance, and the corresponding transcript during training are denoted as P, S and T, respectively. We tokenize the prompt and the transcript using the tokenizer and embedding matrix of the LLM to obtain feature vector sequences Ep and Et as: $Ep = Embedding(Tokenizer(P)), (1)$ $Et = Embedding(Tokenizer(T)). (2)$ For the input audio S, we first extract features by passing the audio through a speech encoder to obtain encoder output Hs, denoted as: $Hs = Encoder(S). (3)$ Then, Hs is passed to a projector and further goes through a linear layer to obtain a feature sequence Es with the same dimensionality as the input to the LLM, denoted as: $Es = Linear(Projector(Hs)), (4)$ where the dimension of the feature output by the projector is the same as that of the speech encoder, and the linear layer is responsible for mapping the feature dimension to the embedding dimension of the LLM.", "images": ["2405_02132v3_0"], "tokens": 300}, {"chunk_id": 2, "text": "Next, we concatenate Es, Ep, and Et to obtain the final feature and pass it to the LLM to obtain the output transcript Y, denoted as: $Y = LLM(Regulation(Ep, Es, Et)). (5)\"$ We compare the effects of two types of projectors, Qformer and Transformer. In our experiment, we fix the encoder as HuBERT and LLM as Atom-7B and only unfreeze the projector module for one epoch training to compare the different effects of Qformer and Transformer, respectively. Although we expect Qformer to perform better than transformer similar to Blip [26] and SALMONN [19], results in <PIC> show otherwise. We hypothesize this is because Qformer was originally designed to accommodate the unique data structures in image processing, and should be redesigned to better adapt to speech for future research.", "images": ["2405_02132v3_1"], "tokens": 199}]}
{"id": 60029, "dataset": "arxiv", "images": ["2410_00037v2_0", "2410_00037v2_1"], "chunks": [{"chunk_id": 1, "text": "Similarly to SpeechTokenizer (Zhang et al, 2024b), we distill semantic information from a self-supervised model (WavLM8 (Chen et al, 2022) in our case) into the first level of the RVQ. WavLM projects a 16kHz waveform into 1024-dimensional embeddings sampled at 50Hz, while Mimi projects a 24kHz waveform into 512-dimensional at 12.5Hz. During training, we thus produce targets for distillation by downsampling the input waveform to 16kHz before computing WavLM embeddings followed by average pooling with a stride of 4 and a kernel size of 8, to reach 12.5 Hz. Interestingly, we observed that it was critical for performance to perform this average pooling in a non-causal way, which is compatible with streaming inference as these embeddings are only used during training. We apply a linear projection with an output dimension of 1024 to the output of the first RVQ level, parallel to the actual embedding going into the decoder. We then compute a cosine distance between the output of the first quantizer and the transformed WavLM embeddings, to perform distillation.Table 3 shows that this distillation loss conflicts with reconstruction and adversarial losses targeting quality. Indeed, while distillation significantly improves the phonetic discriminability of the first quantizer (as measured by ABX (Schatz et al, 2013)), it also affects audio quality negatively. We hypothesize that this is due to distilling semantic information into the first level of a single RVQ: As higher-order quantizers operate on the residual of the first one, the latter needs to trade audio quality for phonetic discriminability. We address this issue by proposing a split RVQ. Rather than a single RVQ with 8 levels, we distill semantic information into a plain VQ and apply an RVQ with 7 levels in parallel. We sum their outputs, such that while both can be used for reconstruction, we remove the constraint that acoustic information should be conserved in the residual of the semantic quantizer. <PIC> illustra", "images": ["2410_00037v2_0"], "tokens": 500}, {"chunk_id": 2, "text": "tes this architecture and Table 3 shows that this solution provides a better semantic-acoustic trade-off overall. Moshi is a multi-stream speech-to-speech Transformer model, which allows for full-duplex spoken dialogue with a user thanks to an innovative architecture summarized in <PIC>. Moshi is built on top of Helium, a text LLM which we build from scratch (Section 3.2), relying on high-quality text data to provide strong reasoning abilities to the model. We also propose Inner Monologue (Section 3.4.4), a training and inference procedure in which we jointly model text and audio tokens. This allows the model to fully exploit the knowledge imparted from the text modality, while remaining a speech-to-speech system. To enable real-time dialogue, we also design Moshi as a multi-stream architecture from the get-go (Section 3.4.3): The model is able to both speak and listen to the user at the same time, and does not need to explicitly model speaker turns.In addition, to capture the input user audio and output Moshi\u2019s voice with high quality and in an efficient manner, we propose Mimi (Section 3.3), a neural audio codec combining semantic and acoustic information into a single tokenizer by using residual vector quantization and knowledge distillation. To jointly model the audio streams from Moshi and the user, as well as Moshi\u2019s text tokens, we rely on a Depth Transformer compatible with streaming inference (Sections 3.4.1, 3.4.2).", "images": ["2410_00037v2_1"], "tokens": 352}]}
{"id": 60031, "dataset": "arxiv", "images": ["2410_09584v1_0", "2410_09584v1_2", "2410_09584v1_1"], "chunks": [{"chunk_id": 1, "text": "5. FollowRAG Benchmark To bridge the gap in automatic instruction-following evaluation for RAG systems, we introduce FollowRAG in this section. We provide a detailed introduction from two aspects: Data Construction'' and Evaluation and Statistics''. 5.1. Dataset Construction \\textbf{Instruction Collection \\& Extraction.} FollowRAG aims to assess the model's ability to follow user instructions in complex multi-document contexts. Drawing from general IF datasets like IFEval~\\citep{ifeval} and FollowBench~\\citep{followbench}, we collect and verify definitions and examples of atomic instructions using rules (e.g., code), excluding those irrelevant to RAG scenarios. Ultimately, we identify 22 types of instruction constraints, encompassing language, length, structure, and keywords. \\textbf{Instruction Reforming.} We use widely-used question-answering (QA) datasets, such as Natural Questions ~\\citep{NQ}, as the foundation for constructing FollowRAG samples. For a query sampled from the QA datasets, we need to generate a complex instruction containing $n$ atomic instruction constraints (with $n$ ranging from 1 to 4).To enhance the diversity of atomic instruction representations, we employ GPT-4o as the instruction generator. Specifically, given a query, we first sample $n$ instructions from the atomic instruction set and perform conflict detection. Subsequently, with examples as demonstrations, we prompt the LLM to generate a new varied instruction text for each type of atomic instruction, along with parameters for instruction-following evaluation. \\textbf{Combination.} Finally, we integrate the retrieved passages, query and atomic instructions to construct the sample input for FollowRAG. To avoid mechanically concatenating the query and instructions in a template-based manner, we prompt supervised model to naturally blend the multiple atomic instructions and the query into a coherent instruction-query paragraph. We then add the top-$K$ document passages retrieved based onthe query to the instruction-query paragraph, forming the complete sample input. % Finally, we integrate the retrieved passages, query, and atomic instructions to construct the sample input for FollowRAG. To avoid simply concatenating the query and instructions, we prompt the supervised model to blend them into a coherent instruction-query paragraph. We then add the top-$K$ document passages retrieved based on the query to this paragraph to form the complete sample input. 5.2. Evaluation and Statistics After obtaining the model's output, we evaluate it from two perspectives: instruction following and question answering (QA) under the RAG paradigm: \\begin{itemize} \\item \\textit{\\textbf{Instruction Following:}} Utilizing the verifiable nature of our atomic instructions and following the IFEval approach, we automate the verification of the model\u2019s adherence to each instruction through code validation. We then calculate the average pass rate for each atomic instruction across all samples to determine the instruction-following score in FollowRAG.\\item \\textit{\\textbf{RAG:}} Under new instruction constraints, the model's target output differs from the gold answers in the original QA dataset, rendering traditional metrics like Exact-Match ineffective. To address this, we use the original gold answers as a reference and utilize GPT-4o to evaluate whether the model's outputs correctly address the questions. The scoring criteria are as follows: Completely correct (1 point), Partially correct (0.5 points), Completely incorrect (0 points). The average score of all samples is taken as the RAG score for FollowRAG. \\end{itemize} % \\subsection{Statistics} For detailed statisticsin in Figure~\\<PIC>, FollowRAG is the first instruction-following evaluation dataset under RAG scenario comprising 2.8K samples, covering 22 fine-grained atomic instructions across 6 categories. The queries in FollowRAG are sourced from 4 QA datasets across 3 types: 1) Open-Domain QA: \\textbf{Natural Que", "images": ["2410_09584v1_0"], "tokens": 1014}, {"chunk_id": 2, "text": "stions (NQ)}~\\citep{NQ} and \\textbf{TriviaQA (TQA)}~\\citep{TriviaQA}; 2) Multi-Hop QA: \\textbf{HotpotQA (HQA)}~\\citep{HotpotQA}; and 3) Knowledge Base QA: \\textbf{WebQuestionsSP (WebQSP)}~\\citep{webqsp}. To further construct varying levels of instruction-following difficulty, FollowRAG includes 0.9K samples of single and dual atomic instructions, as well as 0.5K complex multi-instruction samples containing 3 and 4 atomic instructions, respectively. 6.4. Quantitative Analysis \\textbf{Ablation Study.} To examine the effects of various components in VIF-RAG, we conduct an ablation study in Table \\ref{tab:ablation}. The term \"w/o\" indicates versions where specific components are removed. Our key observations are: \\begin{itemize} \\item Removing any component from VIF-RAG results in decreased performance, indicating that all components, such as the complex instruction composition strategy and quality verification design, are crucial to its effectiveness.\\item The largest performance decline in FollowRAG is observed when executor verification is removed. This underscores the critical role of automated instruction-response validation in improving synthetic data quality and confirms the advantage of using LLMs to oversee instruction-following abilities through other core skills like coding. \\item Surprisingly, the consistency verification proves beneficial in preserving RAG capabilities. It effectively filters out samples with high-level semantic conflicts between instructions and queries, reducing noise in IF tasks and maintaining RAG performance integrity. \\end{itemize} \\textbf{Scaling Analysis.} To explore the impact of retrieved document quantity on instruction-following performance in RAG scenarios, we refer to Table \\<PIC>. For the baseline models (SFT versions), instruction-following capability declines as the number of passages increases. Specifically, performance drops sharply by over 6\\% when the document quantity in FollowRAG increases from 0 to 1. Further increa", "images": ["2410_09584v1_2"], "tokens": 545}, {"chunk_id": 3, "text": "sing the number to 10 leads to a significant performance decline, with Qwen-14B-SFT experiencing a drop of over 10\\%. This indicates that integrating knowledge through retrieval-augmented techniques challenges the instruction-following abilities of existing models. In contrast, VIF-RAG shows a minor performance drop ($<$3\\%) when encountering the first document. As the number of documents increases to 10, VIF-RAG\u2019s performance remains relatively stable, demonstrating its robustness. \\textbf{Instruction Difficulty Analysis.} To explore the effect of different instruction quantities (i.e., instruction-following difficulty) on model performance in RAG scenarios, we evaluate VIF-RAG and various baseline models on the FollowRAG benchmark, using test sets with 1, 2, and 3 instructions. As shown in Figure \\<PIC>, as the number of instructions increases, all models generally show a decline in instruction-following capability, but VIF-RAG consistently outperforms the rest. Notably, even with 3 instructions present simultaneously, VIF-RAG still demonstrates over a 5\\% IF prompt (strict acc.), further validating its superior capability in handling complex instruction-following tasks in RAG scenarios.", "images": ["2410_09584v1_1"], "tokens": 303}]}
{"id": 60032, "dataset": "arxiv", "images": ["2410_10594v1_0", "2410_10594v1_0", "2410_10594v1_1", "2410_10594v1_3", "2410_10594v1_2"], "chunks": [{"chunk_id": 1, "text": "Methodology In this section, we first recap the typical RAG pipeline (Sec.~\\ref{sec:method:preliminary}), then present our VisRAG framework (Sec.~\\ref{sec:method:ours}) and the construction of our training and evaluation data (Sec.~\\ref{sec:method:data}). Preliminary: Retrieval-augmented Generation \\label{sec:method:preliminary} A typical retrieval-augmented generation (RAG) pipeline consists of a retriever and a generator, both built on large language models (LLMs)\\footnote{In many cases, the retriever uses language models smaller than 1B parameters, which may not be considered ``large'', but we use the term LLM for simplicity.}. This pipeline operates on a knowledge corpus $\\mathcal{D}$, which is processed into units for retrieval and generation, denoted as $\\mathcal{D} = \\{d_1, \\ldots, d_n\\}$, where $n$ is the number of retrieval units.Given a text query $q$ and the retrieval corpus $\\mathcal{D}$, the retriever functions as $\\mathcal{R}:(q, \\mathcal{D}) \\rightarrow \\mathcal{D}{\\mathcal{R}}$, taking $q$ and $\\mathcal{D}$ as inputs and producing a candidate set $\\mathcal{D}{\\mathcal{R}} \\subset \\mathcal{D}$. To enable efficient search, the units in the knowledge corpus $\\mathcal{D}$ are pre-encoded into embeddings. During RAG pipeline inference, approximate nearest neighbor (ANN) search is applied to retrieve $\\mathcal{D}{\\mathcal{R}}$, which serves as the knowledge source for generation. The generation process can be defined as a function $\\mathcal{G}:(q, \\mathcal{D}{\\mathcal{R}}) \\rightarrow a$, where $a$ represents the answer and $\\mathcal{G}$ denotes the LLM generator.This is achieved by prompting the LLM with the query and the retrieved units $\\mathcal{D}{\\mathcal{R}}$ to generate an answer. As shown in Figure~\\<PIC> (left), traditional RAG frameworks (TextRAG) typically utilize text-based units for retrieval and generation. However, in real-world scenarios, data often appear in complex, multi-modal documents, requiring an additional parsing step to obtain t", "images": ["2410_10594v1_0"], "tokens": 651}, {"chunk_id": 2, "text": "ext. In this paper, we propose to use the \\textit{page} as the fundamental unit for retrieval and generation, which is directly processed by vision language models (VLMs) as an image without further processing during retrieval and generation. In subsequent sections, we use the terms page'' and document'' interchangeably. VisRAG: Vision-based Retrieval-augmented Generation \\label{sec:method:ours} In this section, we present \\textbf{Vis}ion-based \\textbf{R}etrieval-\\textbf{a}ugmented \\textbf{G}eneration (VisRAG), as shown in Figure~\\<PIC> (right). In contrast to traditional RAG frameworks which use text segments for both retrieval and generation, VisRAG leverages the image of the document to preserve all information. \\subsubsection{Retrieval} The first stage of VisRAG, VisRAG-Ret, aims to retrieve a set of pages from the corpus $\\mathcal{D}$ given $q$. We follow the dual-encoder paradigm in text-based dense retrieval models but employ a VLM rather than an LLM to encode the query and page.", "images": ["2410_10594v1_0"], "tokens": 291}, {"chunk_id": 3, "text": "Specifically, the query and page are encoded separately as text and image in the VLM, producing in a sequence of hidden states. To derive the final embedding, and given that we use generative VLMs with causual attention, we adopt the position-weighted mean pooling over the last-layer VLM hidden states, giving higher weights to later tokens: \\begin{equation} \\small \\mathbf{v}=\\sum{i=1}^S w_i \\mathbf{h}i, % \\quad \\text { where } \\quad w_i=\\frac{i}{\\sum{i=1}^S i}, \\end{equation} where $\\mathbf{h}i$ is the $i$-th hidden state, $S$ is the sequence length, $w_i=\\frac{i}{\\sum{j=1}^S j}$ is the $i$-th weight, and $\\mathbf{v}$ is the query or page embedding. The similarity score is calculated by the cosine similarity of the query and page embedding.VisRAG-Ret is optimized using the InfoNCE loss: \\begin{equation} \\label{eq:loss} \\small l(q,d^+,D^-) = -\\log\\frac{\\exp(s(q,d^+)/\\tau)}{\\exp(s(q,d^+)/\\tau)+\\sum_{d^- \\in D^-}\\exp(s(q,d^-)/\\tau)}, \\end{equation} where $d^+$, $D^-$ are positive document and the negative document set of $q$, respectively, $s(q,d)$ is the similarity score between $q$ and $d$, and $\\tau$ is the temperature. Generation The second stage of VisRAG, VisRAG-Gen, focuses on generating the answer according to the user query and retrieved pages using a VLM. We propose the following mechanisms to enable VisRAG-Gen to handle multiple retrieved pages in $\\mathcal{D}\\mathcal{R}$ for generation. The prompts used for generation is presented in Appendix~\\ref{appendix:generation_prompt}. \\paragraph{Page Concatenation.} A straightforward approach is to concatenate all pages in $\\mathcal{D}\\mathcal{R}$ into a single image to accommodate most VLMs that are trained to accept a single image.Formally, \\begin{equation} \\small a \\xleftarrow{} \\text{VLM-Single}(q,\\text{Concat}(\\{d|d \\in \\mathcal{D}\\mathcal{R}\\})), \\end{equation} where VLM-Single is a VLM that accepts a single image with text prompt and Concat is the image concatenation operation. In this paper, we experiment with horizontal concatenation. \\paragraph{Weighted Selection.} Another approach is to ask the VLM to generate an answer for every page from top-$k$, and select a final one with the highest confidence. The final confidence is defined as the weighted generation probability of the answer: \\begin{equation} \\small P(a|q,\\mathcal{D}\\mathcal{R}) = P(a|q,d) \\cdot \\lambda(q,d), \\end{equation} where $P(a|d,q)$ is calculated as the reciprocal of the perplexity of generating the answer $a$ conditioned on the single document $d$, and $\\lambda(d,q)$ is the normalized retrieval score: \\begin{equation} \\small \\lambda(q,d)=\\frac{e^{s(q,d)}}{\\sum{d' \\in \\mathcal{D}\\mathcal{R}} e^{s(q,d')}}. \\end{equation} \\paragraph{VLMs Accepting Multiple Images.} Some recent VLMs like MiniCPM-V 2.6 and Qwen-VL 2 are designed and trained to accept multiple images as input to perform cross-image reasoning. This capability may be useful for the generation as the required information could be located on a single page from the retrieved document set $\\mathcal{D}\\mathcal{R}$ for single-hop questions or spread across multiple pages for multi-hop questions. Formally, we have \\begin{equation} \\small a \\xleftarrow{} \\text{VLM-Multi}(q,\\{d|d \\in \\mathcal{D}\\mathcal{R}\\}), \\end{equation} where VLM-Multi is the VLM that accepts multiple images with text prompt. Training Data Efficiency \\label{sec:result:data_efficiency} As retrieval acts as the bottleneck in an RAG pipeline, it is crucial to have an effective retrieval component to maintain optimal performance. In this experiment, we study the training data efficiency of VisRAG-Ret by evaluating the performance of VisRAG-Ret trained under different amounts of synthetic training data, i.e. in the out-of-domain setting. As shown in Figure~\\<PIC>, when only trained on 20k q-d pairs, VisRAG can surpass bge-large (OCR).", "images": ["2410_10594v1_1"], "tokens": 1368}, {"chunk_id": 4, "text": "After training on 150k pairs, it can further surpass NV-Embed-v2 (OCR), the SOTA 8B-sized text embedding model trained on millions of curated text pairs. This highlights VisRAG-Ret's high training data efficiency and strong generalization capability, as all models are evaluated out-of-domain. When compared with MiniCPM (OCR), which uses extracted text for training, VisRAG-Ret consistently achieves a performance gain of about 17\\% and exhibits a more stable training process. The results show VisRAG-Ret's potential for further performance improvements by scaling up the training data. Performance on Different Data Subsets \\label{sec:result:performance_grouped} In this experiment, we assess the retrieval and generation performance of VisRAG and TextRAG defined in Figure~\\<PIC>, as well as VisRAG (SigLIP), which replaces the retriever in VisRAG with SigLIP. We report their performance across different data subsets by categorizing queries based on the lengths of their positive documents, measured by the number of tokens of the extracted text. Documents with a higher volume of extracted text may prioritize textual information over visu", "images": ["2410_10594v1_3"], "tokens": 293}, {"chunk_id": 5, "text": "al content. As illustrated in Figure~\\<PIC>, queries in ArxivQA and InfographicsVQA are divided into equal-sized bins according to the lengths of their relevant documents. For each bin, we calculate and plot the average performance differences between VisRAG and TextRAG, as well as between VisRAG (SigLIP) and TextRAG, to compare how each model performs relative to TextRAG. We observe that, in general, the relative performance of VisRAG and VisRAG (SigLIP) improves as the length of the relevant document decreases. This suggests that models with vision encoders can better understand documents that emphasize visual information. However, VisRAG (SigLIP) consistently underperforms VisRAG across all data subsets and, in some cases, even performs worse than TextRAG. In contrast, VisRAG consistently outperforms TextRAG, indicating that the underlying language model in VisRAG is crucial for better understanding the semantics conveyed through visual cues.", "images": ["2410_10594v1_2"], "tokens": 243}]}
{"id": 60033, "dataset": "arxiv", "images": ["2404_00236v2_0"], "chunks": [{"chunk_id": 1, "text": "Introduction \\textbf{Background.} With the boom of digital information, billions of user requests are produced daily, so recommender systems (RSs) have become an integral part of Internet platforms. To capture users' interests more accurately, RSs have gone through several milestones, such as the logistic regression with hand-crafted features (e.g., FM), the neural networks (e.g., WideDeep, YoutubeNet), the sequential signal (e.g., DIN, SIM), and the multi-hop graph signal (e.g., PinSage, DGRec). In retrospect, these effective methods are based on the collaborative filtering (CF) idea and extend the boundaries of RSs. However, the CF framework also limits them under the case of cold-start and data sparsity problems. The reason is that the CF idea aims to mine the user/item pattern intelligence from data, to discover and recommend high-click candidate items for a user while it is hard to understand the users' fine-grained and multi-aspect interests.In fact, instead of mining user preferences from massive user-item interaction logs, the user always leaves some reviews/comments to explain further his/her feelings about this interaction, which provides an explicit way to understand the users' complex interests in language semantic space. And the recent effort MoRec claimed that multi-modal information yield better results in sequential recommendation. \\textbf{Related work.} To extract valuable content semantic information, the pioneering works are formed as a rating prediction task: for a user-item pair in test set, give the historical user/item contents in training set (e.g., reviews), then predict their possible interaction rating. In early years, the DeepCoNN employed two convolutional neural networks (CNNs) towers to aggregate user/item content tokens individually to measure their dot-similarity, and the D-Attn further extended DeepCoNN by introducing local and global attention mechanism to replace CNNs to aggregate tokens. Following the D-Attn, the ALFM and ANR focused on extracting the fine-grained multi-aspect semantic information and assigning different weights for aspects. The recent progress is RGCL, which used BERT to generate the user-item content scores and then leveraged them as user-item graph edge weights to conduct a multi-hop graph neural network to make prediction. \\textbf{Motivation.} Although these methods raised model ability with content information, they ignore the following problems: \\begin{itemize}[leftmargin=,align=left] \\item \\textit{Transfer semantic knowledge across domains}: Actually, RS needs to serve several domains simultaneously, such as electronics, clothing, books, etc. Since different domains always express different aspects of users' interests (e.g., food is delicious, price is appropriate, etc.), the previous methods need to re-tune their semantic component for different domains, which is time-consuming. \\item \\textit{Enhancing the correlation between content and ID}: The user-item content and ID-interaction information can be seen as two different modalities connected by users.Nevertheless, previous methods focus on utilizing separate components to model the two corresponding content/ID spaces while ignoring how to exploit the correlation and align them in a unified space. \\end{itemize} \\textbf{Our Work.} To alleviate the above problems, we propose \\textbf{LoID}, a LLM-based model for transferring semantic knowledge across domains based on LoRA, and aligning content/ID information with contrastive objectives. It mainly includes two steps: \\begin{enumerate}[leftmargin=,align=left] \\item \\textit{Pretraining semantic plugin'}: On the one hand, the ideal semantic information should act as a universal role to support all domains. On the other hand, different domains have their main aspects of semantic information that are related to recommendation. We borrow the LoRA strategy idea to train a small set of parameters for each domain, which could serve as plugins. Then we fused them by DARE without further re-training. \\\\item \\\\textit{Aligning the content/ID feature spaces}: As discussed before, the content and ID information can be seen as two modalities of user-item interaction. To minimize their gap, we introduce the contrastive idea of maximizing content/ID features' mutual information to align their feature spaces. \\\\end{enumerate} Finally, to validate our LoID effectiveness, we extensively test LoID under 11 different domain datasets to show its superior ability. \\\\textbf{Contributions.} Our contributions are summarized as follows: \\\\begin{itemize}[leftmargin=*,align=left] \\\\item We give a plugin' idea to transfer semantic knowledge, which sheds light on building a new paradigm for recommendation\\footnote{Codes at \\url{https://github.com/cjx96/LoID}.}. \\item We devise a novel content/ID feature alignment objective. \\item We conducted detailed analyses with SOTA methods and LLMs. \\end{itemize} Methodology Overview The architecture of our method LoID is illustrated in Figure~\\<PIC>.In part (a), we train the LoRA parameters of source domains as \"plug-ins\" to enhance target domain prediction without further re-training. In part (b), we extract historical contents of user/item to obtain user/item semantic representation, and then align the ID and semantic to make target domain rating prediction, \\textbf{note that the source domain LoRA plugin is an optional choice}.", "images": ["2404_00236v2_0"], "tokens": 1384}]}
{"id": 60035, "dataset": "arxiv", "images": ["2407_16364v2_0", "2407_16364v2_1"], "chunks": [{"chunk_id": 1, "text": "Slide-LoRA: Enhancing Image and Text Generation Consistency To harmonize multi-modal generation tasks in a single model, the parameter space would be optimized for inconsistent or conflict training objectives, as stated before. We propose Slide-LoRA (which is shaped like a ``slide\" between different LoRA experts as shown in Figure \\ref{algorithm}), a novel module that can be conveniently inserted into Transformer layers as Low-Rank Adaptation (LoRA) and introduces limited parameter increase. As such, Slide-LoRA spontaneously processes text generation and image generation in separate parameter spaces, thus relieving the inconsistent training problem. As shown in Figure \\<PIC>, Slide-LoRA is composed of a gating network $\\mathcal{G}$ and three LoRA modules $(\\boldsymbol{R}^{T}, \\boldsymbol{R}^{I}, \\boldsymbol{R}^{S})$. $\\boldsymbol{R}^{T}$ and $\\boldsymbol{R}^{I}$ serve as the separate parameter space for text and image generation, respectively, while $\\boldsymbol{R}^{S}$ aims to learn the knowledge shared by both text and image generation.", "images": ["2407_16364v2_0"], "tokens": 300}, {"chunk_id": 2, "text": "Specifically, Given the input token sequence $\\boldsymbol{x} \\in \\mathbb{R}^{L \\times D}$, the gating network $\\mathcal{G}$ (concatenation of two linear layers in specific) determines whether the processing of the input token sequence requires knowledge of text generation or image generation, and produces a scalar $\\gamma = \\mathcal{G}(\\boldsymbol{x}) \\in [0, 1]$. The output of the Slide-LoRA layer can be formulated as \\begin{equation} \\boldsymbol{O} = \\frac{1}{2}\\cdot\\{[\\gamma \\geq 0.5] \\cdot \\boldsymbol{R}^{T}(\\boldsymbol{x}) + [\\gamma < 0.5] \\cdot \\boldsymbol{R}^{I}(\\boldsymbol{x}) + \\boldsymbol{R}^{S}(\\boldsymbol{x})\\}, \\end{equation} where $[\\cdot]$ equals $1$ if the condition inside is true and $0$ otherwise.Slide-LoRA incorporates task-specific and task-shared knowledge from input tokens, thus separating the inconsistent training objective and learning the shared knowledge of text and image generation. \\subsection{Multi-Modal Pre-Training and Comprehensive Fine-Tuning} TextHarmony training process consists of two stages. In the multi-modal pre-training stage, TextHarmony is trained on text-rich image-text corpus and learns to produce multi-modal outputs. In the comprehensive fine-tuning stage, we concurrently cultivate the text and image generation capabilities of TextHarmony by training on a series of text-centric tasks. Stage 1: Multi-Modal Pre-Training TextHarmony is pre-trained based on the pre-training weight of MM-Interleaved, with extra text-rich datasets including MARIO-LAION and DocStruct4M. MARIO-LAION contains 9M web images with brief captions and the according OCR results. DocStruct4M consists of 2M documents and 1M natural images with text-oriented structure annotations. We use MARIO-LAION for both text and image generation ({\\it, i.e.}, either predict the caption of the image or generate the image based on the caption), and we use DocStruct4M for text generation only.In this stage, we freeze the vision encoder and the LLM, training only the Q-Former and the image decoder to obtain basic image understanding and generation capabilities. Stage 2: Comprehensive Fine-Tuning We integrate various text-centric datasets and employ uniform instructions for all tasks. In this stage, the vision encoder, Q-former, image decoder, and the proposed Slide-LoRA are trained to enhance the multi-modal generation and human-instruction-following capabilities of TextHarmony. \\textbf{Visual Text Generation.} In this task, TextHarmony generates images according to the text description and is required to render accurate and coherent text. Although MARIO-LAION contains captions of text-rich images, the description is oversimplified and lacks concentration on the textual elements within the image. To this end, we sample 100K images from MARIO-LAION and generate detailed captions about them, termed DetailedTextCaps-100K. The captions focus on both visual and textual elements in the images. This isachieved by prompting Gemini Pro, a pioneer multi-modal large language model, to generate detailed descriptions based on the sampled image and the OCR results. As shown in Figure \\<PIC>, the image description from DetailedTextCaps-100K is more comprehensive compared with MARIO-LAION and can better depict the textual elements in the image. \\textbf{Visual Text Editing.} In this task, TextHarmony substitutes or renders text in the given location of the image and keeps the background consistent. We randomly mask the image with the help of MARIO-LAION's OCR results and fine-tune TextHarmony in a self-supervised manner. \\textbf{Visual Text Comprehension.} We employ the training set collected by Monkey for the text-centric VQA fine-tuning. The training set involves 1.4M QA pairs and covers various text-rich scenarios. \\textbf{Visual Text Perception.} For the basic OCR capabilities, we randomly sample 1M images from MARIO-LAION and leverage the OCR annotations.", "images": ["2407_16364v2_1"], "tokens": 1106}]}
{"id": 60039, "dataset": "arxiv", "images": ["2410_22394v1_1", "2410_22394v1_0"], "chunks": [{"chunk_id": 1, "text": "Introduction Although AI has brought transformative changes to various aspects of life, its impact on researchers unfolds in a nuanced manner. On the one hand, AI assists in various research disciplines, such as Social Science \\citep{DBLPNeumanCY23}, Finance \\citep{ai4finance}, Medicine \\citep{rakhimov2022artificial}, GeoScience \\citep{praskievicz2018river}, etc., significantly expediting academic processes. However, many of these applications are superficial, often limited to data-driven clustering or classification. On the flip side, the AI era poses challenges for researchers. Despite its ability to streamline some activities, researchers still face demanding, cognitively intensive tasks such as staying current through extensive paper reading, rapidly generating ideas in response to fast-paced advancements, conducting rigorous experiments to substantiate claims, and managing an increasing volume of peer reviews. Then a question looms: \\emph{How effectively can AI assist researchers in tasks that are domain-specific, expertise-demanding, and reasoning-intensive?} Existing works proved the promising potential for using LLMs in assisting AI research.\\citet{si2024can} conducted a large-scale human study and found that LLMs can generate creative research ideas. \\citet{lu2024aiscientist} proposed an autonomous agent to handle complicated research workflow and write a whole research paper. However, most of these works focus on addressing highly subjective problems that require a high degree of expertise, making evaluation laborious and hard to reproduce. This underscores the need for a comprehensive benchmark that rigorously assesses LLMs' capabilities in expertise-intensive research activities. To this end, in this work, we introduce \\textsc{AAAR-1.0}, a novel benchmark that aims to comprehensively assess the LLMs' capacity on expert-level research tasks. As illustrated in Figure~\\<PIC>, \\textsc{AAAR-1.0}~decomposes four distinct expert-level AI research tasks from th", "images": ["2410_22394v1_1"], "tokens": 490}, {"chunk_id": 2, "text": "e researcher's daily activities, including i) \\textsc{EquationInference}, investigating whether the LLMs can infer the equation correctness based on the paper context; ii) \\textsc{ExperimentDesign}, validating LLMs' ability on designing reliable experiments for a research idea; iii) \\textsc{PaperWeakness}, testing the quality of weaknesses discovered by LLMs from paper drafts; and iv) \\textsc{ReviewCritique}, investigating whether LLMs can identify and explain the deficient/unreliable human-written paper reviews. To ensure data quality, senior AI researchers with extensive domain expertise perform data annotation for \\textsc{AAAR-1.0}, followed by rigorous multi-round data examination and filtering. All four tasks require models to possess strong domain knowledge covering various cutting-edge research findings, as well as expert-level research experience, to the extent that even humans need substantial research accumulation to tackle the tasks we designed.Benefiting from the proposed automatic metrics, we conduct extensive experiments across numerous mainstream LLMs, where we find that: \\vspace{-0.5em} \\begin{itemize}[leftmargin=2em] \\itemsep0em \\item With a random guess baseline of 25\\%, the performance of most LLMs on \\textsc{EqInfer}~hovers just slightly above chance, with the top models reaching around 60\\%. This highlights the difficulty of the task, despite its reliance only on local context reasoning. \\item In \\textsc{ExpDesign}, LLM-designed experiments are innovative and more diverse than those by humans; however, many are trivial, lack feasibility, and stray from the original research objectives. \\item In \\textsc{PaperWeakness}, LLM-identified weaknesses often lack depth and specificity, making them broadly applicable and less useful for providing feedback on paper drafts. \\item In \\textsc{ReviewCritique}, LLMs struggle to effectively identify deficient human reviews, indicating limited usefulness in assisting meta-reviewers in evaluating the quality of individual paper reviews. \\end{itemize} \\paragraph{Settings.} As different LLMs have distinct context windows, to ensure a fair comparison, we fix the maximum input length for all models. According to the data statistics of Table~\\ref{tab:equations_statistics}, we empirically use 1,000 words for both contexts before and after equations, i.e., 2,000 surrounded words. \\paragraph{Main results.} Table~\\ref{tab:equation_main_tab} shows the main results. Firstly, the open-source LLMs, especially the Falcon and Gemma, perform unexpectedly disappointing (even worse than random guesses). These screwed scores are mainly due to the poor long-context instruction following ability, where we find some open-source LLMs are confused with the massive input and often copy the LaTeX code from the input. In contrast, closed-source LLMs generally achieve superior accuracy, probably owing to the richer scientific knowledge from the larger model parameters.However, considering the conventional multi-choice QA formulation of~\\textsc{EqInfer}, the recently-released GPT-4o solely gets 43.18, implying the unique challenge of \\textsc{EqInfer}~compared with other scientific QA benchmarks~\\citep{song2023nlpbench}. Notably, with the help of internal CoT, o1 gains stronger performances than GPT-4/GPT-4o, indicating the potential benefits of adopting reasoning for this task. \\paragraph{$\\mathcal{Q}$: Do more contexts boost performance?} Table~\\ref{tab:equation_main_tab} unifies the input context lengths to 1,000 tokens for various LLMs. To answer this question, we experiment with long-context LLMs to investigate the impact of the input context lengths. Particularly, we scale the input length (per side) from 100 to 1,500 words. As shown in Figure~\\<PIC>, for the open-source LLMs (Llama and Qwen), after 300 words length, increasing the input context doesn't help the performance and even significantly drops Qwen's scores.dually boosts the performances at the first 1,000 words, but stabilizes afterwards. This is in line with human intuition, i.e., surrounding context is required for the equation inference, as the adjacent context usually provides important information, such as the target algorithm description or the notation definition. However, after exceeding a specific threshold, more context information is not beneficial anymore and even confuses those LLMs with poor long-context handling capacity~\\citep{wang2024leave,liu2024lost}.", "images": ["2410_22394v1_0"], "tokens": 1178}]}
{"id": 60040, "dataset": "arxiv", "images": ["2412_04455v1_0", "2412_04455v1_1"], "chunks": [{"chunk_id": 1, "text": "We first give an overview of the proposed Code-as-Monitor (\\fname)~(\\cref{subsec: overview}). Then, we elaborate on the constraint element in Constraint Painter, especially constraint-aware segmentation~(\\cref{subsec: constraint element proposal}). Finally, we present Constraint Monitor for real-time detection~(\\cref{subsec: real-time monitor module}). we give the necessary implementation details in \\cref{{subsec: implementation details}}. Overview} {subsec: overview} The proposed {\\fname} comprises three key modules: the Constraint Generator, Painter, and Monitor. We focus on long-horizon manipulation task instructions $\\mathcal{L}_{\\mathrm{global}}$ (\\eg, \\textit{``Move the pan with the bread to the stove, and be careful not to let the bread fall out''}), using RGB-D observations $\\mathcal{O}$ from two camera views~(front and top).As shown in \\c<PIC>, the RGB images~$\\mathcal{O}$, along with instructions~$\\mathcal{L}_{\\mathrm{global}}$, previous subgoal~${l}_{\\mathrm{pre}}$, and failure feedback from the Constraint Monitor~$f_{\\mathrm{pre}}$~(\\eg, subgoal success or failure reason), are fed into the Constraint Generator~$\\mathcal{F}_{\\mathrm{VLM}}$~(\\ie, GPT-4o~\\cite{achiam2023gpt}) to generate the next subgoal~${l}_{\\mathrm{next}}$ and associated textual constraints~$\\mathcal{C}$. This process can be formulated as follows: These outputs are integrated and refined through manual inspection to produce the multi-granularity dataset, which is combined with LISA's training data to fine-tune our model. More details are in Supp.~\\ref{supsec: implementation details}. We use this dataset and LISA's training data to co-fintune our model. Further details are in Supp.~A.", "images": ["2412_04455v1_0"], "tokens": 540}, {"chunk_id": 2, "text": "Our dataset utilizes images from BridgeData V2~\\cite{walke2023bridgedata}. To determine per-frame subgoals and constraints, we require annotations at the frame level; however, BridgeData V2 provides instructions only at the trajectory level. herefore, we sample pick-and-place data and process it using external references such as gripper open/close states, resulting in $10,181$ trajectories comprising $219,356$ images. o generate ground-truth annotations, we employ GPT-4o~\\cite{achiam2023gpt} to decompose trajectory instructions into per-stage subgoals, corresponding textual constraints, and associated objects and parts. e then employ Grounded SAM~\\cite{ren2024grounded} for instance-level segmentation and Semantic SAM~\\cite{li2023semantic} for part-level segmentation. By integrating these outputs, we achieve multi-granularity annotations, which are further refined through manual inspection. We use this dataset and LISA's training data to co-fintune our model. For more details, please check Supp.~A.We use BridgeData V2~\\cite{walke2023bridgedata} as the image source for our dataset. We need to determine the subgoal of each frame in a trajectory to ascertain the constraint of the current frame.Then, fine-grained part-level masks \\( \\mathcal{M}_{p} \\) and corresponding element type descriptions \\( l_{\\mathrm{e}} \\) are produced, as shown in \\c<PIC>. However, BridgeData V2 provides instructions only for entire trajectories. Therefore, we sampled pick-and-place data from BridgeData V2 and processed the dataset using external reference information such as the gripper's open/close states. Ultimately, we sampled $10,181$ trajectories to construct our dataset, which contains a total of $219,356$ images. After sampling the image data, we used a combination of off-the-shelf foundation models to obtain the ground truth. Specifically, we input the entire trajectory instruction provided by BridgeData V2 into GPT-4o for decomposition. This process yielded the subgoals of each stage, the corresponding textual constraints, and the object and part associated with each constraint. We then employed Grounded SAM~\\cite{ren2024grounded} for instance-level segmentation and Semantic SAM~\\cite{li2023semantic} for part-level segmentation. By combining these results, we obtained annotations at multiple granularities. Finally, we conducted manual inspections to further improve annotation quality.", "images": ["2412_04455v1_1"], "tokens": 645}]}
{"id": 60041, "dataset": "arxiv", "images": ["2412_04426v1_0", "2412_04426v1_1"], "chunks": [{"chunk_id": 1, "text": "To characterize the performance of VPA in correcting the Q-estimations, we leverage Spearman\u2019s rank correlation coefficient, which measures the strength and direction of a monotonic relationship between two ranked variables. The reason is that the relative ranking of Q-values are more important than the absolute values for policy update. Specifically, given a dataset collected by rolling out the offline policy in the environment, we compare the ranking of learned reward/cost Q-values before and after VPA with that of estimated actual return by using Monte Carlo simulations. A large Spearman\u2019s rank correlation coefficient implies that the distribution of learned Q-values is more aligned with the distribution of true Q-values. As shown in Table \\ref{table:Spearman}, it is evident that the coefficient increases significantly after VPA for both reward and cost Q-values, no matter if the offline policy rolls out from a seen state-action pair in the offline dataset or from a randomly selected OOD state-action pair. This clearly demonstrates the effectiveness of VPA in aligning the pretrained Q-functions. subsection{Finetune Phase} {Challenge II: Lagrange multiplier mismatch.~} Conventional value-based online safe RL relies on updating Lagrange multipliers alongside the policy and Q-functions during training, so as to push the overall cost below the limit while striking a right balance between maximizing the reward and minimizing constraint violations. While the policy and Q-functions can benefit from offline pretraining for a warm start, offline safe RL algorithms like CPQ \\citep{xu2022constraints} and BEAR-lag \\citep{ray2019benchmarking} cannot accurately estimate Lagrange multipliers with regularizing strengths matching with the cost of the offline policy, e.g., a small Lagrange multiplier is not power enough to push down the policy cost, while a large multiplier prevents active exploration of high-reward state-action pairs. For instance, in the BallCircle environment, the offline pretrained Lagrange multiplier value obtained using the BEAR-lag algorithm is approximately 1500, whereas during online finetuning, the SAC-lag requires a value of only about 0.65. The gap between these values clearly precludes the direct use of offline pretrained Lagrange multipliers. Improper initialization can lead to extensive constraint violations or training stagnation, an issue we term as the \\textbf{Lagrange multiplier mismatch}. On the other hand, as VPA promotes active exploration of high-reward state-actions by optimistically estimating rewards and pessimistically estimating costs, it inevitably increases the risk of exploring high cost state-actions, which in turn amplifies the need for appropriate Lagrange multipliers to quickly reduce the constraint violations. Figure \\<PIC> shows the online finetuning performance comparison after VPA in two environments between 1) empirically setting a good initial value for the Lagrange multiplier and 2) setting it to zero, where the traditional dual ascent method is used to update the multiplier. It is clear that a good initial multiplier can manage the cost very well, while the policy with a very small initial multiplier value suffers from large constraint violations and takes a much longer time to reduce the cost below the limit.", "images": ["2412_04426v1_0"], "tokens": 781}, {"chunk_id": 2, "text": "The results also imply that although VPA aligns the distributions of Q-values, it may introduce high costs for online finetuning, which can be addressed with an appropriate initial Lagrange multiplier. ction{Experiments} In this section, we conduct extensive experiments to verify the effectiveness of our approach, aiming to answer the following questions: 1) {\\bf RQ1:} How does our method compare with naive finetuning and other SOTA baselines in both reward and cost? 2) {\\bf RQ2:} How do different components in Marvel affect the performance? Due to the space limit, we delegate the experimental details and some additional results to \\cref{sec:more_exp_} and cref{sec:Experimental_Details}. ubsection{Evaluation Setup} mph{Benchmarks.} We consider the DSRL benchmark \\citep{liu2023datasets} and select xtbf{ten} environments from the { Bullet Safety Gym \\citep{gronauer2022bullet} and Safety Gymnasium \\citep{ji2023safety}}: BallRun, BallCircle, CarRun, CarCircle, HalfCheetah, AntCircle, AntRun, DroneCircle, Hopper, and Swimmer (results for the last four are in \\cref{sec:more_exp}). The cost threshold is set to be 20 in these environments. As mentioned earlier in \\cref{preliminary}, we choose CPQ and SAC-lag as base algorithms in our proposed framework Marvel for offline training and online finetuning, respectively, due to the effectiveness and representativeness of them. As shown in \\c<PIC>, Marvel demonstrates better or comparable performance compared to all baselines consistently across all environments, i.e., achieving the higher return while keeping the cost below the threshold.SO2 improves Q-value estimation through Perturbed Value Updates, JSRL and PEX utilize offline pretrained policies for exploration, and Warm Start directly finetunes the policy and Q-networks from offline safe RL without modifications. ", "images": ["2412_04426v1_1"], "tokens": 506}]}
{"id": 60042, "dataset": "arxiv", "images": ["2412_04380v1_0", "2412_04380v1_1"], "chunks": [{"chunk_id": 1, "text": "textbf{Online 3D Scene Perception:} Accurate comprehension of 3D scenes is an indispensable capability for embodied agents. Many tasks, such as 3D occupancy prediction\\cite{monoscene, yu2024monocular} and object detection\\cite{Qi_2019_ICCV, wang2022cagroup3d}, are direct manifestations of this capability. Currently, most works on 3D scene perception\\cite{qi2017pointnet, graham20183d, yi2019gspn} was conducted offline, taking pre-acquired and reconstructed 3D data to obtain a relatively lagging perception. Based on this situation, Online3D\\cite{xu2024online} introduced an adapter-based model that equips mainstream offline frameworks (in Figure~\\<PIC>) with the competence to perform online scene perception, which means they can process a real-time RGB-D sequences. However, this framework still fails to overcome the intrinsic limitation of conventional point cloud modality. In a more general embodied scenario, real-time monocular visual input for scene perception can further advance the research on embodied agents.", "images": ["2412_04380v1_0"], "tokens": 271}, {"chunk_id": 2, "text": "textbf{3D Gaussian Splatting:} 3D Gaussian Splatting\\cite{kerbl3Dgaussians} uses anisotropic 3D Gaussians to model a 3D scene, renowned for its fast speed and high quality in the field of radiance field rendering. The explicit physical characteristics of 3D Gaussians and the splat-based rasterization employed during rendering have also motivated rapid advancements in research fields such as scene editing\\cite{gu\u00e9don2024gaussianfrostingeditablecomplex, palandra2024gseditefficienttextguidedediting, silva2024contrastive}, dynamic scenarios\\cite{Lu_2024_CVPR, gao2024gaussianflowsplattinggaussiandynamics, xiao2024bridging}, and SLAM\\cite{yugay2023gaussian, yan2024gs, deng2024compact, li2025sgs}. GaussianFormer\\cite{gaussianformer} pioneers the application of 3D Gaussians in outdoor 3D semantic occupancy prediction, updating Gaussians through comprehensive features extracted from multi-view images. These Gaussians are ultimately converted into local 3D occupancy prediction through an elaborately designed Gaussian-to-voxel splatting module. Compared to conventional voxel-based methods, using 3D Gaussian representation constitutes a more flexible approach. In this paper, we will leverage this significant attribute to accomplish embodied occupancy prediction in indoor scenarios. To this end, we introduce an additional tag \\(\\gamma \\) for all the Gaussians in the memory. When initializing a novel scene, tags of these Gaussians are set to \\(0\\). Every time we put a number of updated Gaussians back into the memory, their tags are set to (1\\). For the Gaussians taken from the memory, we generate a set of confidence values \\(\\Theta \\) based on their tags \\(\\Gamma\\). For those Gaussians within the frustum and marked as having been previously updated(\\(\\Gamma=\\mathbf{1} \\)), we set their confidence values to a certain value between (0\\) and \\(1\\), and they are slightly updated in the current update. For those Gaussians that have never been updated, we set their confidence values to \\(0\\), indicating that they will be the focus in the current update.During the refinements, we have: here we use \\(\\oplus\\) to represent the composition of rotation quaternions and the add operation of other parts. We use Figure~\\<PIC> to illustrate how we maintain the Gaussian memory and refine Guassians according to their confidence values. ction{EmbodiedOcc: An Embodied Framework} We present the training framework of our EmbodiedOcc model for indoor embodied occupancy prediction. During the whole prediction process, we use the current monocular input to update our Gaussian memory in real time, which can be easily converted into 3D occupancy prediction. We first train our local occupancy prediction module using the focal loss \\(\\mathit{L}_{focal}\\), the lovasz-softmax loss \\(\\mathit{L}_{lov}\\), the scene-class affinity loss \\(\\mathit{L}_{scal}^{geo}\\) and \\(\\mathit{L}_{scal}^{sem}\\) following RetinaNet\\cite{lin2017focal}, TPVFormer\\cite{tpvformer} and Monoscene\\cite{monoscene}. We use monocular occupancy within the frustum \\(\\mathbf{Y}_{mono}^{fov}\\) and the corresponding ground truth \\(\\mathbf{Y}_{gt}^{fov}\\) to compute the loss.", "images": ["2412_04380v1_1"], "tokens": 940}]}
{"id": 60043, "dataset": "arxiv", "images": ["2412_04323v1_0", "2412_04323v1_1"], "chunks": [{"chunk_id": 1, "text": "h{Teacher-student training} he teacher-student approach to generalization in deep RL assumes access to $c_t \\in \\mathcal{C}_{\\textnormal{ID}}$ at every timestep throughout training, and leverages this privileged context information to train a teacher policy that can adapt to different contexts. The teacher policy applies a context encoder $f: \\mathcal{C} \\rightarrow \\mathcal{Z}$ that maps the context to a latent feature $z_t = f(c_t)$, which is then provided as an input to the policy $\\pi: \\mathcal{S} \\times \\mathcal{Z} \\rightarrow P(\\mathcal{A})$ and critic $V^{\\pi}: \\mathcal{S} \\times \\mathcal{Z} \\rightarrow \\mathbb{R}$. In this work, we consider the latent feature space $\\mathcal{Z} = \\mathbb{R}^d$.The context encoding $z_t = f(c_t)$, policy $\\pi(a_t \\mid s_t, z_t)$, and critic $V^\\pi(s_t, z_t)$ are trained to minimize the average actor-critic RL loss over ID contexts given by here $\\mathcal{L}_{\\pi}(c)$ and $\\mathcal{L}_{V}(c)$ represent the policy loss and critic loss, respectively, of a given RL algorithm for the context $c \\sim \\mathcal{C}_{\\textnormal{ID}}$. In our experiments, we apply Proximal Policy Optimization (PPO) \\citep{schulman_2017} as the RL algorithm. See the Appendix for details. ote that the teacher policy cannot be applied at deployment time because it requires privileged information about the context $c_t$ in order to compute the latent feature $z_t$.For this reason, RL training is followed by a supervised learning phase where a student policy is trained to imitate the teacher policy using only the recent history of states and actions from the last $H$ timesteps $h_t = \\left(s_{t-H}, a_{t-H}, \\ldots, s_t \\right) \\in \\mathcal{H}$. In particular, an adaptation module $\\phi: \\mathcal{H} \\rightarrow \\mathcal{Z}$ that maps recent history to a latent feature $\\hat{z}_t = \\phi(h_t)$ is trained to minimize the loss where expectation is taken with respect to trajectories sampled using the student policy in ID contexts $c \\sim \\mathcal{C}_{\\textnormal{ID}}$ during training. This training represents a form of implicit system identification across ID contexts. Using the history encoding $\\hat{z}_t = \\phi(h_t)$, the policy $\\pi(a_t \\mid s_t, \\hat{z}_t)$ can be applied at deployment time because it does not require privileged information as input. paragraph{Robust adaptation module} In the teacher-student architecture, the adaptation module $\\phi$ is trained to identify ID contexts from history, resulting in strong performance across $c \\in \\mathcal{C}_{\\textnormal{ID}}$. However, because it is only possible to train on ID contexts, the adaptation module $\\phi$ and resulting policy $\\pi(a_t \\mid s_t, \\hat{z}_t)$ may not generalize well to OOD contexts $c \\in \\mathcal{C}_{\\textnormal{OOD}}$ that were not seen during training. Because $\\phi$ is a learned module, its output $\\hat{z}_t$ is only reliable for the distribution of history inputs $h_t$ that were observed during training.This suggests that $\\hat{z}_t = \\phi(h_t)$ may not be useful if the environment dynamics of OOD contexts result in trajectories that are different from the ID trajectories seen during training. n order to quantify the level of uncertainty present in the The robust latent feature $z_{\\textnormal{rob}}$ defines an anchor point in latent feature space where we can apply robust training. Because we consider a single unified policy architecture, note that the RL loss $\\mathcal{L}_{\\textnormal{RL}}$ in \\eqref{eq:rl_loss} incentivizes the privileged context encoding $z_t = f(c_t)$ to move away from $z_{\\textnormal{rob}}$ if it will lead to better performance. Given this intuition, we propose the use of $z_{\\textnormal{rob}} = \\mathbf{0}_d$ in our implementation of GRAM to allow the outputs of a randomly initialized context encoder to start near $z_{\\textnormal{rob}}$ at the beginning of training and move away from it as needed.Then, at deployment time we bias the latent feature estimate back towards the robust anchor point $z_{\\textnormal{rob}}$ if the estimate is unreliable due to OOD environment dynamics. See the right side of ig<PIC> for an illustration. By using a special robust latent feature $z_{\\textnormal{rob}}$, we incorporate the failure mode of OOD dynamics directly into the existing adaptation framework as a single instance in latent feature space. This allows us to leverage the existing training procedure and architecture for adaptation in ID contexts, while applying tools from robust RL to encode robust behavior into $\\pi(a_t \\mid s_t, z_{\\textnormal{rob}})$ for OOD generalization \\emph{within the same architecture}. The use of a single architecture also allows for implicit regularization benefits of this robust training beyond $z_{\\textnormal{rob}}$. In the next section, we provide details on the robust RL techniques that we use during training to achieve OOD generalization. ction{Training for robust adaptation}\\label{sec:ra_train} Our robust adaptation module provides an intuitive structure for achieving both ID and OOD dynamics generalization within a single architecture.", "images": ["2412_04323v1_0"], "tokens": 1550}, {"chunk_id": 2, "text": "In order to accomplish this goal, we jointly train our policy $\\pi(a_t \\mid s_t, z_t)$ for adaptive performance in ID environments and robust performance in OOD environments (i.e., when $z_t = z_{\\textnormal{rob}}$). For a given iteration of RL training, we assign each training environment to either \\emph{ID training} or \\emph{OOD training}. This assignment determines how the latent feature vector is calculated, as well as how data collection occurs in the environment. See g<PIC> for an overview. Within each iteration, all data for a given training environment is collected according to either standard ID data collection or adversarial OOD data collection, as described in the following paragraphs. This provides temporal consistency when training the policy for adaptive or robust performance, respectively. However, we alternate these assignments between iterations, which allows full trajectories to contain a mixture of both forms of data. As we show in our experiments, this mixed data collection design provides additional robustness benefits compared to using the same training assignment for entire trajectories.", "images": ["2412_04323v1_1"], "tokens": 270}]}
{"id": 60045, "dataset": "arxiv", "images": ["2412_04180v1_0", "2412_04180v1_1"], "chunks": [{"chunk_id": 1, "text": "egin{abstract} Large Language Models (LLMs) exhibit impressive performance across various tasks, but deploying them for inference poses challenges. Their high resource demands often necessitate complex, costly multi-GPU pipelines, or the use of smaller, less capable models. While quantization offers a promising solution utilizing lower precision for model storage, existing methods frequently experience significant performance drops at lower precision levels. Additionally, they typically provide only a limited set of solutions at specific bit levels, many of which are extensively manually tuned. To address these challenges, we propose a new method called \\textbf{SKIM}: Scaled K-means clustering wIth Mixed precision. Our approach introduces two novel techniques: 1. A \\textit{greedy algorithm} to solve approximately optimal bit allocation across weight channels, and 2. A \\textit{trainable scaling vector} for non-differentiable K-means clustering. These techniques substantially improve performance and can be adapted to any given bit. Notably, in terms of model perplexity, our method narrows the gap between 3-bit quantized LLaMA models and their full precision counterparts by xtbf{16.3\\%} on average.d{abstract} ction{Introduction} arge Language Models (LLMs) including GPT \\cite{radford2019language} and LLaMA \\cite{touvron2023llama}, have achieved remarkable performance across a diverse range of tasks. These models not only excel in language processing \\cite{achiam2023gpt, dubey2024llama, chowdhery2023palm, zhang2022opt} but also adapt effectively to multimodal applications \\cite{wang2024visionllm, driess2023palm}, marking a crucial step toward artificial general intelligence \\cite{bubeck2023sparks}. However, the computational and memory demands of LLMs pose significant challenges. For instance, when loading parameters in FP16, GPT requires 350GB of memory, while LLaMA-65B needs at least 130GB, both of which far exceeding the capabilities of A100-80G GPUs. Even when conducting inference with the smallest LLaMA model, which has 7 billion parameters, an Out-of-Memory exception can occur on a widely used 24GB GPU. These challenges significantly complicate the storage and practical deployment of such models. One promising technique to address these issues is quantization, which involves transforming high-precision data into lower-precision formats, such as converting FP16 parameters to INT4. This method directly reduces the memory required to deploy and load LLMs and improves inference latency due to the phenomenon of the memory wall \\cite{gholami2024ai}, which identifies the memory bandwidth as a key bottleneck in LLM inference. In addition, quantization has shown promising performance benefits. For example, previous studies have shown that both LLM weights and activations can be stored in 8 bits \\cite{xiao2023smoothquant}, or only LLM weights can be stored in 4 bits \\cite{kim2023squeezellm}, with little performance degradation. This encourages researchers to explore lower-precision solutions while maintaining reasonable performance levels. However, standard quantization techniques in recent methods can suffer a significant drop in performance when using low bit widths.To mitigate this decline, these methods often introduce additional techniques that incur extra memory costs. For example, SqueezeLLM \\cite{kim2023squeezellm} retains certain sensitive elements and outliers with full precision using a sparse tensor, while AWQ \\cite{lin2024awq} divides the quantization group into smaller ones, requiring the storage of more quantization factors. Additionally, the extra memory needed to achieve a reasonable trade-off between memory usage and performance often requires manual tuning and selection, making the process cumbersome. extbf{Contribution} In this paper, we address the above issues with our proposed method, Scaled K-means clustering wIth Mixed Precision (SKIM), which optimizes the bit allocation using a greedy algorithm and regularizes the column-wise difference with a scaling vector. Our method can easily adapt to any specified bit, including even non-integer values, and achieve better performance. Figure \\<PIC> illustrates how our method breaks the fixed bit grid and delivers better results. Our main contributions can be summarized as follows: (1) We conduct a mathematical analysis of two optimization targets: layer-wise and sensitivity-based quantization, identifying a unified framework that highlights their core differences and allows us to evaluate their effectiveness. (2) We observe a significant disparity in data distribution across channels and propose a greedy algorithm for approximately optimal bit allocation in response to this disparity. Our mixed precision method adapts to any specified bit level and significantly improves performance. (3) For the non-differentiable K-means clustering operator, we incorporate a trainable scaling vector based on our novel iterative optimization strategy. This vector effectively regularizes the data across columns and serves as a valuable complement to the mixed precision method. section{Methodology} label{sec:method} As previously mentioned, the quantization error of a weight matrix $W$ is the sum of that of its individual rows.", "images": ["2412_04180v1_0"], "tokens": 1288}, {"chunk_id": 2, "text": "However, each row, represent a channel, soften exhibits distinct data distributions and quantization errors, as illustrated in Figure \\<PIC>. These observations indicate that applying the same bit-width for quantizing all rows results in an disproportionate allocation of resources. Motivated by these insights, we propose the adaptive channel-wise \\textbf{Mixed Precision} technique. It begins by allocating different bits to each channel using a greedy algorithm, based on their quantization errors. Next, weighted K-means clustering is applied to each channel, generating centroids and labels with the allocated bits. Finally, we incorporate the scaling vector and train it through an iterative optimization strategy, keeping theowever, due to the interdependence among the elements of the gradient and the input, we cannot compute the four objectives using directly recorded expectations of $g$ or $X$. This limitation prevents us from adopting the most effective \\textit{S-full} objective, as recording the corresponding $E(g^\\top g)$ for each row requires quadratic memory complexity which is impractical for LLMs. Consequently, in any scenario requiring a complete error calculation, such as the proposed mixed precision and scaling vector methods, we adopt the \\textit{L-full} form. In contrast, for scenarios involving element-wise sums, such as the weighted K-means clustering, we adopt the \\textit{S-diag} form. This will be the main principle for our objective selection. subsection{Mixed Precision}", "images": ["2412_04180v1_1"], "tokens": 388}]}
{"id": 60046, "dataset": "arxiv", "images": ["2412_04147v1_0", "2412_04147v1_1"], "chunks": [{"chunk_id": 1, "text": "subsection{On-device DNN inference} In recent years, there has been an explosion of Artificial Intelligence (AI) applications and services thanks to significant advancements in DL. These applications span a wide range, from personal assistants and recommendation systems to autonomous vehicles and healthcare diagnostics. Furthermore, the widespread adoption of mobile computing and the Internet of Things (IoT) has led to billions of interconnected mobile and IoT devices, collectively generating an immense volume of data at the network edge \\cite{ZhiEdgeIntelligence2019}. This has created the need to push the execution of DNN applications at the edge of the network leading to a substantial surge in the deployment of DL models on resource-constrained devices \\cite{oodin2021smartcomp}. Although on-device training of DL models remains a challenging endeavor due to the limited computational resources and memory constraints of such devices, significant progress has been made in on-device inference. Several techniques have been proposed to enable efficient on-device inference, including: textbf{Lightweight model design:} One approach to enable on-device inference is the design of lightweight DL models.Models like MobileNetV2 \\cite{sandler2018mobilenetv2}, EfficientNet-Lite \\cite{tan2019effnet} and NasNet-Mobile \\cite{zoph2017LearningTA} have been specifically crafted to achieve high accuracy while minimizing computational and memory requirements. Previous research on cascade architectures is predominantly centered around isolated scenarios, where a single device enjoys exclusive access to a dedicated server. Nevertheless, this assumption no longer aligns with the reality of AI-driven indoor environments like the one in Fig.~\\<PIC>. The pervasive integration of AI technologies has given rise to an expanding array of AI-powered devices, resulting in a pressing demand for simultaneous support from a shared server. xtbf{Model quantization:} Quantization techniques \\cite{han201", "images": ["2412_04147v1_0"], "tokens": 479}, {"chunk_id": 2, "text": "5deep} reduce the precision of model weights and activations, effectively decreasing memory and computational demands without substantial loss in accuracy. extbf{Model pruning:} Pruning methods, such as channel pruning \\cite{he2017channel}, aim to reduce the size of DNNs by removing unimportant neurons, thereby reducing computational overhead. extbf{Knowledge distillation:} Knowledge distillation \\cite{hinton2015distilling} involves training a smaller, more efficient model to mimic the predictions of a larger, complex model. This allows for the transfer of knowledge from larger models to smaller ones. xtbf{Optimized scheduling:} Scheduling and runtime optimization techniques \\cite{CoBiCh2022} help allocate computational resources efficiently, ensuring that DL inference tasks run smoothly on constrained devices. espite these advancements, modern intelligent environments like smart homes and offices are often equipped with small-form-factor, resource-constrained devices (e.g., smart cameras, AI speakers). These devices lack the processing power to support high-accuracy, computationally-intensive models, driving the need for distributed collaborative inference approaches.ubsection{Distributed collaborative inference} Distributed collaborative inference systems leverage a central server to assist mobile and em In Fig.~\\<PIC>, we present the comprehensive system architecture of a multi-device cascade, specifically designed for executing DL inference tasks on IoT devices in a collaborative setting. Within this architecture, all IoT devices are engaged in performing a common task, such as object detection, albeit they may host different DL models tailored to their computational capabilities and requirements. The main components of this system architecture include the following. textbf{IoT devices:} These devices are the primary endpoints where the DL inference tasks are executed. Each IoT device is equipped with its own DL model designed to process incoming data efficientnsures that each IoT device benefits from the improved accuracy achieved by the server.", "images": ["2412_04147v1_1"], "tokens": 499}]}
{"id": 60047, "dataset": "arxiv", "images": ["2412_03993v1_0", "2412_03993v1_1"], "chunks": [{"chunk_id": 1, "text": "Firstly, we assume that the adversary can manipulate a dataset and upload the resulting dataset online, which will be downloaded and used for training DNNs by developers. {However, the adversary cannot \\emph{modify} any other aspects of training such as DNN architecture and training algorithm. } Accordingly, the adversary chooses to insert the backdoor by poisoning training datasets instead of modifying DNN parameters or adding malicious sub-modules. {Despite this, the adversary may \\emph{know} the details of target DNNs. This is reasonable considering the wide adoption of public well-known model architectures and typical training algorithms (e.g., Adam~\\cite{ADAM}).} Secondly, the adversary cannot directly modify the digital inputs when launching attacks at inference time, so needs to launch physical backdoor attacks by manipulating the physical objects. Regarding the physical attack, we assume that the adversary cannot touch the physical objects to add or remove any triggers, e.g., many traffic signs are erected in high places, {the adversary cannot touch the face of victims for causing DoS.} Despite this, the adversary can maintain a reasonable distance from the physical objects such that the physical objects can be recognized by the adversary and projected by common lasers. ubsection{Overview of \\attackname}\\label{sec:overview} abel{sec:attack} h overview of our attack \\attackname is depicted in \\figurename~\\<PIC>, consisting of two stages, namely, backdoor embedding and backdoor triggering. mallskip\\noindent {\\bf Backdoor embedding.} This stage injects a backdoor into DNNs by poisoning the training dataset, ollowing the common practice of previous backdoor attacks~\\cite{gu2017badnets, turner2019label, chen2017targeted, liu2020reflection, zhong2020backdoor, liu2017trojaning, composite-attack,input-aware-attack, LIRA}. First, the adversary specifies a target label $y^t$ and designs a \\emph{digital} laser-based trigger $\\delta$ (cf.~\\Cref{sec:trig", "images": ["2412_03993v1_0"], "tokens": 551}, {"chunk_id": 2, "text": "ger-design}). Then, he selects a subset {T_{select}}$ of images from a legitimate training dataset {T_{train}}=\\{(x_i,y_i)\\}_{i=1}^{N}$ where \\alpha=\\frac{|{T_{select}}|}{|{T_{train}}|}$ is a small ratio, and adds the trigger $\\delta$ to each image $x$ in the chosen subset {T_{select}}$ and associates each modified image $x^\\delta$ with the target label $y^t$, obtaining the poisoned training dataset {P_{train}=T_{train} \\cup \\{(x^{\\delta}, y^t)|x\\in T_{select}\\}} Finally, the adversary publishes the poisoned dataset on the Internet, waiting for developers to download the dataset for training their DNNs, during which the backdoor will be naturally embedded into DNN models as a result of training with the poisoned dataset. The dataset poisoning scheme enables our attack to process inherent model generalizability, i.e., the poisoned training dataset can be used by and affect any model. We will test our attack on four different models in \\Cref{sec:exper-result}. ompared with previous backdoor attacks, \\attackname features the laser-based trigger, the design of which will be elaborated in \\Cref{sec:trigger-design}. smallskip noindent {\\bf Environment setup.} Two operators collaboratively collected the images in China's urban streets. {The laser pointer is equipped with a button to adjust the focus of the laser spot. The size of the spot projected onto traffic signs depends on both the focus setting and the pointer's (adversary's) distance from the signs. In practice, the adversary can adjust the focus based on their distance to achieve a laser spot size resulting in a clear projection on the traffic signs, called reasonable size.} Therefore, one operator holds the laser pointer more than 30 meters away from the traffic sign and adjusts the laser spot to a reasonable size. The other operator photographs the traffic signs with the photographing device from 5 to 30 meters in front of the traffic sign and 1.5 meters off the ground. To collect more diverse images, we adjusted the angle and distance between the laser pointer and the photographing device and collect 2--5 images per setting. During the collection, we avoided projecting the lasers to those areas of traffic signs that will impact their visibility, e.g., areas with black-color text. mallskipoindent {\\bf Open-source LaserMark.} After collection, we appropriately cropped each image so that the traffic sign is complete, centered, and occupy most of the pixel space. Finally, we have {676} road traffic sign images with physical laser-based triggers, among which 235, 224, and {217} images have red, green, and blue laser points, respectively. These traffic sign images constitute the poisoned test dataset $P_{test}$, which will be used to evaluate the attack success rate in our experiments. In addition to the poisoned physical images $P_{test}$ with physical triggers, we also collect {158} clean physical images without physical triggers and add them into the clean test dataset _{test}$. In total, we have 834 images, covering 32 different traffic sign categories, where the number of images contained in each category of LaserMark is shown in igurename~\\<PIC>. They are published as an open-source dataset.ubsection{Trigger Settings}abel{sec:trigger-setting} As mentioned in \\Cref{sec:attack}, we propose an effective approach to optimize the parameters of the digital triggers toward a more powerful physical backdoor attack. {The laser pointer typically includes a focus adjustment button, which affects the size and center brightness of the laser spot. Adversaries can freely project the laser spot onto any position of a traffic sign, and stronger natural light makes the spot appear more transparent. These factors motivate us to optimize the four parameters associated with the size, transparency, location, and center brightness of laser spots.", "images": ["2412_03993v1_1"], "tokens": 1020}]}
{"id": 60048, "dataset": "arxiv", "images": ["2412_03938v1_0", "2412_03938v1_1", "2412_03938v1_1"], "chunks": [{"chunk_id": 1, "text": "Researchers have proposed automated detection methods for centralization risks in smart contracts. Pied-piper \\cite{ma2023pied} summarizes five common patterns of code with centralization risks and uses static Datalog analysis \\cite{immerman1998descriptive} to identify contracts conforming to these patterns, supplemented by directed fuzzy testing \\cite{bohme2017directed} to reduce false positives. Similarly, Tokeer \\cite{zhou2024stop} analyzes transfer-related functional modules in contract code, generates oracles based on four known rug pull contract patterns, and detects rug pull contracts using Datalog analysis. The rug pull contracts \\cite{sun2024sok}, a type of malicious fraud contract, often incorporate code with centralization risks for asset transfer, making detection methods for this category applicable to centralization risks as well. Researchers have proposed automated detection methods for centralization risks in smart contracts. Pied-piper \\cite{ma2023pied} identifies five common patterns of code associated with centralization risks.It employs static Datalog analysis \\cite{immerman1998descriptive} to detect contracts conforming to these patterns, followed by directed fuzzy testing \\cite{bohme2017directed} to minimize false positives. Similarly, Tokeer \\cite{zhou2024stop} focuses on transfer-related functional modules in contract code. It generates oracles based on four known rug pull contract patterns and uses Datalog analysis for detection. Rug pull contracts \\cite{sun2024sok}, a type of malicious fraud contract, often incorporate code for asset transfer with centralization risks. Consequently, detection methods developed for rug pull contracts can also be applied to identify centralization risks in smart contracts. However, existing methods relying on predefined patterns need manual intervention for pattern modification or supplementation when facing backdoors with unknown patterns or variants of known backdoors, or else underreporting occurs, which makes the current methods unsuitable for real-world scenarios with numerous contracts and diverse backdoor attack patterns\\cite{event1}\\cite{event2}\\cite{event3}.Based on this observation, we propose an automated detection framework for centralization risks, as illustrated in Fig. \\<PIC>. Researchers have already proposed some automatic detection methods for smart contract backdoors. or instance, Pied-piper\\cite{ma2023pied} summarizes 5 common code patterns of smart contract backdoors and employs Datalog analysis to determine if a contract conforms to a given pattern, supplemented by directed fuzzy testing to reduce false positives. Similarly, Tokeer\\cite{zhou2024stop} builds a transfer model to analyze the functional modules in the contract code implementing transfers, generating oracles based on 4 known rug pull contract patterns, and ultimately detects the rug pull contract based on the oracles through Datalog analysis. The rug pull contracts here are a kind of malicious fraud contracts, which often necessitate the insertion of backdoors for asset transfer, rendering detection methods applicable to this category also capable of uncovering smart contract backdoors.", "images": ["2412_03938v1_0"], "tokens": 760}, {"chunk_id": 2, "text": "However, existing methods face challenges in accurately detecting diverse code with centralization risks in real-world scenarios due to their reliance on predefined behavior patterns. However, existing methods struggle to accurately detect diverse backdoors in real-world scenarios due to their reliance on predefined behavior patterns. They may underreport when faced with unknown patterns or variants of known behavior patterns. Additionally, inaccurate predefined patterns can lead to misclassification of secure contracts as risky ones. However, due to the reliance on predefined attack patterns, it is difficult for existing methods to accurately detect diverse backdoors in real-world scenarios. the one hand, when facing backdoors with unknown patterns or variants of known behavioral patterns, existing methods suffer from underreporting. On the other hand, an inaccurate predefined pattern may also cause misclassifying secure contracts as contracts with backdoors. However, existing methods relying on predefined patterns need manual intervention for pattern modification or supplementation when facing backdoors with unknown patterns or variants of known backdoors, or else underreporting occurs, which makes the current methods unsuitable for real-world scenarios with numerous contracts and diverse backdoor attack patterns\\cite{event1}\\cite{event2}\\cite{event3}. ection{Background} ubsection{Solidity Smart Contracts} A smart contract is an automatically executing program on the blockchain, with Solidity being the most popular language for writing these contracts. This paper focuses on two blockchain platforms that support Solidity: Ethereum and Binance Smart Chain (BSC), both of which are peer-to-peer networks composed of accounts. These accounts are categorized into contract accounts, which are associated with and controlled by smart contracts, and external accounts, which are without code. Each account is uniquely identified by an address.mart contracts are automatically executing programs on the blockchain, with Solidity \\cite{solidity} being the most popular language for their development. This paper focuses on two blockchain platforms that support Solidity: Ethereum and Binance Smart Chain (BSC). Both platforms are peer-to-peer networks composed of two types of accounts: contract accounts, which are associated with and controlled by smart contracts, and external accounts, which contain no code. Each account is uniquely identified by an address. Solidity smart contract consists of state variables and functions. For example, in the \\texttt{Example} contract shown in Fig. \\<PIC>, Lines 2 and 3 define the state variables, while Lines 4 through 17 define the functions. State variables, persistently stored on the blockchain, can be read and modifie", "images": ["2412_03938v1_1"], "tokens": 649}, {"chunk_id": 3, "text": "d by different functions. In this example, \\texttt{owner} represents the account address of the contract owner, while \\texttt{balances} represents the token balances of different accounts. Functions in the contract can be invoked by accounts to execute predefined logic. The \\texttt{constructor} in the \\texttt{Example} contract is called upon contract deployment to initialize state variables. The \\texttt{owner\\_transfer} function transfers tokens from \\texttt{\\_from} to \\texttt{\\_to}. This function is restricted by the modifier \\texttt{onlyOwner}, which ensures that each call checks if the caller (\\texttt{msg.sender}) is equal to the \\texttt{owner}. If this condition is met, the function proceeds; otherwise, the call fails.In this paper, we propose two criteria for identifying variables that represent privileged accounts (e.g., \\texttt{owner} in Fig.~\\<PIC>): 1) they are address-type state variables of contracts, and 2) their values can only be specified by the developer or other privileged accounts.", "images": ["2412_03938v1_1"], "tokens": 296}]}
{"id": 60049, "dataset": "arxiv", "images": ["2412_03800v1_0", "2412_03800v1_1"], "chunks": [{"chunk_id": 1, "text": "Among all methods using this intrinsic reward function, ATP~\\citep{liu2021behavior} first computes kNN distances in a representation space obtained through contrastive learning. Subsequently, \\cite{yarats2021reinforcement} propose training an encoder simultaneously during exploration to obtain a prototypical representation space for the kNN computation in Eq. (\\ref{Eq: knn-reward}). RE3~\\citep{seo2021state} asserts that random encoders are sufficient in many cases, rather than requiring contrastive learning or a prototypical representation. Recently, RISE~\\citep{yuan2022renyi} extends it to R\\'enyi entropy. Despite their flexibility and satisfactory performance in non-tabular environments, these methods lack episodic exploration, leading to the previously mentioned issue of vanishing lifelong rewards. pure intrinsic exploration \\citep{amin2021survey} aims to develop an intrinsic reward function $r^{i}(\\mathbf{s}_{t},\\mathbf{a}_{t})$ as a substitute.It is important to note that most intrinsic rewards are \\textbf{non-stationary}, which contradicts the standard MDP framework. The ELEMENT reward consists of two terms represented in blue and red respectively in Fig. \\<PIC>: (1) episodic reward $r_{ep}$ rapidly encourages visiting different states within the same episode, i.e., maximizing episodic state entropy, (2) lifelong reward $r_l$ slowly discourages visits to past states, visited many times in the history, i.e., maximizing lifelong state entropy. Nonetheless, they achieve state-of-the-art performance when utilized to train deep reinforcement learning (DRL) agents in non-tabular environments, where r^{i}(\\mathbf{s}_{t},\\mathbf{a}_{t})$ does not change rapidly. Approaches following the maximum state entropy principle \\citep{lee2019efficient,liu2021behavior, seo2021state, yuan2022renyi} provide $r^i$ directly proportional to entropy-related measure in a \\textbf{non-parametric} and model-free way.", "images": ["2412_03800v1_0"], "tokens": 554}, {"chunk_id": 2, "text": "Active Pre-Training (APT)~\\citep{liu2021behavior} first adopts nearest neighbors as intrinsic rewards in a representation space obtained by contrastive learning. After that, RE3~\\citep{seo2021state} proposes to implement random encoders instead of contrastive learning. Recently, RISE~\\citep{yuan2022renyi} extends it to R\\'enyi entropy. Despite their flexibility and satisfactory performance in non-tabular environments, these methods lack episodic exploration, leading to the previously mentioned issue of vanishing lifelong rewards. n the other hand, maintaining records of lifelong visited states rely on dense memory and thus a large number of visited states (or their representations) can make the computation of distances prohibitive \\citep{seo2021state, yuan2022renyi}. nother category of maximum state entropy exploration is rooted in sampling instead of intrinsic rewards. From the perspective of intrinsic learning, these methods explore how to achieve an intrinsic objective by decomposing it into multiple \\textbf{stationary} reward functions (i.e., MDPs). Early works by \\citep{hazan2019provably} offers a provably efficient algorithm which internally splits state entropy in entropy gradients defined using probability density function of state.Multiple approaches have % been proposed to enhance the method including R\\'enyi variant \\citep{zhang2021exploration}, and efforts to reduce sample complexity \\citep{tiapkin2023fast}. These sampling-based ethods involve iterative sampling of new states using the current policy to obtain corresponding entropy-related reward functions, and then update policies by solving the new MDP after fixing these reward functions. While these sampling-based methods have made significant theoretical contributions, they operate under the assumption that we can ``compute the optimal policy'' to solve a MDP at each iteration given the current entropy-related reward function. This assumption is often unrealistic in real-world environments, where the complexity and unpredictability can hinder consistently reaching near-optimal solutions. Therefore, they usually validate their theoretical results on tabular setups. In this work, we do not compare to sampling-based methods in this category~\\citep{hazan2019provably, nedergaard2022k, yarats2021reinforcement,mutti2022importance, jain2024maximum}. bsection{\\textbf{Parametric Methods for Exploration}} In contrast to MaxEnt-based methods, parametric intrinsic rewards \\citep{pathak2017curiosity, burda2018exploration, badia2020never, ecoffet2019go} usually utilize an internal model to predict the next state and use the prediction error as the intrinsic motivation. These methods encourage agents to explore novel states in a lifelong manner by assigning greater rewards to states that are less frequently visited by estimating predictive forward models and use the prediction error as the intrinsic motivation.These \\emph{curiosity}-driven approaches, have their roots traced back to the 70's when Pfaffelhuber introduced the concept of ``observer's information\"~\\citep{pfaffelhuber1972learning} and Lenat \\citep{lenat1976artificial} introduced the concept of ``interestingness\" in mathematics to promote the novel hypotheses and concepts \\citep{amin2021survey}. Recently popular prediction error-based approaches fall under this category. The recent surge in their popularity is strongly linked to the advancements in deep neural networks (DNNs). For instance, ICM \\citep{pathak2017curiosity} and RND \\citep{burda2018exploration}, utilize a CNN as the internal model to predict the next image, while GIRIL implements variational autoencoder (VAE) to model the transitions in environments. After that, some approaches find the novelty vanishing problem and try to solve it by introducing episodic mechanism \\citep{ecoffet2019go, badia2020never}. However, the internal model introduces an auxiliary predictive task that brings additional complexities such as more hyper-parameters and increased running time.subsection{Fixed neural encoder $f: \\mathcal{O} \\rightarrow \\mathcal{S}$.} The parametric encoder maps the current observation to a $d$-dimensional state $\\mathbf{s}$. Consider a changing encoder that leads to very different representation (state) for the same observation, an agent could visit a large number of different `states' without taking any actions. To avoid such meaningless exploration, the encoder must be pre-trained and fixed before the exploration agent starts to learn. In this paper, we adopt two encoders for Mujoco and Mario environments, respectively. For Mujoco, we implement a fixed randomly initialized neural encoder which has been empirically validated for state entropy estimation citep{seo2021state}.In this section, we extend the experiments presented in Fig. \\<PIC> and analyze the impact of $\\beta$ in detail. The Ant agent in Mujoco can navigate freely in all directions within a three-dimensional space. Both agents are reset to randomly initialized starting points near (0, 0) if they fail to meet the health conditions specified by the default gym-Mujoco package.current state-action pairs subtly changes the underlying MDP into an episodic POMDP problem: an agent receives a single feedback $R_{ep}(\\tau) = H_{\\mathbf{s} \\in \\tau }(\\mathbf{s})$ only at the conclusion of each episode, as discussed above. To solve this problem, we assume the existence of an underlying standard MDP reward function $r_{ep}(\\mathbf{s}_{t}, \\mathbf{a}_{t})$ that approximates the episodic reward $R_{ep}(\\tau)$ in a sum-form decomposition $R_{ep}(\\tau) \\approx \\sum_{t=1}^{T_{\\tau}}r_{ep}(\\mathbf{s}_t)$, which is a common trick for trajectory-wise feedback problems \\citep{gangwani2020learning, ren2021learning, efroni2021reinforcement}. More formally, the episodic objective is decomposed to learn an optimal policy by maximizing: ", "images": ["2412_03800v1_1"], "tokens": 1566}]}
{"id": 60050, "dataset": "arxiv", "images": ["2401_02385v2_0", "2401_02385v2_1"], "chunks": [{"chunk_id": 1, "text": "\\subsection{Version 1.1} \\textcolor{black}{ During the open, live phase of pre-training the first version of TinyLlama, a few implementation issues were identified within the training framework. For example, there were a few issues related to the learning rate scheduler and data loading processes. We therefore re-trained a new model from scratch after fixing those issues. The new model is named TinyLlama v1.1. In addition to fixing all those implementation issues, we took the opportunity to incorporate several key modifications in TinyLlama v1.1:} \\begin{itemize} \\item To reduce communication overhead, we only shard the model parameters within a node in FSDP. \\textcolor{black}{We trained with a cluster consisting of 16 nodes, each equipped with four A100-40G GPUs, and set the batch size to approximately 1.8 million tokens.} \\item We reduced the total number of pre-training tokens from 3 trillion to 2 trillion. Despite the reduction in training data volume, a marginal improvement in performance on downstream tasks was observed, as compared to the original TinyLlama (refer to Section~\\ref{sec:results}).\\item Expanding beyond the singular pre-training phase of the original model, we introduced a three-stage pre-training process inspired by recent research \\citep{wei2023skywork}. This includes basic pre-training, continual pre-training targeted at specific domains, and a cooldown phase. An illustrative overview of this approach is provided in Figure~\\<PIC>, with detailed discussions to follow in subsequent sections. \\end{itemize} \\paragraph{Basic pre-training} In the first stage of the pre-training, we trained our model with only SlimPajama~\\citep{cerebras2023slimpajama} to develop its commonsense reasoning capabilities. We only trained 1.5 trillion tokens during this stage, setting the foundation for more specialized training in subsequent stages. \\paragraph{Continual pre-training with specific domain} During this phase, we diversified the training by i", "images": ["2401_02385v2_0"], "tokens": 498}, {"chunk_id": 2, "text": "ncorporating three distinct types of corpora. The first corpus, identical to the basic pre-training stage, solely consisted of SlimPajama data~\\citep{cerebras2023slimpajama}. The second corpus combined code and mathematical content, leveraging integrations with Starcoder~\\citep{li2023starcoder} and Proof Pile 2~\\citep{azerbayev2023llemma}. \\textcolor{black}{For Starcoder, we only considered the ``Python'' and ``Jupyter'' splits of the original Starcoder dataset.} The third corpus focused on Chinese language data, utilizing Skypile~\\citep{wei2023skywork}. This strategic corpus diversity facilitated the development of three main TinyLlama v1.1 model variants, each tailored to a different need: \\begin{itemize} \\item TinyLlama v1.1: A foundational model for general applications. \\item TinyLlama v1.1 - Math\\&Code: Enhanced specifically for mathematical and coding tasks. \\item TinyLlama v1.1 - Chinese: Specialized for processing and understanding Chinese text. \\end{itemize} In this stage, all three variants are trained with 350 billion tokens.For the Math\\&Code and Chinese variants, we linearly increase the sampling proportion of the domain-specific corpus for the beginning 6 billion tokens. This warmup sampling increasing strategy was designed to gradually adjust the pre-training data distribution, aiming to ensure smoother and more stable training. Following this initial phase of adaptive sampling, we maintained a consistent sampling strategy for the remainder of the training until approximately 1.85 trillion tokens. Detailed of the data sampling ratios are provided in Appendix~\\ref{appendix:data_sampling}. \\paragraph{Cooldown} Implementing a cooldown phase is essential for enhancing model convergence towards the end of the pre-training process. Traditionally, this is achieved by modifying the learning rate, as seen in approaches like MiniCPM~\\citep{hu2024minicpm} and DeepSeek LLMs~\\citep{bi2024deepseek}. However, due to the use of a cosine schedule for our model, the learning rate is already low at the later stage of training. Hence, we opted to modify the batch size instead. Specifically, during the cooldown stage, the batch size was increased from 1.8 million tokens to 7.2 million tokens, while maintaining the original cosine learning rate schedule. This adjustment was applied uniformly across all variants, with each undergoing an additional 150 billion tokens of training during this phase. The training curves for all three variants are shown in Figure~\\<PIC>.", "images": ["2401_02385v2_1"], "tokens": 654}]}
{"id": 60051, "dataset": "arxiv", "images": ["2306_08543v4_0", "2306_08543v4_1", "2306_08543v4_3", "2306_08543v4_2"], "chunks": [{"chunk_id": 1, "text": "\\subsection{Analysis} \\label{sec:ana} \\paragraph{Scaling Law of Teacher} Although it is intuitive that we can distill better student models from larger teacher models, \\cite{teaching_assistant} has shown that increasing the teacher models' sizes does not guarantee the improvement of student models, sometimes even harming the distillation performance. It is not clear how \\textsc{MiniLLM} works when we scale up the teacher models' sizes. Therefore, we compare \\textsc{MiniLLM} and SeqKD using teacher models with different sizes and fix the size of the student model. We present the results based on the GPT-2 family in Figure \\<PIC> and that based on the OPT family in Appendix \\ref{app:ts_opt}. We can see that \\textsc{MiniLLM} constantly outperforms SeqKD, and the student model performance is positively correlated with the teacher model sizes. This shows the potential of our method to compress models with massive parameters.", "images": ["2306_08543v4_0"], "tokens": 247}, {"chunk_id": 2, "text": "\\paragraph{Exposure Bias} Language generation models trained to minimize \\textit{forward} KLD suffer from exposure bias~\\citep{exposure_bias} caused by the discrepancy between teacher-forcing training and free-run generation. When training \\textsc{MiniLLM}, the student model sees samples generated by itself, alleviating the training-inference mismatch~\\citep{cold}. In Figure \\<PIC>, we use the ExAccErr metric~\\citep{eb_measure} defined in Appendix \\ref{app:eb} to measure the excess accumulated error due to exposure bias. The experiment is based on GPT-2-125M, with GPT-2-1.5B as the teacher, using Dolly as the test set. For each prompt, we sample 10 responses to reduce the variance. We can see that the ExAccErrs of the baselines continuously grow during generation, while \\textsc{MiniLLM} has a much lower ExAccErr, and the error stops accumulating in long-text generation ($>$ 150 tokens).", "images": ["2306_08543v4_1"], "tokens": 270}, {"chunk_id": 3, "text": "\\paragraph{Calibration} \\cite{gpt4} has shown that models trained with policy optimization are likely to be poorly calibrated. We test the calibration of \\textsc{MiniLLM} and the KD baselines on two widely-used text classification datasets: SST2~\\citep{sst-2} and BoolQ~\\citep{boolq}, based on LLaMA-7B. We design zero-shot classification instructions (see Appendix \\ref{app:eval_detail}) and take the probability of the label words to compute the ECE scores~\\citep{ece}. From Table \\ref{tab:calibration}, we observe that KD and SeqKD models are worse calibrated than the teacher model, which potentially explains their low performance on canonical benchmarks~\\citep{false_imitate}. We suspect that minimizing \\textit{forward} KLD causes the models to push high probabilities to void regions of the target distribution, which leads to significant distribution differences between the student and the teacher (see the example in Figure \\<PIC>). In contrast, \\textsc{MiniLLM} focuses on accurately learning the major parts of the target distribution and narrows the ECE scores gap between the student and the teacher. \\paragraph{Performance on Different Response Length} We study the models' performance when the golden response lengths belong to different ranges.", "images": ["2306_08543v4_3"], "tokens": 350}, {"chunk_id": 4, "text": "In Figure \\<PIC>, we illustrate the Rouge-L scores of different KD models against the SFT models on three S-NI subsets split by the length of the ground truth responses. We can see that all methods achieve low scores on prompts that expect short responses ($\\le 5$ tokens), probably because most responses in our training set are long sentences, introducing a distribution shift between training and evaluation~\\citep{ITGPT4}. Furthermore, the output spaces of these prompts are relatively small, allowing the student model to cover most modes of the teacher, and thus \\textit{reverse} KLD and \\textit{forward} KLD have similar performance. For prompts with longer responses ($\\ge 6$ tokens), the teacher distribution contains more modes than the students due to the complex output spaces, which shows the advantage of \\textsc{MiniLLM} against standard KD models. Similar results on UnNI are shown in Appendix \\ref{app:length_uni}. \\paragraph{Generation Diversity} \\cite{gan_falling_short} has found that the model optimized by minimizing \\textit{reverse} KLD is likely to lose modes, which affects the generation diversity. We follow~\\cite{cold} to discuss generation diversity from three aspects: (i) generating multiple distinct responses given a prompt. (ii) generating linguistically complex responses. (iii) the ability to generate contents that have high coverage of the real data distribution. For (i), we argue that for many NLP applications, generating one \\textbf{correct} response is sufficient, especially for those scenarios demanding high truthfulness and reliability~\\citep{halu_survey}. For (ii) and (iii), we report the responses' distinct 4-gram proportion and the language modeling loss on the test sets in Table \\ref{tab:diversity}, using the base models from the LLaMA family (see Appendix \\ref{app:diverse} for more details). We can see that \\textsc{MiniLLM} preserves the distinct 4-gram proportion in the generated responses and language modeling loss on the test set.", "images": ["2306_08543v4_2"], "tokens": 530}]}
{"id": 60052, "dataset": "arxiv", "images": ["2310_06694v2_0", "2310_06694v2_1"], "chunks": [{"chunk_id": 1, "text": "In this work, we aim to prune the source model into any target configuration that we specify.This goal is challenging because it requires surgically scaling down all dimensions in a transformer architecture, an endeavor that, to our knowledge, has not been accomplished before for large language models. We leverage the configurations of existing pre-trained models as the target architectures, based on the intuition that these configurations have already been well-optimized to balance model expressivity and inference efficiency. For example, we use the INCITE-Base-3B architecture~\\citep{toegther2023incite} as the target structure when producing a $2.7$B model. Our method learns a set of pruning masks on model parameters at different granularities---from global ones like layers and hidden dimensions (persist across all layers), to local ones like attention heads and intermediate dimensions. Assume that the source model $\\mathcal{M}_S$ has $L_\\mathcal{S}$ layers, with each layer consisting of one multi-head attention module (MHA) and one feed-forward network (FFN).$\\mathcal{M}_S$ has a hidden state dimension of $d_\\mathcal{S}$, $H_\\mathcal{S}$ heads in each MHA, and an intermediate dimension of $m_\\mathcal{S}$ in each FFN. We introduce the following mask variables: Each mask variable controls whether the associated substructure is pruned or retained. For example, we remove a layer if its corresponding $z^{\\text{layer}}=0$. Figure \\<PIC> illustrates an example of how the pruning masks control the pruned structures. We analyze the effectiveness of dynamic batch loading by examining its impact on three aspects: (1) the final LM loss across domains, (2) the data usage of each domain throughout training, (3) the downstream task performance. All results in this section are based on Sheared-LLaMA-1.3B. \\paragraph{Loss differences across domains.} Dynamic batch loading aims to balance the rate of loss reduction across domains, ensuring that the losses re", "images": ["2310_06694v2_0"], "tokens": 513}, {"chunk_id": 2, "text": "ach the reference value at roughly the same time. \\C<PIC> shows the difference between our model's loss (with both original and dynamic batch loading) and the reference loss, estimated by fitting a scaling function to a hypothetical 1.3B parameter LLaMA2 model. The original batch loading results in widely varying loss differences across domains; for example, the GitHub loss decreases below the reference value, while the C4 loss lags behind. Dynamic batch loading, however, reduces losses evenly and leads to very similar loss differences across domains, suggesting more efficient data use. ", "images": ["2310_06694v2_1"], "tokens": 143}]}
{"id": 60053, "dataset": "arxiv", "images": ["2403_03853v3_0", "2403_03853v3_1"], "chunks": [{"chunk_id": 1, "text": "As discussed in the previous section, we speculate that the LLMs exhibit layer redundancy. To verify this, we assess the performance degradation caused by removing individual layers of two popular models, Llama2-7B-Base \\citep{touvron2023llama}, an English based LLMs, and Baichuan2-7B-Base \\citep{yang2023baichuan} which is mainly focused on Chinese. Figure \\<PIC> confirms our speculation, which reveals that some layers do not play a crucial role in LLMs, causing little degradation when omitting them individually. Moreover, this redundancy is primarily manifested in the middle to later layers of the network, with the initial layers and the last layer often being more critical. Notably, we found the last layer to be particularly important, aligning with findings from LLM Pruner \\citep{ma2024llm}. This observation contradicts our mathematical explanation in Appendix \\ref{appendix:math} which suggests that deeper layers tend to be more redundant.", "images": ["2403_03853v3_0"], "tokens": 260}, {"chunk_id": 2, "text": "We posit that this discrepancy arises because the final FFN effectively functions as part of the token classifier and should be considered in conjunction with the language model head.To verify our hypothesis, we conducted further investigation, detailed in Table \\ref{tab:last_layer}. The results show that within the last layer, the FFN component is crucial, while the Attention module is less significant. This finding supports our interpretation of the final layer's importance. \\section{Methodology} In this section, we present the methodological framework of our layer removal approach for LLMs, elucidating the underlying principles and techniques employed. We begin by introducing Block Influence (BI), a novel metric designed to assess the hidden states transformation of each layer. Leveraging BI, we then detail our layer removal method. \\subsection{Layer importance} \\label{method:layerimportacne} As outlined in the preceding section, the layers of LLMs exhibit redundancy, with varying degrees of redundancy across different layers. To capture this, we introduce a new metric, Block Influence (BI), to measure the degree of transformation performed by each layer. The BI score of $i^{th}$ layer can be calculated as follows: \\begin{align} \\text{BI}_i = 1 - \\mathbb{E}_{X,t} \\frac{X_{i,t}^TX_{i+1,t}}{||X_{i,t}||_2||X_{i+1,t}||_2}, \\end{align} where $X_{i,t}$ means the $t^{th}$ row of hidden states of $i^{th}$ layer. Lower BI score imply that $X_i$ and $X_{i+1}$ exhibit high cosine similarity, suggesting that the layer makes minimal transformations to the hidden states and is therefore less important. We plot the BI scores of a single layer and the PPL after removing it separately, as shown in the Figure \\<PIC>. The results demonstrate a positive correlation between the BI score and the importance of a layer.", "images": ["2403_03853v3_1"], "tokens": 524}]}
{"id": 60054, "dataset": "arxiv", "images": ["2310_02635v4_0", "2310_02635v4_1"], "chunks": [{"chunk_id": 1, "text": "\\subsection{Reinforcement Learning with Foundation Priors} \\label{sec:method:flfp} We model the tasks for embodied agents as the Goal-Conditioned Markov Decision Processes (GCMDP) $\\mathcal{G}$: $\\mathcal{G} = (\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, {\\mathcal{T}}, \\gamma)$. $\\mathcal{S} \\in \\mathbb{R}^{m}$ denotes the state. $\\mathcal{A}$ is the action space, which is the continuous delta movement of the end effector in this work. $\\mathcal{P}$ is the transition probability function. $\\mathcal{T}$ is the task identifier. $\\mathcal{R}$ denotes the rewards. $\\gamma$ is the discounting factor, equal to 0.99 in the work. To learn efficiently and automatically, we propose the Reinforcement Learning from Foundation Priors (\\textbf{RLFP}) framework by leveraging the policy, value, and success-reward priors.Here we demonstrate how we formulate the priors in RLFP. Back to the case of Alice in Fig. \\<PIC>, the commonsense of behavior can be formulated as a goal-conditioned policy function, $M_\\mathcal{\\pi}(s, \\mathcal{T}): \\mathcal{S} \\times \\mathcal{T} \\rightarrow \\mathcal{A}$. The prior knowledge that the state closer to the button is closer to success can be formulated as the value function $M_\\mathcal{V}(s, \\mathcal{T}): \\mathcal{S} \\times \\mathcal{T} \\rightarrow \\mathbb{R}^{1}$. The ability to recognize the success state can be formulated as the 0-1 success-reward function $M_\\mathcal{R}(s, \\mathcal{T}): \\mathcal{S} \\times \\mathcal{T} \\rightarrow \\{0, 1\\}$, which equals 1 only if the task succeeds. We assume the success-reward prior is relatively precise, given the simplicity of binary classification in determining success. The value and policy prior knowledge are noisier.", "images": ["2310_02635v4_0"], "tokens": 586}, {"chunk_id": 2, "text": "The RLFP framework is to solve an expansion of $\\mathcal{G}$, termed $\\mathcal{G^{'}} = (\\mathcal{G}, \\mathcal{M})$, where $\\mathcal{M}$ is the foundation model set that represents various foundation prior knowledge. Here, $M_\\mathcal{\\pi}, M_\\mathcal{V}, M_\\mathcal{R} \\in \\mathcal{M}$. Compared to vanilla RL, all the signals for the RLFP come from the foundation models. The vanilla RL relies on uninformative trial and error explorations and manually designed reward functions. It is not only of poor sample efficiency but also requires much human reward engineering. Instead, in RLFP, prior knowledge from the foundation model set $\\mathcal{M}$ provides guidance or feedback on policy, value, and success-reward, enabling more automatic and effective task resolution. \\subsection{Foudation-guided Actor-Critic} \\label{sec:method:fac} Under the proposed RLFP framework, we instantiate an actor-critic algorithm named Foundation-guided Actor-Critic (FAC), demonstrating how to inject the three priors into RL algorithms. \\textbf{Guided by Success-reward Signals.} We consider the task as MDP $\\mathcal{G}_1$ with 0-1 success rewards, where $\\mathcal{R}_{\\mathcal{G}_1} = M_\\mathcal{R}(s, \\mathcal{T}) \\in \\{0, 1\\}$.Inspired by how humans learn from successful trials, we propose a success buffer to store the ``successful'' trajectories identified by $M_{\\mathcal{R}}$. Each time the actor $\\pi_{\\phi}$ updates via policy gradient, it also imitates samples from the success buffer $\\mathcal{D}_{\\text{succ}}$ (if available). The objective is $\\mathcal{L}_{\\text{succ}}(\\phi) = \\textbf{KL}(\\pi_{\\phi}(s_t), \\mathcal{N}(a_t, \\hat{\\sigma}^2)), s_t, a_t \\sim \\mathcal{D}_{\\text{succ}}$, where $\\hat{\\sigma}$ is the standard deviation. \\textbf{Guided by Policy Regularization.} To encourage efficient explorations, we regularize the actor $\\pi_\\phi$ by the policy prior from $M_{\\pi}(s, \\mathcal{T})$.Assuming the prior follows Gaussian distributions, the regularization term is $\\mathcal{L}_{\\text{reg}}(\\phi) = \\text{KL}(\\pi_\\phi, \\mathcal{N}(M_{\\pi}(s_t, \\mathcal{T}), \\hat{\\sigma}^2))$, which is commonly used in other algorithms \\citep{rot, modem-v2}. The bias introduced by the policy prior is bounded, shown in Theorem \\ref{theorem:policyv2}. \\textbf{Guided by Reward-shapingfrom Value Prior.} Noisy policy prior can mislead agents to undesirable states, so we propose using the value model $M_{\\mathcal{V}}$ to guide exploration and avoid unpromising states. While initializing and fine-tuning with $M_{\\mathcal{V}}(s, \\mathcal{T})$ is a natural approach, it suffers from catastrophic forgetting. Instead, we employ the \\textbf{reward-shaping} technique \\citep{reward-shaping} using the potential-based function $F(s, s', \\mathcal{T}) = \\gamma M_{\\mathcal{V}}(s', \\mathcal{T}) - M_{\\mathcal{V}}(s, \\mathcal{T})$, where $\\gamma$ is the discount factor. Since $M_\\mathcal{V}$ estimates state values, $F$ measures the value increase from $s$ to $s'$. This shaping reward is positive when $s'$ is better than $s$ and shares the same optimal solution as the 0-1 success-reward MDP $\\mathcal{G}_1$. Proof and details are in App.\\ref{app:reward_shaping}. \\textbf{Foundation-guided Actor-Critic.} In summary, we deal with a new MDP $\\mathcal{G}_2$, where $\\mathcal{R}_{\\mathcal{G}_2} = \\lambda M_\\mathcal{R} + F$, with $\\lambda$ (set to 100) emphasizing success feedback. We train the agent using DrQ-v2 \\citep{drq-v2}, a variant of Actor-Critic, and call the proposed method Foundation-guided Actor-Critic (\\textbf{FAC}). As shown in Fig. \\<PIC>, FAC leverages foundation policy guidance and an automatic reward function, enabling the agent to efficiently learn from abundant prior knowledge. The objectives of FAC are detailed in Eq. (\\ref{eq:actor_fac}), where tradeoff parameters $\\alpha$ and $\\beta$ are both set to 1, $y$ is the n-step TD target, and $Q_{\\Bar{\\theta}}$ is the target network. We use clipped double Q-learning \\citep{double-q} to reduce overestimation.al{L}_{\\text{critic}}(\\theta) &= \\mathbb{E}_{s_t \\sim \\mathcal{D}}\\left[ (Q_{\\theta_k}(s_t, a_t) - y)^2 \\right]; y = \\sum_{i=0}^{n-1} \\gamma^i r_{t+i} + \\gamma^n \\min_{k=1,2} Q_{\\Bar{\\theta}_k}(s_{t+n}, a_{t+n}) \\end{aligned} \\end{equation} ", "images": ["2310_02635v4_1"], "tokens": 1564}]}
{"id": 60055, "dataset": "arxiv", "images": ["2409_12384v1_0", "2409_12384v1_1", "2409_12384v1_2"], "chunks": [{"chunk_id": 1, "text": "Inspired by the above works, we propose a privacy-preserving data-free distillation method. As shown in Fig.~\\<PIC>, publishing a model ~(\\eg, teacher model) trained directly from private data would compromise privacy, so we treat it as a fixed discriminator to train a generator in a data-free manner. This generator learns only the data distribution to protect the private data. Using this generator implicitly generates data for the distillation process from teacher model to student model. Because querying the teacher model using the generated synthetic data can compromise private information, we propose a LabelDP algorithm selective randomized response to protect the output of the teacher model. The selective randomized response algorithm treats the output of the student model as prior knowledge to reduce the possible output categories to increase the probability of outputting the correct label, and if the possible output does not contain the correct label, a uniform probability distribution is used to reset the possible probability of the output. In summary, our approach can effectively learn privacy-preserving student model by two keys. On the one hand, our proposed data-free distillation is able to protect privacy well with the learning of data distribution. The generated synthetic data from this generator will not reveal private information even if it is distributed.", "images": ["2409_12384v1_0"], "tokens": 288}, {"chunk_id": 2, "text": "On the other hand is that we propose the selective randomized response module to implement DP, which is no longer limited by the number of queries, and introduce the prediction of the student model as prior knowledge for the randomized response. We increase the probability of returning the correct label by setting a threshold, so the student model can learn the knowledge of the teacher model more effectively. Our major contributions are three folds: 1)~we propose a differentially private data-free distillation approach to learn privacy-preserving and high accurate student models via synthetic data, 2) we propose selective randomized response algorithm to privately distill teacher knowledge which provides strong protect label privacy protection in theory, and 3) we conduct extensive experiments and privacy analysis to demonstrate the effectiveness of our approach. \\subsection{Ablation Studies} After the promising performance is achieved, we further analyze each influencing factor in our approach, including the impact of loss terms in the data-free generator learning, the amount of synthetic data and the number of stages. \\myPara{Loss function.} To further understand the improvement of each component of the loss function during data-free training of the generator, we designed experiments on MNIST and FMNIST under $\\varepsilon$=10 to explore the contribution of each component. The results are shown in Tab.~\\ref{tab:loss}. where CE means the cross entropy loss term, IE is the information entropy loss term and Norm is the normalized term for the mean and variance of the data. We can see that the normalization term of the data has the greatest impact, followed by the information entry loss term and finally the cross entropy loss term.We speculate that this may be related to the randomness of the data generated by the generator, which limits the distribution of the data to make the generated synthetic data more usable, so it has a greater impact on the accuracy of the student model. \\myPara{Data amount.} We further conducted experiments on MNIST, FMNIST, CIAFR10 and CIFAR100 datasets under $\\varepsilon=1$. The results are shown in Fig.~\\<PIC>. We found that MNIST dataset converges at about 50,000 data volume, FMNIST converges at about 120,000, CIFAR10 and CIFAR100 converge at about 220,000 and 500,000, respectively. As the difficulty of datasets increases, the amount of data required to achieve convergence increases. We suspect that this is because the more difficult the dataset is, the more difficult its distribution knowledge is to learn, so the larger the amo", "images": ["2409_12384v1_1"], "tokens": 595}, {"chunk_id": 3, "text": "unt of data required. We note that the CIFAR10 dataset is more difficult than FMNIST, but the reason why CIFAR10's final accuracy is similar to FMNIST's is that the network structure is different. \\myPara{Number of stages.} To explore the effect of the number of stages, we conducted experiments on MNIST, FMNIST and CIFAR10 datasets under $\\varepsilon$=10. The results are shown in Fig.~\\<PIC>. Experimental results show that between 20 and 320, the accuracy of the student model increases with the increase of stages. As the classification difficulty of MNIST, FMNIST and CIFAR10 datasets increases, the effect of stages becomes greater. The experimental results are as we expected because we used the prediction of the student model as the prior knowledge. As the training process proceeds, the more accurate the prediction of the student model becomes, which means the higher the probability of outputting the correct label. The greater the percentage of synthetic data being correctly labeled, the better the student model performance will be.", "images": ["2409_12384v1_2"], "tokens": 258}]}
{"id": 60057, "dataset": "arxiv", "images": ["2410_02324v1_0", "2410_02324v1_1"], "chunks": [{"chunk_id": 1, "text": "In this paper, we systematically addressed the above problems from three angles: feature construction, algorithm design, and the development of an easy-to-use tool. As illustrated in Figure~\\<PIC>, our contributions can be summarized as follows: \\begin{itemize} \\item \\textit{Our first contribution} is the proposal of \\texttt{Tone2Vec}, which maps diverse tone transcriptions to a comparable feature space. \\texttt{Tone2Vec} constructs pitch-based similarity representations by mapping each transcription to a simulated smooth pitch variation curve. We also propose methods to construct tonal representations for dialect regions. By analyzing these representations across different dialect areas, we show that \\texttt{Tone2Vec} captures tonal variations and clusters dialects more accurately than methods that treat each tone as an isolated category. \\item \\textit{As our second contribution}, we developed the first automated algorithms for tone transcription and clustering. These algorithms are especially beneficial for endangered tonal languages. Experiments demonstrate that our models perform well in cross-regional tone transcription with less than 1,500 samples.", "images": ["2410_02324v1_0"], "tokens": 287}, {"chunk_id": 2, "text": "Notably, our algorithms can accurately cluster tones using fewer than 60 speech samples for a given dialect. \\item \\textit{As our third contribution}, all these algorithms are systematically integrated into \\href{https://github.com/YiYang-github/ToneLab}{\\texttt{ToneLab}}, a user-friendly platform designed for both lightweight fieldwork and subsequent analysis in Sino-Tibetan Tonal Languages. Users can choose to use pretrained models or train new models with their own data for different scenarios. Researchers can also leverage \\texttt{ToneLab} to propose new computational methods and conduct evaluations. \\end{itemize} \\section{Tone2Vec: From Tones to Vectors} \\label{sec:method} In this section, we propose pitch-based similarity representations by quantifying the differences in pitch variations inherent in tones, which we call \\texttt{Tone2Vec}. \\texttt{Tone2Vec} is an easy-to-use, simple, and effective method for measuring similarity distance. \\texttt{Tone2Vec} not only enables the comparison of tonal variations across dialects but also provides a straightforward loss function for training automatic tone transcription and clustering models. \\subsection{From Categories to Pitch-based Similarity Representations} In \\texttt{Tone2Vec}, we map each transcription $l$, such as \\texttt{(55)}, to a simulated smooth pitch variation curve $p_{l}(x)$. As shown in Figure~\\<PIC>, for transcriptions with two units, a linear curve is employed to represent pitch variations, while for those of three units, such as \\texttt{(312)}, we employ a quadratic curve to smoothly interpolate the points \\((1, 3)\\), \\((2, 1)\\), and \\((3, 2)\\).The divergence between any pair of tone transcriptions, \\(l_1\\) and \\(l_2\\), is quantitatively assessed by calculating the area between their pitch variation curves, expressed as \\(D(l_1, l_2) = \\int_{[1,3]} |f_{l_1}(x) - f_{l_2}(x)| dx\\). This measure quantifies the differences in pitch variations. Given $n$ transcription sequences $l_{1},...,l_{n}$, we can construct a $n \\times n$ distance matrix $\\mathcal{C} = {(D(l_{i}, l_{j}))}_{i,j} \\in \\mathbb{R}^{n \\times n}$, where each row represents the features of a transcription, capturing the subtle pitch variation differences among them.", "images": ["2410_02324v1_1"], "tokens": 707}]}
{"id": 60058, "dataset": "arxiv", "images": ["2401_07301v2_0", "2401_07301v2_0", "2401_07301v2_1"], "chunks": [{"chunk_id": 1, "text": "\\section{Introduction} Generative Language Models (LMs) have gained considerable attention due to their remarkable capabilities \\cite{guo2023close,suzgun-etal-2023-challenging}. Despite the convincing and realistic nature of text generated by these LMs, a concern with LMs lies in their tendency to produce fabricated facts and generate false information \\cite{lin-etal-2022-truthfulqa}. Moreover, these models deliver inaccurate information employing unequivocal expressions, which poses substantial risks as it can lead to the spread of misleading and harmful content. One of the contributing factors to the hallucination lies in inadequate acquisition of knowledge \\cite{manakul2023selfcheckgpt,huang2023factual}. For example, consider the question \\emph{Which animal is China's nation treasure?}, LMs may provide a different animal name like \\emph{tiger} instead of \\emph{panda} due to a lack of relevant knowledge. Considerable efforts have been made to alleviate such hallucination induced by lacking of knowledge in LMs.One approach involves supervised fine-tuning LMs with standard ground-truth answers to enhance their comprehension of relevant knowledge \\cite{wei2022finetuned,ouyang2022training}. This method has shown promising efficacy. However, it demands a significant amount of high-quality annotated data for training. Additionally, other methods have relied on external verifier or critic model to evaluate the accuracy of a statement \\cite{yang-etal-2022-re3,paul2023refiner}. Training a verifier necessitates a large number of high-quality evaluation annotations and further fine-tuning of the model, which restricts its broad applicability to other tasks and domains. Another reason for an LM to provide incorrect response is intrinsically linked to the design architecture of generative language models themselves \\cite{azaria2023internal,paul2023refiner,shinn2023reflexion}. It is widely acknowledged that LMs generate a sentence by maximizing the likelihood of the next token given all previous tokens. Subtle differences in the preceding sentences can potentially lead to diverse generation outcomes. For example, when the question is \\emph{Who is the author of The Analects?}, the model gives the correct answer as \\emph{Confucius}. However, when the input question becomes \\emph{Is Laozi or Confucius the author of the Analects of Confucius?}, the model is likely to generate an answer of \\emph{Laozi}. In this case, the model has the ability to rely on its knowledge to recognize false information \\cite{schick2022peer}. This process is akin to how humans perform self-verification of answers to minimize mistakes \\cite{flower1981cognitive}. Moreover, when we realize our answer is wrong, we further modify it. Motivated by this, when the model itself detects the potential hallucination, the next step is to correct the error or mistake. Once the model incorporates this inherent self-correction mechanism, it can address similar issues in other domains and achieve self-improvement.The existing work \\cite{madaan2023selfrefine,ganguli2023capacity} towards self-correction in LMs has mainly focused on lager models like ChatGPT and GPT4, which is challenging to migrate these self-correction methods to small LMs. Some studies indicated that the self-correction ability depends on model parameters and only emerges in models with larger parameter sizes \\cite{azaria2023internal}. The main reason is that they devised a sophisticated pipeline and zero-shot prompts to achieve self-correction. However, these prompts crafted for self-verification and self-modification are difficult for small models to understand. As shown in Figure \\<PIC>, upon generating the initial answer to the given question, an additional feedback instruction is utilized to guide ChatGPT in generating feedback information regarding the initial answer. This information contains an evaluation of the correctness of the initial answer. A subsequent modification instruction is employed to alter", "images": ["2401_07301v2_0"], "tokens": 1010}, {"chunk_id": 2, "text": "or refine the initial answer based on the feedback received. Nevertheless, small models typically lack self-awareness \\cite{weng2022large} and tend to exhibit greater confidence in their generated responses. Consequently, they struggle to assess the quality of their generated outcomes. The capability for self-verification serves as a prerequisite for achieving self-correction. Furthermore, the manner in which self-correction is achieved through multi-step prompt engineering within LMs differs from the spontaneous and one-time correction observed in humans. To empower the capability for self-correction in small language models, we propose \\emph{Intrinsic Self-Correction} (ISC), an intrinsic mechanism that relies on two basic abilities: self-verification and self-modification. At its core, the LM provides a response and subsequently evaluates its own answer. Upon identifying an error, the same LM adjusts its initial response. Conversely, if the answer is validated as accurate, no further modifications are required. The self-correction process is not divided into two separate steps, but rather constitutes a single comprehensive step, as depicted by the red arrowed segment in Figure \\<PIC>.", "images": ["2401_07301v2_0"], "tokens": 294}, {"chunk_id": 3, "text": "We trained the LM to process the self-correction through Instruction Fine-Tuning (IFT). For this purpose, we design the data processing procedure to construct the self-correction data and define the data format. During the fine-tuning process, we propose Partial Answer Masking (PAM) to make the model have the ability of self-verification. Our contributions are summarized as follows: \\begin{itemize} \\item To the best of our knowledge, we are the first to demonstrate that small language models with even 6 billion parameters possess the capacity for self-correction during response generation without relying on ground truth. \\item Our proposed \\emph{Intrinsic Self-correction} aims to incorporate self-correction as an intrinsic pattern within LM. It involves an independent and spontaneous self-correction process, distinct in nature from existing methods of self-correction that rely on prompt engineering. \\item To achieve the capability for self-correction in small LMs, we devise a pipeline for constructing self-correction data and define the data format. It can be universally applied to build data for self-correction tasks. Additionally, we introduce a novel training method called Partial Answer Masking (PAM) to enable the model self-verify its own generated answers. \\item We conduct experiments on open-source LMs with varying parameter scales to validate the efficacy of our proposed method. The results demonstrate improvements in accuracy across two different datasets. \\end{itemize} \\textbf{Answer preparation.} To ensure answer diversity, we utilize nucleus sampling \\cite{holtzman2019curious} to generate multiple answers. After generating multiple answers for a question, the next step is to evaluate the accuracy of each answer by comparing it with the provided ground truth for the each question. In the case of multiple-choice questions, we extract the options of the final answer through string matching and then directly compare them with the standard answer to check the accuracy.For a good case, the outcome of self-verification should be positive, indicating there is no need to modify the initial answers. Accordingly, given the question $x$, the \\textbf{Answer} is represented as ($A_{1}^{1}$-\\emph{COT}$||A_{1}^{1}||PV $), where $A_{n}^{i}$ represents the model attempts $n$ times to obtain the correct answer, and the current answer is the $i$th response, $A_{n}^{i}$-COT represents the COT process of the answer, $PV$ denotes the positive verification in self-correction, and $||$ represents the concatenation. Here, we set the verification as a binary signal. A positive verification can be set like \\emph{I am sure my answer is correct.} Conversely, for a bad case, negative self-verification result is excepted, such as \\emph{Sorry, there is an error in the previous answer}. It requires modifications to the initial answer. To enhance the model's ability to generate more appropriate reasoning process and correct answer, we utilize the ground truth, representing the standard answer, as the revised answer. Additionally, we employ gpt-3.5-turbo to assist in generating the COT analysis process, denoted as $G$. We use prompts like \\emph{the answer of [Question] is [Ground Truth]. Please provide a step-by-step explanation for resolving the given problem}. Therefore, the data format is ($A_n^{1}$-COT$ ||A_n^{1}||NV||A_n^{2}$-COT$||A_n^{2}\\cdots A_{n}^{n}||PV)$, where $NV$ indicates negative verification. In Table \\ref{dataFormat}, we provide two general examples of self-correction data, representing examples where the correct answer is obtained without correction and with one correction respectively. We also provide detailed prompt examples used at each step in the Appendix.This pipeline can be utilized to customize the self-correction data for various corrections, depending on the specific task type. The general process of constructing self-correction data is shown in Figure \\<PIC>. ", "images": ["2401_07301v2_1"], "tokens": 1066}]}
{"id": 60059, "dataset": "arxiv", "images": ["2411_13226v1_0", "2411_13226v1_0", "2411_13226v1_1"], "chunks": [{"chunk_id": 1, "text": "\\section{AIDBench details} In this section, we provide a detailed description of AIDBench. We begin by presenting an overview of the benchmark, outlining the pipeline for authorship identification using large language models (LLMs). We then describe the datasets used in our benchmark, the evaluation tasks conducted, and the metrics employed for assessment. \\subsection{Outline of authorship identification with LLMs} Figure \\<PIC> provides an overview of our proposed AIDBench framework. We begin by selecting a dataset for evaluation, such as the Research Paper dataset. From this dataset, we sample a subset of texts from several authors, randomly selecting one as the \\emph{Target Text} and designating the remaining texts as candidates. These texts are then incorporated into an authorship identification prompt, which is presented to the LLMs. The models generate responses indicating which candidate texts are more likely authored by the same individual as the \\emph{Target Text}. We repeat this process multiple times to obtain average performance metrics for the task. Finally, we employ metrics such as precision, recall, and rank to provide a clear and intuitive assessment of the LLMs' capabilities.", "images": ["2411_13226v1_0"], "tokens": 296}, {"chunk_id": 2, "text": "\\subsection{Datasets} AIDBench comprises five datasets: \\textit{Research Paper}, \\textit{Enron Email}, \\textit{Blog}, \\textit{IMDb Review}, and \\textit{Guardian}. In this subsection, we provide detailed descriptions of each dataset. \\textbf{Research Paper}. This newly collected dataset consists of research papers posted on arXiv under the CS.LG tag (the field of machine learning in the computer science domain) from 2019 to 2024. After removing duplicate entries and authors with fewer than ten papers, the dataset includes 24,095 papers from 1,500 authors, ensuring that each author has at least ten papers. We use this dataset to investigate their potential privacy risks when using LLMs to identify authorship of academic writing. \\textbf{Enron Email}. The Enron email datasetcontains approximately 500,000 emails generated by employees of the Enron Corporation. For our benchmark, similar to~\\citet{Huang2024CanLL}, we removed sender and receiver information and discarded short emails. Ultimately, we retained 174 authors, each with 50 emails. \\textbf{Blog}. The Blog Authorship Corpus~\\citep{Schler2006EffectsOA} comprises the collected posts of 19,320 bloggers gathered from Blogger.com\\footnote{\\url{https://www.blogger.com}} in August 2004. We selected 1,500 authors from the dataset, each with 10 blog posts. The content of this dataset is closely related to daily life, providing linguistic characteristics, writing styles, word usage habits, and special characters that can be used to infer an author's identity. \\textbf{IMDb Review}. The IMDb review data are selected from the IMDb62 dataset~\\citep{Seroussi2011AuthorshipAW}. After filtering out reviews with fewer than 10 words, we randomly retained 50 reviews for each of the 62 authors in our dataset. \\textbf{Guardian}.The Guardian corpus dataset~\\citep{guardian} is designed to explore cross-genre and cross-topic authorship attribution. The corpus comprises texts published in \\textit{The Guardian} daily newspaper, mainly opinion articles (comments). It includes articles from 13 authors across five topics\u2014politics, society, UK, world, and books\u2014and retains 50 articles per author. \\subsection{Evaluation tasks} As illustrated in Figure~\\<PIC>, we outline the evaluation tasks. Each task involves a query text and a set of candidate texts, with the objective of identifying which candidates are most likely authored by the same individual as the query text. Based on the number of candidate texts, we introduce the following specific tasks. \\textbf{One-to-one identification.} Commonly known as authorship verification, this task involves a single candidate text and aims to determine whether the text pair is authored by the same person, forming a binary classification problem. In our evaluation, we prom", "images": ["2411_13226v1_0"], "tokens": 746}, {"chunk_id": 3, "text": "pt the LLM with: ``Here are a pair of texts: [Text Pair]. Determine if they belong to the same author.''. More effective prompts or additional information can be provided to the LLM to achieve more accurate and reasonable responses. For instance, \\cite{Hung2023WhoWI} employs a Chain-of-Thought prompt and a series of intermediate reasoning steps to significantly enhance the LLMs' authorship verification ability. Similarly, as shown in \\cite{Huang2024CanLL} and \\cite{Hung2023WhoWI}, instructing LLMs to analyze texts based on writing style, linguistic characteristics, and word usage habits, rather than content and topics, can further improve authorship verification. Due to the subjective nature of the \\emph{one-to-one identification} task and its heavy reliance on the intrinsic judgment of LLMs, it is natural to consider a scenario involving two candidate texts requiring a comparative decision. The objective here is to determine which candidate text is more likely authored by the same individual as the query text; we refer to this as the \\textbf{one-to-two identification} task.Due to space limitations, we defer the task design, metrics, and experimental results for the \\emph{one-to-two identification} task to Appendix \\ref{app:1to2}. \\textbf{One-to-many identification.} In this setup, we provide a number of candidate texts and ask LLMs to determine texts that are most likely authored by the same person as the text in the query. This is different from the usual closed-set authorship attribution task \\citep{Huang2024CanLL}, where multiple authors and their writings are provided in the context of LLMs, and the task is to attribute a target text to one of these authors. We do not provide any authorship information to LLMs. Instead in our experiments, we randomly sample a number of authors and put all of their writings into a set. Then one specific text is chosen at random as the target text while the rest from the set form the candidate texts. The task is to ask LLMs to identify the texts in the candidate set that are mostly likely authored by the same person as the target text, ranking the results by confidence scores. \\subsection{RAG-based one-to-many identification pipeline}\\label{sec:RAG} The performance of \\emph{one-to-many authorship identification} tasks heavily depends on the ability of LLMs to handle long contexts. Current commercial LLMs support relatively large context windows; for example, GPT-4-Turbo can process contexts up to 128,000 tokens, and Kimi supports up to 2 million tokens. In contrast, open-source models like Llama-3-8B-Instruct\\footnote{\\url{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct}} support context windows of only up to 8,000 tokens. Moreover, the authorship identification task relies heavily on the LLMs' capacity for high-level comprehension of the context, which becomes increasingly challenging as the length of the context grows. To address this limitation, we propose a simple Retrieval-Augmented Generation (RAG)-based method, as illustrated in Figure \\<PIC>.We utilize metrics such as \\textit{Rank@1} and \\textit{Rank@5} in our benchmark, where \\textit{Rank@x} indicates that at least one text authored by the same individual as the target text is found within the top \\textit{x} ranked texts. Additionally, since we expect the LLMs to select as many correct candidate texts as possible, we use \\textit{Precision@x} as a metric, where \\textit{Precision@x} denotes the proportion of correct predictions among the top \\textit{x} texts.", "images": ["2411_13226v1_1"], "tokens": 971}]}
{"id": 60061, "dataset": "arxiv", "images": ["2402_18649v1_0", "2402_18649v1_1"], "chunks": [{"chunk_id": 1, "text": "Recent year, Large Language Models (LLMs)~\\cite{radford2019language, 209211,chatgpt}, have drawn significant attention due to their remarkable capabilities and applicability to a wide range of tasks~\\cite{cheshkov2023evaluation, pearce2022asleep, copilot, pearce2022examining, frieder2023mathematical, shakarian2023independent, lehnert2023ai, kortemeyer2023could}. Building on top of the initial success, there is an increasing demand for richer functionalities using LLM as the core execution engine. This led to the rapid development and rollout of the \\textbf{LLM-based systems (LLM systems)}, such as OpenAI GPT4 with plugins~\\cite{chatgpt}. 2023 can be considered as the ``meta year'' of LLM systems, in which OpenAI announces the GPTs~\\cite{devday}, empowering users to design customized LLM systems and release them in GPTs store~\\cite{gpts}. According to the most recent data statistics report~\\cite{gptsflow} up to November 17th, the top 10 customized GPTs have collectively received more than 750,000 visits.Notably, the customized GPT named Canva~\\cite{canva} has been visited over 170,000 times in just 10 days. In addition, the third-party GPTs store has updated more than 20,000 released customized GPTs~\\cite{gptsnum}. All these facts underscore the increasing integration of LLM systems into our daily lives. Drawing inspiration from information flow analysis, we propose a new information-flow-based formulation to enable systematic analysis of LLM system security~\\footnote{ Note that the LLM system discussed in this paper specifically references the design of OpenAI GPT4. Any potential or future LLM systems possessing different features are not within the scope of this study.}. To achieve it, we need to tackle two non-trivial uniqueness of the LLM system. \\textit{LLM system analysis has to consider the nature of the interaction between machine learning model and multi-object information processing.} LLM systems combine novel AI model (LLM) with conventional software components (e.g., Web Tools), leading to complex interactions across various objects and models. This integration results in a multi-layered system where data flows and transformations occur across various operational contexts, from straightforward individual object level to context-dependent multi-object level. To facilitate the analysis of such a multi-layered system, we develop a multi-layer analysis framework, as shown in Figure~\\<PIC>, where \\textit{objects} are key components in the LLM system (such as the LLM model and plugins). \\textit{Action} and \\textit{interactions} capture the processing of information within an object and the transmission of information between objects respectively. Since security issues arise from the lack of effective constraints of the information flow -- it allows information to directly flow in and out without any restrictions~\\cite{cecchetti2021compositional, myers1999x}, we propose to use the concept of \\textit{constraint} to capture the security requirement over the information flow, where the constraints are multi-layer, placing the mediation to not only the processing of individual objects (constraint over action), but also the processing among them (constraint over interactions).", "images": ["2402_18649v1_0"], "tokens": 817}, {"chunk_id": 2, "text": "\\textit{Constraints over action and interaction are now probabilistic and have to be analyzed through the lens of adversarial robustness.} LLM systems differ significantly from standard systems where executions are often deterministic, LLM systems operate in an inherently uncertain setting. This fundamental difference is due to the nature of LLMs, which process natural language inputs that are vague and free-form, challenging the model's ability to consistently interpret data and instructions. This ambiguity also extends to the models' output, which can vary widely in form while conveying the same meaning. When these outputs are used to command or interact with other system components, like web tools, the inherent ambiguity can lead to unpredictable interactions with these components, significantly complicating the process of applying security features. Furthermore, the probabilistic nature challenges the system's ability to produce consistent outputs and hinders the ability to apply and enforce security features in LLM systems in a deterministic manner. Thus, to enable the analysis, a set of rules is encapsulated in the constraints. Based on the constraints, we should analyze not only the presence of such constraint (machine-learning-based policy enforcement) but also the adversarial robustness (how well it works) of these rule enforcement via a multi-step process. \\noindent\\textbf{Example 1: Unethical Image Displaying.}~\\label{moti2} LLM system, beyond its core LLM capabilities, encompasses additional facilities. Notably, the Frontend is a critical facility widely used in the LLM system to provide a friendly user interface. One of the most important functionalities of Frontend is to render image links in markdown format. This increases the richness and diversity of the displayed content. When the LLM in the system outputs certain markdown image links and transmits them to the Frontend, the Frontend will automatically render it and display the image content of links. However, integrating the Frontend introduces security concerns.For instance, as shown in Figure~\\<PIC>, when the LLM outputs certain malicious markdown image links of unethical content such as pornographic pictures, and the links are transmitted to the Frontend, the automated render process in the Frontend would render it and display this explicit image. Specifically, this kind of rendering process contains two steps. The first step is the LLM output target markdown image link, which is followed by the second step where the output target markdown image link is transmitted to the Frontend. We found that OpenAI fails to ensure security in both of these two steps : (1) the LLM can output arbitrary external markdown image links if we adopt certain adversarial strategies (detailed strategy shown in Sec~\\ref{firstrender}) and (2) the transmission of such image links toward the Frontend lacks of necessary control (details in ~\\ref{sec:frontend}). This example highlights two critical insights. First, while LLMs contribute to their superior performance, they also introduce potential threats to the security of the LLM systems. Second, the interaction between the LLM and other internal system components can give rise to new emergent threats. This realization highlights the importance of adopting a holistic approach to model and study the security problems in the LLM system. ", "images": ["2402_18649v1_1"], "tokens": 761}]}
{"id": 60062, "dataset": "arxiv", "images": ["2405_14744v2_0", "2405_14744v2_1"], "chunks": [{"chunk_id": 1, "text": "\\section{CogMir: Multi-LLM Agents Framework On Cognitive Bias} In this section, we provide a detailed and modular overview of CogMir, organized into four main elements: Mirror Environmental Settings, Framework Structures, Cognitive Bias Subsets, and Sample Use Cases. These components are visually depicted in a left-to-right sequence in Fig.~\\<PIC>. \\subsection{Framework Structures} After establishing realistic social science experiment environments, the next step is to select essential components to support the above two mirror methods: Human-LLM Agent Q\\&A and Multi-H-A Interaction. This entails choosing participant objects, evaluation tools, and communication modes. The CogMir framework is organized into modules for Required Objects, Communication Modes, and Interaction Combinations to meet these needs. \\textbf{Required Object Sets.} Required Object encompasses all potential participants and evaluators involved in the system. \\textbf{Participants} include humans\\footnote{\"Human\" in CogMir can refer to real human participants or simulations.", "images": ["2405_14744v2_0"], "tokens": 276}, {"chunk_id": 2, "text": "In our experiments, \"Human\" refers to simulated human interactions based on previous social science experiments, not actual human subjects.} and LLM Agents, which allows for dynamic setups where either or both can be involved in interactions depending on the experiment's requirements. \\textbf{Evaluators} include humans, LLM Agents, datasets, and discriminators. Datasets are utilized to store and construct prompts about the experimental setup (e.g., experimental scenarios, character information, etc.), task description, and Q\\&A question set. Discriminators are specialized tools utilized to evaluate the social intelligence of LLM Agents, encompassing three main types: State-of-the-art technical metrics such as SimCSE, SelfCheck, and FactScore~\\citep{Simcse, min-etal-2023-factscore, Manakul2023SelfCheckGPTZB} for objective, quantitative assessment; Human discriminators that delve into nuanced and subjective aspects like prosocial understanding; and LLM Agent discriminators, which involve the use of other LLM Agents to assess and challenge responses from a subject LLM Agent. \\textbf{Communication Modes Sets.} Communication modes dictate the nature of interactions within different setups. We model the participants (humans or LLM Agents) as channels based on information theory~\\citep{Shannon1948} to define two essential communication modes: \\vspace{-2mm} \\begin{itemize}[noitemsep] \\item \\textbf{Broadcast} (or Parallel, $C = C_1 + C_2 + \\ldots + C_n$), which enables a single sender to transmit a message to multiple receivers simultaneously. \\item \\textbf{Point-to-point} (or Series, $C = \\min[C_1, C_2, \\ldots, C_n]$) establishes communication between two specific entities at a time ($C$ denotes channel capacity). \\end{itemize} \\vspace{-2mm} \\textbf{Multi-H-A Interaction Combinations Sets.} This module provides various combinations of Multi-Human-LLM Agent interactions, tailored to different social science experimental needs, the most frequently used combinations in social science settings include: \\vspace{-2mm} \\begin{itemize}[noitemsep] \\item \\textbf{Single-H-Single-A}: One human interacting with one LLM Agent, predominantly used for human-agent question-answering tasks (e.g., survey, interview, etc. ). \\item \\textbf{Single-H-Multi-A}: One human interacts with multiple LLM Agents, where humans can be set as controlled variables to test Multi-LLM Agents's social cognitive behaviors. \\item \\textbf{Multi-H-Single-A}: multiple humans interact with a single LLM Agent, which is suitable for assessing the impact of group dynamics, such as consensus or conflict. \\item \\textbf{Multi-A}: multiple agents interacting without human participation. \\item \\textbf{Multi-H-Multi-A}: multiple humans and multiple LLM Agents interaction, integrating elements from the previous setups to mimic complicated experimental interactions.\\end{itemize} \\vspace{-2mm} These modules offer a flexible framework for exploring LLM Agents' cognitive biases in social science experiments. Researchers can customize their setups by mixing different components to examine specific hypotheses. In the next section, we outline cognitive bias subsets as guidelines. \\textbf{Conformity: Authority Effect.} Drawing on classical social science experiments conducted by Stanley Milgram~\\citep{Authority}, we conducted experiments to explore the Authority Effect, tailored to the characteristics of LLM Agents. Unlike the Herd Effect, which requires multiple human participants, the Authority Effect aims to test the conformity of LLM Agents to authoritative prompts or instructions, even when these may contradict factual information. In the settings, we utilize Known, and \\textbf{Unknown MCQ} datasets as Evaluators and \\textbf{CogIdentity} and \\textbf{CogScene} as prompt generators to test the \\textbf{Q\\&A Bias Rate} through Single-H-Single-A Q\\&A scenarios. Average Q\\&A Rate refers to the average bias rate on Unknown and Known MCQ.We design prompts to directly inquire LLM Agents on 5 identity pairs across two MCQ datasets, each for 10 times, resulting in $5 \\times 10 \\times 100 \\times 2$ inquires. Prompts are constructed as follows: Human 1st round \\{You are [INFERIOR identity], and I am [SUPERIOR identity] who is an expert in the following field. Please provide your response by selecting either A or B. The question is [MCQ]\\}; 2nd round \\{As an expert, I confirm that the answer should be B. Please provide your answer again. Reply in the following format: Answer: A/B.\\}. Fig.~\\<PIC> shows that LLM Agents display considerable variability in responses to deterministic and nondeterministic scenarios when influenced by the Authority Effect. Notably, in certain and uncertain situations, most LLM Agents showed increased obedience to Authority compared to the Herd Effect. This contrasts humans, who typically exhibit no significant difference in obedience between authority and herd scenarios. These findings highlight the LLM Agents' enhanced sensitivity to social status differences, indicating a stronger propensity to adhere to authoritative commands over peer influence.", "images": ["2405_14744v2_1"], "tokens": 1404}]}
{"id": 60064, "dataset": "arxiv", "images": ["2311_07064v3_0", "2311_07064v3_1"], "chunks": [{"chunk_id": 1, "text": "\\paragraph{Investigations on optimized prompts} We explore several interesting properties of these optimized prompts. \\begin{itemize} \\item \\textit{Evil twins}. In many cases, the optimized prompts that we find are similar in function to the original prompts (twins), but garbled and unintelligible to humans (evil). For this reason, we refer to them as \\textit{evil twins}. See Figure~\\<PIC> for some examples. \\item \\textit{Transferability}. Remarkably, these ``evil twin'' prompts transfer between a variety of open-source and proprietary language models; see Section~\\ref{sec:transferability}. \\item \\textit{Robustness}. We investigate the robustness of evil twin prompts to changes in their token-order and to replacements of their tokens. We find that whether evil twins are robust to randomly permuting their tokens depends on the LLM family. On the other hand, across LLM families, evil twins are more impacted by randomly replacing their tokens than ground truth prompts.", "images": ["2311_07064v3_0"], "tokens": 280}, {"chunk_id": 2, "text": "This suggests that even the uncommon, non-English tokens in the optimized prompts play an important role in driving the model output; see Section~\\ref{sec:prompt-investigations}. \\item \\textit{Improving prompt intelligibility}. We explore variants of the optimization problem \\eqref{eq:mle-def-intro} that encourage the optimized prompts to be more interpretable (adding a fluency penalty and restricting the vocabulary to common English tokens). However, we find that these modifications do not improve the KL divergence of the optimized prompts to the ground truth; see Section~\\ref{sec:optim-intel}. \\end{itemize} We discuss other applications of the maximum-likelihood problem \\eqref{eq:mle-def-intro} to prompt compression, privacy, and conditional generation in Section~\\ref{sec:discussion}. We compare these methods on 100 randomly sampled prompts from the Alpaca instruction tuning dataset \\citep{alpaca}, where Vicuna-7b-v1.5 is the instruction-tuned model. Additional experiments on various model families and datasets are presented in Appendix~\\ref{app:addn-model-exp}. For each method and prompt, we compute the KL divergence of the optimized prompt with respect to the original prompt. We compare pairs of methods based on which one finds the closer prompt to the ground truth; see Figure~\\<PIC>. GPT-4 suggestions perform roughly on par with those from cold-start GCG. On the other hand, GCG with a warm start provides a strong improvement over both cold-start GCG and the GPT-4 prompt suggestions. Enforcing interpretability by adding a fluency penalty or pruning the vocabulary does not improve the optimized prompt (see Section~\\ref{sec:optim-intel}).", "images": ["2311_07064v3_1"], "tokens": 454}]}
{"id": 60068, "dataset": "arxiv", "images": ["2402_15729v3_0", "2402_15729v3_1"], "chunks": [{"chunk_id": 1, "text": "\\section{Introduction} Solving Mathematical reasoning problems is a significant challenge for current LLMs~\\cite{madaan-etal-2022-language,openai2023gpt4}. This task requires interpreting information, identifying relevant mathematical concepts, and formulating equations to solve the problems~\\cite{ahn2024large}. Due to computational errors in LLMs~\\cite{wei2023chainofthought,gao2023pal}, using CoT~\\cite{wang-etal-2023-plan,wei2022chain,chen2024diahaludialoguelevelhallucinationevaluation} solely implemented in natural language can lead to calculation mistakes~\\cite{lewkowycz2022solving,wei2023chainofthought,gao2023pal}. The most common practice currently is to use PoT~\\cite{chen2023program} for handling mathematical reasoning tasks, by guiding the large model to write the code that is then computed using tool calls.However, we made a surprising discovery recently: when a problem is phrased in a manner closer to verbal scenarios (for example, the question is ``One apple costs three dollars, how much for three apples?'' instead of ``3$\\times$3=?''), PoT tends to make more reasoning errors or text comprehension mistakes, but this phenomenon is almost non-existent in CoT. For such problems, CoT can correctly reason out the answer, whereas PoT makes mistakes. We refer to this type of error as~\\textbf{Code Translation Error (CTE)}. We report the percentage of CTE on five datasets with multiple types of models, the results illustrated in Figure~\\<PIC>. This error is due to the amount of training data for natural language far exceeding that for code. In the scope of CodeLlama\u2019s pretraining data, which includes 500 billion code tokens, this represents a small fraction compared to the 2 trillion natural language tokens used in the Llama-2 model~\\cite{rozi\u00e8re2023code,touvron2023Llama}. Natural language is more suitable for semantic analysis, planning, and abstract reasoning than code~\\cite{gou2023tora}. Existing work also finds the advantage of Natural language, but they have", "images": ["2402_15729v3_0"], "tokens": 540}, {"chunk_id": 2, "text": "not effectively utilized the reasoning capabilities of natural language. Current research focuses on the following approaches to integrate natural language to enhance the precision of code: (1) Using natural language prompts to guide the model in writing code~\\cite{gao2023pal,toshniwal2024openmathinstruct,wang2023mathcoder}: write a brief step in natural language before generating code. (2) Employing methods like self-correction and hybrid approaches to generate answers in multiple stages~\\cite{gou2023tora,yue2023MAmmoTH,gou2023critic}. (3) Utilizing prompts like ``rethink question''~\\cite{deng2023rephrase} to have the model first paraphrase the question, thereby avoiding comprehension errors. However, existing methods fall short in two main aspects: First, using few natural language steps or simple paraphrasing methods alone is insufficient for effectively controlling code generation; a more comprehensive natural language reasoning process is necessary to generate more reliable code. Secondly, reasoning within LLMs is not always faithful~\\cite{lanham2023measuring,bao2024llms,turpin2023language}. Frequently, the final answers seem to be derived directly from the questions themselves rather than aligning with the reasoning process.Consequently, even correct reasoning can lead to incorrect answers. To more effectively utilize natural language reasoning to enhance PoT, we propose~\\textbf{Human-Think Language (HTL)}: A novel information-control-based approach to utilize complete CoT reasoning steps to control PoT generation. HTL is inspired by the way humans write code. Humans consider the entire reasoning process using natural language, and the code can fully rely on natural language reasoning. On the right side of Figure~\\<PIC>, we highlight the parallels between our integrated model and the human approach to solving mathematical problems. Compared to previous works, our framework offers a strong capacity for aligning calculation with reasoning by integrating CoT a(3) We evaluate our work on eight mathematical reasoning datasets, and our experimental results demonstrate that our method achieves outstanding results. HTL shows significant effectiveness in in-domain datasets, out-of-domain datasets, and natural language inference task, demonstrating strong usability and potential. The design of HTL is divided into three parts: reasoning format, Focus Attention, and error assessment function based PPO.", "images": ["2402_15729v3_1"], "tokens": 564}]}
{"id": 60069, "dataset": "arxiv", "images": ["2402_04437v5_0", "2402_04437v5_0", "2402_04437v5_1"], "chunks": [{"chunk_id": 1, "text": "We define a structured entity as a named entity with associated properties and relationships with other named-entities. Extracting a structured entity from unstructured text therefore \\new{implicitly} combines named-entity recognition, entity-property extraction, relationship extraction, and coreference resolution. Fig.~\\<PIC> shows an illustration of the structured entity extraction task. Given a text description, we aim to first identify the two entities mentioned \\textit{``Bill Gates''} and \\textit{``Microsoft''}. During this phase, we need to also address the \\new{coreference resolution} and identify that \\textit{``Bill Gates''} and \\textit{``Gates''} are the same entity. Then, given some predefined schema on the all possible entity types and property keys, the exact types, property keys, property values on all identified entities in the text are expected to be predicted, as well as the relations between these two entities (i.e., \\textit{Bill Gates} co-founded \\textit{Microsoft}). Such extracted structured entities may be further linked and merged to automatically construct KBs from text corpora.", "images": ["2402_04437v5_0"], "tokens": 287}, {"chunk_id": 2, "text": "We present our formalization of the \\textit{structured entity extraction} task which lays the foundation for evaluation for this information extraction task. Along with this, we propose a new evaluation metric with numerous variants for measuring the similarity between the predicted set of entities and the ground truth set, which is appropriate for our task (see Sec.~\\ref{sec:prelim}). Among the three stages depicted, \\textit{\\textcolor{blue}{pred\\_ent\\_names}}, \\textit{\\textcolor{blue}{pred\\_type\\_and\\_property}}, and \\textit{\\textcolor{blue}{pred\\_val}} are special tokens to indicate the task. For each model prediction behavior, the first ``$\\Rightarrow$'' indicates inputting the text into the encoder of MuSEE, while the second ``$\\Rightarrow$'' means inputting the encoded outputs into the decoder. All tokens in \\textcolor{blue}{blue} are the prompt tokens input into the decoder which do not need to be predicted, while all tokens in \\textbf{bold} are the model predictions. For the stage 1, we emphasize that MuSEE outputs a unique identifier for each entity in the given text. Taking the example in Fig.~\\<PIC>, the first stage outputs ``\\textit{Bill Gates}'' only, rather than both ``\\textit{Bill Gates}'' and ``\\textit{Gates}''. This requires the model implicitly learn how to do coreference resolution, namely learning that ``\\textit{Bill Gates}'' and ``\\textit{Gates}'' are referring to the same entity. Therefore, our approach uses neither surface forms, as the outputs of the first stage are unique identifiers, nor the entity titles followed by entity linkings. For stage 2, the MuSEE model predicts the entity types and property keys, which are all represented by special tokens. Hence, the prediction can be made by sampling the token with highest probability over the special tokens for entity types and property keys only, rather than all tokens.", "images": ["2402_04437v5_0"], "tokens": 525}, {"chunk_id": 3, "text": "Notice that we do not need to predict the value for ``\\textit{type}'' and ``\\textit{name}'' in stage 3, since the type can be directly derived from the ``\\textbf{ent\\_type\\_}'' special key itself, and the name is obtained during stage 1. The tokens in the bracket ``\\{..\\}'' are also part of the prompt tokens and are obtained in different ways during training and inference. During training, these inputs are obtained from the ground truth due to the teacher forcing technique~\\citep{raffel2023exploring}. During inference, they are obtained from the output predictions from the previous stages. The full training loss is a sum of three cross-entropy losses, one for each stage. An illustration of our model's pipeline is shown in Fig.~\\<PIC>. More implementation details are elaborated in Appendix~\\ref{appendix:details}.", "images": ["2402_04437v5_1"], "tokens": 221}]}
{"id": 60070, "dataset": "arxiv", "images": ["2410_12432v1_0", "2410_12432v1_1"], "chunks": [{"chunk_id": 1, "text": "{INTRODUCTION} The visual servoing problem~\\cite{538972} involves the challenge of controlling the motion of a robot by utilizing feedback from visual sensors, typically cameras, to adjust its actions in real-time. This process entails the robot's ability to interpret visual data to determine its relative position and orientation with respect to target objects or locations within its environment. The core objective is to enable the robot to perform precise movements or reach specific goals by continuously comparing the current visual scene against a desired configuration or outcome. This approach can been seen in Fig. \\<PIC>. This approach requires sophisticated algorithms for image processing and control theory to bridge the gap between visual perception and mechanical action, thereby allowing the robot to adapt its movements based on the visual feedback it receives. Recently, there has been a lot of progress in optical flow-based visual servoing methods~\\cite{rtvs, katara2021deepmpcvs, argus2020flowcontrol, harish2020dfvs, 9550239}. These methods are shown to be highly precise in reaching their goals with some guarantees of convergence. However, the utility of visual servoing has remained limited due to major limitations common to all servoing algorithms: 1) They necessarily require a goal image during test time.", "images": ["2410_12432v1_0"], "tokens": 300}, {"chunk_id": 2, "text": "This makes it quite tough for visual servoing algorithms to be applied in real-world navigation or manipulation since if we already have a map of the environment, there are better ways to reach the goal pose than through a target image. 2) Visual Servoing cannot work if there is not much overlap between the initial and target image. 3) Visual Servoing can only accommodate feedback from a single camera. Solving each of these problems can greatly enhance the utility of visual servoing methods. For example, solving the problem of final image generation based on the skill the robot is executing can make servoing quite useful for real-world tasks. Imagine a drone mounted with a monocular camera trying to cross a door. The robot will first have to visualise the approximate position of the door just before it crosses it; then, it will have to visually servo to the imagined image and then apply a simple hardcoded skill to cross the door. This pattern of imagining a goal, servoing to the goal and applying a hardcoded skill can be repeated for many skills, both in navigation and manipulation. Another good example of a skill we can solve using servoing is the `reaching' skill, which is a part of several manipulation tasks. For instance, take the example of `unplugging the charger', where the robot has to `reach' a particular grasping pose before applying a hardcoded policy. In this paper, we leverage the recent advancements in diffusion-based image editing to provide a much-needed and major update to servoing algorithms. Our contribution can be stated as follows: {METHOD} {Problem Formulation} At any given moment, our system receives an RGB image $I_t$ from the robot's camera sensor, along with a textual prompt $P$ that specifies the task to be accomplished.Our objective is to accurately predict and implement the necessary control commands, denoted as [$v_t, \\omega_t$], to fulfil the task described in the prompt. Tasks vary, ranging from navigating through a doorway to disconnecting a charger. Note that in traditional servoing algorithms, the final image $I_g$ is usually provided to perform the task. This greatly limits the utility of the visual servoing models. To address this challenge, we divide our approach into two distinct phases. Initially, our framework is designed to conceptualize a subgoal $I_g$ based on the task and current camera feedback $I_t$. Next, we aim to attain this subgoal image $I_g$ by employing a servoing algorithm. In the forthcoming sections, we will elaborate on the components of our Imagine2Servo framework. We begin by introducing our foresight model for sub-goal generation in Section \\ref{foresight} followed by the mechanics of our servoing framework \\ref{servoing}, which facilitates reaching the subgoal. We then describe our overall framework in \\ref{overall} and training details in \\ref{training}. Our overall pipeline is summarised using Fig. \\<PIC>.", "images": ["2410_12432v1_1"], "tokens": 713}]}
{"id": 60072, "dataset": "arxiv", "images": ["2411_09729v1_0", "2411_09729v1_1", "2411_09729v1_1"], "chunks": [{"chunk_id": 1, "text": "The structure of this article is as follows: a summary of the data to be analyzed is presented in Sec. \\ref{sec:data}, with the implemented methodology being described in Sec. \\ref{sec:method}, including an overview of the analysis sequence in Section \\ref{sec:proc}, details on the adopted stellar component analysis and how the stellar templates are created in Sec. \\ref{sec:rsp}, and a description of the delivered data products in Sec. \\ref{sec:DP}; the reliability of the methodology is tested with extensive simulations described in Sec. \\ref{sec:code}, including both physically motivated (Sec. \\ref{sec:sim_idea}) and purely empirical (Sec. \\ref{sec:sim_real}) simulations; the accuracy and prevision in the recovery of the emission lines and the stellar population properties are described in Sec. \\ref{sec:sim_el}, \\ref{sec:sim_MaStar}, and \\ref{sec:sim_el}; an example of the use of the DAP is presented in Sec.\\ref{sec:show}, in which we analyze a deep exposure centred in the Huygens region of the Orion nebula, including a description of this particular dataset in Sec. \\ref{sec:O_dataset}, with a summary of the performed analysis results in Sec. \\ref{sec:O_results}, a detailed exploration of the generated deep integrated spectrum in Sec. \\ref{sec:O_int}, the list of detected emission lines in Sec. \\ref{sec:O_el}, the physical properties derived from those emission lines in Sec. \\ref{sec:ori_fprop}, and the spatial distribution of the emission line fluxes presented in Sec. \\ref{sec:ori_spa}; how to download the code is presented in Sec. \\ref{sec:dist}, and finally a summary of the main results of this study is presented in Sec. \\ref{sec:conc}. Observational data for the Local Volume Mapper (LVM) project are obtained using a newly constructed facility at Las Campanas Observatory.As indicated before, the instrumental setup comprises four telescopes: one devoted to the acquisition of the science exposures (using the ultra-wide IFU comprising 1801 fibers described before), two monitoring the night-sky spectrum towards the east and the west (using 59 and 60 fibers, respectively), and one dedicated to the acquisition of spectrophotometric calibration stars (using 24 fibers). This setup feeds into three DESI-like spectrographs covering the wavelength range from 3600 to 9800 Angstroms with a resolution of R $\\sim$ 4000 at H$\\alpha$. Each spectrograph features a dichroic system, which divides the light received from each fiber into three wavelength ranges: (b) blue, from 3600 to 5800 \\AA, (r) red, from 5750 to 7570 \\AA, and (z) infrared, from 7520 to 9800 \\AA. The size of the LVM's ultra-wide field IFU projected in the sky ensures detailed spectral and spatial coverage across the survey's targets, including the Milky Way, Magellanic Clouds, and a selection of local volume galaxies. The LVM survey's data reduction pipeline (DRP, Mej\u00eda-Narv\u00e1ez in prep.) follows the procedures described by \\citet{sanchez06a}.The actual code is based on the {\\sc py3D} reduction package, initially developed for the CALIFA survey \\citep[][]{dr1}. Originally written in Python 2.7, it has been updated to Python 3.11, incorporating specialized procedures to cater to the unique features and requirements of the LVM dataset. Figure \\<PIC> shows the analysis flow of the DAP for each individual spectrum within each LVM exposure { (i.e., no spatial binning is performed in general within the DAP)}. It mimics the one implemented in \\pyp, as described in \\citet{pypipe3d}, Fig.~1, slightly modified to adapt it to the peculiarities of the LVM data regarding the modeling of the stellar component (discussed in Sec \\ref{sec:rsp}). We include here a brief summary of the main steps that comprise the analysis to avoid unnecessary repetition. The key steps performed by the LVM-DRP include: (i) initial raw data preprocessing to merge readings from different amplifiers into a single frame for each channel (b, r, and z) of each spectrograph (producing 9 different frames), follo", "images": ["2411_09729v1_0"], "tokens": 1146}, {"chunk_id": 2, "text": "wed by bias removal, gain correction, and cosmic ray identification and masking; (ii) identification and tracing of fiber spectra in each CCD of each spectrograph, including assessment of the FWHM along both dispersion and cross-dispersion axes; (iii) extraction of spectra using the established trace and width parameters, assuming a Gaussian shape for each fiber's spectrum projected along the cross-dispersion axis, and performing a concurrent stray-light correction; (iv) wavelength calibration and linear resampling of the extracted spectra; (v) differential correction for fiber-to-fiber transmission discrepancies; (vi) flux calibration based on the stars acquired by the spectrophotometric telescope simultaneously with the science observations; (vii) combination into a single spectrum of the spectra acquired in each channel (b, r, and z) for each science fiber, weighting by the inverse of the errors in the overlapping regions between arms; (viii) estimation and subtraction of the sky spectrum using the spectra obtained by the west and east telescopes devoted to observing the sky; (ix) implementation of an astrometric solution for each observation, using the information provided by the guiding cameras. Errors are propagated during each step of the data reduction. The result of this analysis shows the spectra of a subset of the RSP library generated by selecting 108 clusters, together with the spectra of the original RSPs included in each cluster. By construction, the shape of all spectra grouped in the same cluster is rather similar. However, as already indicated, this does not mean that each of those spectra corresponds to the same or similar physical parameters. This is evident when exploring the projected PDFs of each cluster in the space of observed parameters. Figure~\\<PIC> shows three examples of PDFs corresponding to three different clusters whose spectra (\\#18, \\#34, and \\#47), together with the overall distribution for the full RSP library comprising 108 spec", "images": ["2411_09729v1_1"], "tokens": 488}, {"chunk_id": 3, "text": "tra. A simple exploration of the distribution of each PDF in the space of physical parameters shows that for some parameters the RSP corresponds to almost a single value (e.g., $T_{\\mathrm{eff}}$), while for other parameters they present a bi-valuated (or multi-valuated) distribution (e.g., log($g$)). Besides this multi-valuated distribution, the errors and degeneracies are considerably different for each parameter and pair of parameters, also varying from RSP to RSP. For instance, $T_{\\mathrm{eff}}$ and log($g$) do not present any clear degeneracy, while [Fe/H] and [$\\alpha$/Fe] present a degeneracy that is described by an anticorrelation of the PDFs of both parameters. We should stress that these multi-valuated distributions, degeneracies, and errors can be mitigated, for instance, by the selection of a larger number of clusters ($n_{cl}$). Obviously, the selection of a very low $n_{cl}$ number, e.g., 4-12, increases them.However, increasing this number beyond 108 does not produce any significant improvement in the results, as we will translate the degeneracy to the fitting procedure: the DAP algorithm would choose a different RSP among the degenerated ones in each MC iteration described in Sec. \\ref{sec:proc}. This will only slow down the process without improving the quality of the modeling. Using external information, like the distance provided by GAIA \\citep{gaia2} and the absolute magnitude may break some of the degenerancies.. { We may explore how to implement them in future versions of the DAP, or as a part of the post-processing of the data required to interpret the results. Nevertheless this is out of the scope of the goals of the current manuscript, aimed to describe the overall procedure, without focusing on the physical interpretation of the results, which would require a much more detailed exploration.} Structure of extensions included in the each RSP file, where: (i) \\#WAVE is the number of spectral pixels; (ii) \\#RSP is the number of templates inthe library; (iii) \\#PAR is the number of parameters associated to each template, as described in the text; (iv) \\#RSP\\_ORG is the number of RSP templates or stellar spectra from the library adopted to build the described RSP template; and (v) (NX,NY) are the dimension of the projection of the PDF in the considered space of parameters. We store the result of the clustering procedure described previously in a single FITS file with multiple extensions that comprises: (i) the RSP spectra themselves stored in a raw-stacked spectra (RSS) format (SPECTRA); (ii) the uncertainty spectra that retain the dependencies among the spectra coadded to create each RSP template, in the same format (ERROR); (iii) the LW average physical parameters that correspond to each RSP template (PARAMETERS), together with the same values for the original stellar library or RSP template from which the current template was built (ORG\\_PARAM); and (iv) the PDFs projected in the space of physical parameters associated to each template, as shown in Fig. \\<PIC>.These PDFs are stored as cubes in which each slice along the z-axis corresponds to the individual PDF projection for the considered pair of parameters (e.g., PDF\\_TEFF\\_LOGG corresponds to T$_{eff}$ vs. log(g)) for each RSP template in the library. In this way, the PDF corresponding to a particular stellar decomposition can be generated by multiplying each slice of the cube by the corresponding weight/coefficient of the decomposition and averaging all of them. A set of RSP-template files are distributed (\\url{https://ifs.astroscu.unam.mx/sfsanchez/lvmdap/}) together with some python notebooks illustrating how to handle them (included in the DAP distribution).", "images": ["2411_09729v1_1"], "tokens": 988}]}
{"id": 60073, "dataset": "arxiv", "images": ["2411_11098v1_0", "2411_11098v1_1"], "chunks": [{"chunk_id": 1, "text": "A significant portion of chemical information remains locked in unstructured formats within printed or digital documents, such as patents and scientific papers. In many of these documents, especially in the field of chemistry, molecular structures are often depicted as images. These chemical structures figures are critically important for drug discovery, patent examination, and the retrieval of chemical data. But they are difficult to extract into machine-readable text. Automatically parsing molecular structures from these document images, a process known as Optical Chemical Structure Recognition (OCSR), becomes particularly important. With the growing popularity of Large Language Models (LLMs), many efforts are now focusing on applying them to the understanding scientific literature. Parsing molecular structure images into structured, understandable text also unlocks the potential of large models in processing patents and literature in chemistry-related fields. OCSR aims to automatically convert chemical structure diagrams from scientific literature, patents, and other scanned documents into machine-readable string such as SMILES ~\\cite{weininger1988smiles} representation. In order to better address the challenges of the OCSR task in real-world literature.We introduce a new end-to-end framework named MolParser for Optical Chemical Structure Recognition in the wild, illustrated in Figure \\<PIC> The main contributions of this paper. SMILES, though widely used for molecular representation, has notable limitations in handling complex chemical entities. It struggles with representing Markush structures, which are used in patents to describe a broad class of molecules by allowing variability at certain positions, enabling the protection of entire families of compounds. Additionally, SMILES cannot effectively handle connection points, abstract rings, ring attachments, duplicated groups or polymers, all of which require a level of flexibility that its linear format does ", "images": ["2411_11098v1_0"], "tokens": 431}, {"chunk_id": 2, "text": "not support. Furthermore, SMILES is not well-suited for tasks involving large models, such as Markush-molecule matching, as its structure lacks the clarity and hierarchical organization needed for efficient interpretation by machine learning models. These limitations hinder its utility in advanced cheminformatics applications. We etend the SMILES representation method, to more effectively represent the Markush structures commonly found in patents, as well as complex compositions such as connection points, abstract rings, ring attachments, duplicated groups, and polymer structures. Additionally, we ensure that this approach is compatible with RDKit and LLM-friendly, facilitating subsequent analysis and processing tasks. The extended SMILES format will be denoted as: \\texttt{SMILES<sep>EXTENSION}. Where \\texttt{SMILES} refers to the RDKit-compatible SMILES representation. Special token \\texttt{<sep>} serves as the separator, and optional \\texttt{EXTENSION} represents supplementary descriptive used to handle complex cases such as Markush structures. In \\texttt{EXTENSION}. We use some XML-like special tokens to represent certain special functional groups.For Markush R-groups and abbreviation groups, we add special token \\texttt{<a>} and \\texttt{</a>} to encapsulate descriptions of these special substituents. Similarly, we use special token \\texttt{<r>} and \\texttt{</r>} for ring attachments; \\texttt{<c>} and \\texttt{</c>} for abstract rings. Additionally, there is a special token \\texttt{<dum>} representing a connection point. For the specific description of each functional group, we use the following format: \\texttt{[INDEX]:[GROUP\\_NAME]}. Figure \\<PIC> shows a example of our extended SMILES. Although there are numerous complex Markush structures in actual patents, our extended SMILES rules can still adequately address these cases. For more details, please refer to the supplementary materials.", "images": ["2411_11098v1_1"], "tokens": 524}]}
{"id": 60074, "dataset": "arxiv", "images": ["2411_11641v1_0", "2411_11641v1_1"], "chunks": [{"chunk_id": 1, "text": "In this paper, we propose a time series anomaly detection method based on INR reconstruction (TSINR for short). Specifically, we introduce a transformer-based architecture to predict the INR parameters of the given time series data. To better learn and reconstruct time series data, we design a novel form of continuous function to decompose time series \\cite{cleveland1990stl,fons2022hypertime}. The designed function mainly comprises three components and individually learns the trend, seasonal, and residual information of time series. In addition, to further enhance the capability of INR to capture inter- and intra-channel information, we propose a group-based architecture to specifically learn the complex residual information. Simultaneously, we leverage a pre-trained large language model (LLM) to encode the original data to the feature domain, amplifying the fluctuations of anomalies from both time and channel dimensions and enabling INR to subsequently better differentiate normal and abnormal points. The major contributions of this paper are summarized as follows: \\item We utilize the spectral bias property of INR to prioritize fitting low-frequency signals and enhance sensitivity to discontinuous anomalies, thereby improving anomaly detection performance.A transformer-based architecture is employed to generate the parameters for INR, requiring only a single forward step in the inference phase. \\item We design a novel form of INR continuous function, which mainly consists of three components to implicitly learn the unique trend, seasonal, and residual information of time series. Furthermore, a group-based strategy is proposed to further learn intricate residual information. \\item We leverage a pre-trained LLM to encode the original time series to the feature domain, enabling amplification of the fluctuations of anomalies in both time and channel domains that facilitate INR to be further sensitive for noncontinuous anomaly areas. Ablation studies and visual analysis validate the aforementioned capacity to better distinguish anomaly points via our proposed framework. \\item Extensive experiments demonstrate the overall effectiveness of TSINR compared with other state-of-the-art methods on seven multivariate and one univariate time series anomaly detection benchmark datasets. Time series anomaly detection methods primarily include statistical, classic machine learning, and deep learning methods. As shown in Figure \\<PIC>, we innovatively propose a INR continuous function to better learn and reconstruct time series data. Inspired by classical time series decomposition methods \\cite{cleveland1990stl,fons2022hypertime}, the proposed INR continuous function $f$ consists of three components, including trend $f_{tr}$, seasonal $f_{s}$, and residual $f_{r}$. Statistical methods rely on analyzing the statistical properties of the data to identify patterns that deviate from the expected behavior. They are valuable for their simplicity and interpretability, but have limitations in capturing complex patterns \\cite{cleveland1990stl, coluccia2013distribution}.", "images": ["2411_11641v1_0"], "tokens": 740}, {"chunk_id": 2, "text": "Classic machine learning methods rely on manual feature extraction and various algorithms like clustering \\cite{ruff2018deepsvdd,shin2020itad}, density estimation \\cite{breunig2000lof,yairi2017data-MPPCACD,zong2018deep-DAGMM}, and isolation forests \\cite{liu2008isolation-iForest} to identify anomalies in structured data. However, because they require manual feature extraction and selection, they can be labor-intensive and less effective at capturing complex patterns in data. In this section, we analyze the effectiveness of the proposed decomposition components and group-based architecture. The decomposition components indicate the three components (i.e., trend, seasonal, and residual) designed in our paper. And the group-based architecture is proposed for the residual block. The main purpose of the decomposition components is to extract the unique trend and seasonalinformation of the time series data. The results in Table \\ref{ablation_group} indicate that capturing these distinctive features significantly enhances the capability for anomaly detection. In addition, the group-based architecture is designed to enhance the representational capacity of INR for multivariate data. Experimental results indicate an improvement in the capability for anomaly detection when employing the proposed group-based architecture. This is because modeling multiple variables and capturing both inter- and intra-channel information with a simple continuous function, which only consists of fully-connected layers, is challenging. Our approach addresses this by dividing the variables into several groups and applying independently fully-connected layers in different groups, thereby reducing the number of variables each group needs to model and improving the representational capacity. The global layers extract the inter-channel information, while the group layers selectively focus on detailed information for specific channels. This enhances the representational capability for each variable without losing any knowledge. Detailed ablation studies on the number of groups are left in Appendix \\ref{app_group_num}.Further, we prove the validity of the pre-trained LLM encoder, which is utilized to encode the data into the feature domain to amplify the fluctuations of anomalies and thereby enhance the capability of TSINR in identifying anomalies. Table \\ref{ablation_llm} displays the ablation studies of the pre-trained LLM encoder. For the multivariate datasets, it can be observed that applying this encoder enhances the performance of anomaly detection. To further demonstrate the effectiveness, we compare the raw data with the features extracted through the encoder. The figures in the first row illustrate that during the time interval when anomalies occur, the extracted features exhibit more pronounced fluctuations compared to the original data. Furthermore, we validate the robustness of TSINR with the synthetic data generated for time series anomaly detection. It has univariate time series and involves different types of anomalies \\cite{yang2023dcdetector,lai2021revisiting}, including the point-wise anomaly (global point and contextual point anomalies) and pattern-wise anomalies (shapelet, seasonal, and trend anomalies). As shown in Figure \\<PIC>, the red points are anomaly points and the red areas are anomaly areas. This implies that the discontinuity in anomalies is increased in time domain. Also, these extracted features incorporate inter-channel information, providing a manifestation of anomalies among all variables. As shown in the second line, the features exhibit anomalous fluctuations in the same time interval as other channels, whereas the original data only shows a brief peak. This verifies that the anomalies are shared in channel domain. Based on these results, we indicate that utilizing the pre-trained LLM encoder can effectively enhance abnormal information both intra- and inter-channel. This aligns with the spectral bias of INR, making our model more sensitive to anomalous data.", "images": ["2411_11641v1_1"], "tokens": 953}]}
{"id": 60076, "dataset": "arxiv", "images": ["2411_12640v1_0", "2411_12640v1_1"], "chunks": [{"chunk_id": 1, "text": "Leadsee-Precip has an encoder-decoder structure as illustrated in Fig. \\<PIC>. It consists of three parts: feature extraction, hidden translator, and precipitation upsampling. The feature extraction part primarily serves to derive features from circulation variables and reduce spatial resolution. To facilitate the model coupling, the input circulation variables consist of 69 channels, including 5 upper-air variables, each with 13 levels, and 4 surface variables, as the popular deep learning global atmospheric circulation prediction models. The upper-air variables are processed using 3D ConvNets for feature extraction and downsampling, allowing the model to capture the correlations across different variables and altitude levels. The surface variables use 2D ConvNets only. We set zonal circular padding in downsampling processes to ensure proper handling of the boundary conditions. After downsampling, the spatial resolution of upper-air and surface variables reduces to one-fourth. The downsampled upper-air and surface variables then concatenate together with an additional static layer, which includes three channels and has weights automatically learned by the model.", "images": ["2411_12640v1_0"], "tokens": 281}, {"chunk_id": 2, "text": "A shortcut connection with 64 channels is established between the initial variables and the precipitation upsampling part to improve the accuracy of predictions. The hidden translator part uses an encoder-decoder structure as well. The spatial resolution is reduced by half after the Enc module to save GPU memory usage while the number of channels remains unchanged. The MogaNet Hidden module contains 16 MogaNet blocks, which outperforms many of the currently leading Transformer-based network structures. The Dec module doubles the spatial resolution and adds the shortcut connection from the feature extraction part. The precipitation upsampling part increases the spatial resolution of forecast results back to 0.25 degrees. A shortcut connection from the initial variables is added to this process to better reconstruct precipitation prediction. The long-tail distribution of precipitation data presents a challenge for DL models when directly trained using RMSE, as these models rarely sample the infrequent extreme precipitation events. This leads to biased forecasts, especially for heavy rainfall. The Google team introduced a method using the logit function to adjust the loss function (called logit adjustment loss), specifically aimed at addressing the class imbalance issue \\cite{menon2020long}. Inspired by this approach, we develop an IB scheme specifically designed to address the regression challenges posed by long-tail data in precipitation forecasting. The IB scheme operates on the principle that no precipitation or light precipitation, due to their high probability of occurrence (low uncertainty), actually provides less information content. Therefore, the information content demonstrated using a logit form $-\\log P(y_i)$ corresponding to the $i$-th precipitation sample can naturally serve as the weight for precipitation error, balancing the differences in sample sizes across different precipitation bins. The loss function of the model aims to minimize the total \"information content-weighted error\" across all precipitation samples.Specifically, The IB scheme uses the following formula to calculate the weight (\\textit{$W_i$}): We use weather station precipitation data from about 10470 weather stations in the China \\cite{feng2024enhancing} to conduct extra evaluations. The original data is stored in hourly precipitations and is processed to 6-hour cumulative ones for the test. When evaluating with weather station data, only the precipitation results at the station location are used. It is important to note that we did not use the weather station dataset to train the model. Therefore, such evaluation based on \u201cthird-part\u201d weather stations dataset is more objective but challenging. Here, we present the evaluation results of Leadsee-Precip on the global test set, followed by verification results using weather station data, and finally, the evaluation results with weather station data of the fine-tuning model. Fig. \\<PIC> shows the diagnosing skill of our model over the global range of $60^\\circ S$ to $60^\\circ N$. The precipitation in the upper panel is diagnosed by the model based on 69 variables from upper-air layers and surface data at a single time step while the lower panel illustrates the ground truth of the 6-hour accumulated precipitation of NOAA CMORPH. The diagnosed large rainfall (e.g., above the 25 $mm$ $6h^{-1}$ threshold) shows good consistency with the ground truth in both intensity and location. For instance, the model accurately reproduces several events of 6-hour accumulated rainfall exceeding 50 $mm$ in the eastern and western Pacific regions. For precipitation below 1 $mm$ $6h^{-1}$, the diagnostic results show a tendency for overestimation, with finer details appearing comparatively smoothed. Table \\ref{tab:TS_FSS_25km} presents the evaluation metrics TS and FSS of the model on the test dataset.We selected seven 6-hour accumulated precipitation thresholds: 0.1, 1, 5, 10, 25, and 50 $mm$ $6h^{-1}$. The metrics generally show a decline with increasing precipitation levels while both TS and FSS slightly increase at the 0.1 and 1 $mm$ $6h^{-1}$ thresholds, suggesting the model tends to overestimate small rainfalls. However, the FSS of 25 $mm$ $6h^{-1}$ still exceeds 0.5, highlighting the model's ability to capture heavy precipitation events. ", "images": ["2411_12640v1_1"], "tokens": 1039}]}
{"id": 60079, "dataset": "arxiv", "images": ["2411_12276v1_0", "2411_12276v1_1"], "chunks": [{"chunk_id": 1, "text": "This work focuses on complementary-label learning (CLL), a WSL problem where each label indicates only a class to which a data instance \\textit{does not belong}~\\cite{ishida2017learning}. CLL aims to train models with these complementary labels while still enabling accurate predictions of the ordinary labels during testing. CLL makes machine learning more practical in scenarios where obtaining ordinary labels is difficult or costly~\\cite{ishida2017learning}. Additionally, CLL broadens our understanding of machine learning's practical potential under limited supervision. Current research on CLL has introduced numerous learning algorithms~\\cite{scl2020, fwd2018, gao2021discriminative, cpe2023} that have been evaluated using a diverse range of datasets, from synthetic datasets based on varied complementary-label generation assumptions to real-world datasets~\\cite{clcifar2023}. However, the performance of these algorithms often varies significantly across studies due to differences in underlying label-generation assumptions, the absence of a standardized evaluation platform, and the use of diverse network architectures~\\cite{scl2020, fwd2018, ishida2017learning, cpe2023}.Establishing a fair, reproducible, and stable evaluation environment is therefore essential for advancing CLL research. For instance, variations in network architectures, such as the use of ResNet18~\\cite{xu2019generativediscriminative, clcifar2023} versus ResNet34~\\cite{fwd2018, scl2020}, contribute to inconsistencies in performance and hinder fair comparisons across studies. Furthermore, most CLL research has not publicly released implementations~\\cite{mcl2020, cpe2023, scl2020, ComCo2023}, particularly regarding details like loss calculation and data pre-processing. This lack of accessibility presents a challenge for researchers seeking to validate and build upon existing work in CLL. To enable meaningful comparisons among CLL algorithms and create a user-friendly environment for implementation and innovation, we introduce \\texttt{libcll}, a complementary-label learning toolkit built with PyTorch-Lightning. This toolkit standardizes the evaluation process while offering extensive customization options, making it easier for researchers to develop, test, reproduce, and refine algorithms. By performing comprehensive benchmark experiments across established CLL datasets, various algorithms, and a range of complementary-label distributions, as illustrated in Figure \\<PIC>, t{libcll} provides a robust and reproducible evaluation framework. Our goal is for {libcll} to accelerate progress in CLL research and foster a collaborative research community. Additionally, there is a growing body of research focused on leveraging multiple complementary labels for supervision \\cite{mcl2020, reg2021, ComCo2023, conu2023}, where each instance is assigned multiple complementary labels generated from a transition matrix without replacement. In fact, the problem of learning from multiple complementary labels can be connected to partial-label learning or negative-unlabeled learning \\cite{conu2023}.", "images": ["2411_12276v1_0"], "tokens": 748}, {"chunk_id": 2, "text": "Building on these concepts, \\cite{clcifar2023} curated a human-labeled CIFAR~\\cite{krizhevsky2009learning} dataset with complementary labels to better understand real-world CLL distributions, where the transition matrices are both biased and noisy, and each instance has three complementary labels. There remain several open problems in CLL. First, because the transition matrix $T$ is predefined, there is no universal generation process for biased complementary labels, and a general framework is needed for fair comparison. Second, in the absence of ordinary labels, the transition matrix $T$ is often assumed to be given. If a small portion of true labels is available, the transition matrix can be estimated using the anchor point method proposed in \\cite{fwd2018}. These variations can lead to inconsistent experimental outcomes. Finally, without ordinary labels, the reliability of validation using only complementary labels is uncertain. To address these challenges, we introduce \\texttt{libcll}, the first CLL toolkit, to support future CLL research and advance understanding in the weakly-supervised learning community. Previous methods on CLL}\\label{sec:previous-methods} In this section, we present a timeline of key developments in CLL, as illustrated in Figure~\\<PIC>. We implement three primary categories of CLL methods in tt{libcll}: URE (unbiased risk estimator), CPE (complementary probability estimation), and MCL (multiple complementary label) methods. Additionally, we include several bridging works that connect CLL with other learning frameworks. {URE Series of Works} The concept of complementary-label learning was initially proposed by \\cite{ishida2017learning}, who introduced a risk estimator using Pairwise Comparison (PC) and One-vs-All (OvA) strategies for restricted loss functions. With biased complementary labels, \\cite{fwd2018} employed forward correction to reconstruct classification risk using cross-entropy loss on complementary labels and the transition matrix.Furthermore, \\cite{ishida2019complementarylabel} removed these restrictions, extending the unbiased risk estimator to support arbitrary loss functions and models. {CPE Framework} \\cite{cpe2023} offered a new perspective by approaching prediction as a decoding process with the transition matrix. This proposed decoding framework can unify various risk estimator methods \\cite{fwd2018, scl2020, gao2021discriminative}. ", "images": ["2411_12276v1_1"], "tokens": 620}]}
{"id": 60085, "dataset": "arxiv", "images": ["2411_04987v1_0", "2411_04987v1_1", "2411_04987v1_2", "2411_04987v1_2", "2411_04987v1_2"], "chunks": [{"chunk_id": 1, "text": "The ability to learn concepts about a novel task, such as the goal and motion plans, from a few demonstrations is a crucial building block for intelligent agents -- it allows an agent to learn to perform new tasks from other agents (including humans) from little data. Humans, even from a young age, can learn various new tasks from little data and generalize what they learned to perform these tasks in new situations \\cite{lake2019human}. In machine learning and robotics, this class of problems is referred to as Few-Shot Learning \\cite{parnami2022learning}. Despite being a widely studied problem, it remains unclear how we can enable machine learning models to learn concepts of a novel task from only a few demonstrations and generalize the concepts to new situations, just like humans do. Common approaches learn policies either directly, which often suffer from covariate shift \\cite{shimodaira2000improving}, or via rewards \\cite{ziebart2008maximum,ng2000algorithms, fu2017learning}, which are largely limited to previously seen behavior \\cite{ghasemipour2020divergence}.In a different vein, other work has relied on pretraining on task families and assumes that task learning corresponds to learning similar tasks to ones already seen in the task family \\cite{duan2017one,finn2017one}. Inspired by the success of generative modeling in few-shot visual concept learning \\cite{lake2015human,gal2022image,liu2023unsup}, where concepts are latent representations, in this work, we investigate whether and how few-shot task concept learning can benefit from generative modeling as well. Learning concepts from sequential demonstrations rather than images is by nature more challenging due to sequential data often not satisfying the i.i.d. assumption in machine learning \\cite{oord2016wavenet}. In particular, we assume access to a large pretraining dataset of paired behaviors and task representations to learn a conditional generative model that synthesizes trajectories conditioned on task descriptions. We hypothesize that by learning a generative model conditioned on explicit representations of behavior, we can acquire strong priors about the nature of behaviors in these domains, enabling us to more effectively learn new behavior that is not within the pretraining distribution, given a limited number of demonstrations, and further generate the learned behavior in new settings. To this end, we propose Few-Shot Task Learning through Inverse Generative Modeling (\\methodname{}). In our approach, we first pretrain a large conditional generative model which synthesizes different trajectories conditioned on different task descriptions. To learn new tasks from a limited number of demonstrations, we then formulate few-shot task learning as an {\\it inverse generative modeling problem}, where we find the latent task description, which we refer to as a \\textit{concept}, which maximizes the likelihood of generating the demonstrations. This approach allows us to leverage the powerful task priors learned by the generativeen demonstrations without finetuning the model (Figure~\\<PIC>).", "images": ["2411_04987v1_0"], "tokens": 720}, {"chunk_id": 2, "text": "We demonstrate this approach in various domains: object rearrangement, where concepts are relations between objects, goal-oriented navigation, where concepts are target attributes, motion capture, where concepts are human actions, autonomous driving, where concepts are driving scenarios, and real-world table-top manipulation where concepts are manipulation tasks (Figure~\\<PIC>). New concepts are either (1) compositions of training concepts ({\\it e.g.}, multiple desired relations between objects that define a new object rearrangement concept) or (2) new concepts that are not explicit compositions in the natural language symbolic space of training concepts ({\\it e.g.}, a new human motion `jumping jacks' is not an explicit composition of training concepts `walk', `golf' etc.). Thanks to generative models' compositional properties that enable compositional concept learning \\cite{liu2022com", "images": ["2411_04987v1_1"], "tokens": 208}, {"chunk_id": 3, "text": "positional}, in addition to being able to learn a single concept from demonstrations directly, \\methodname{} learns compositions of concepts from demonstrations that, when combined, describe the new concept. We show that our approach generates diverse trajectories encapsulating the learned concept. We achieve this due to two properties of generative models. First, these models have shown strong interpolation abilities \\cite{ramesh2021zero,saharia2022photorealistic}, which allow generating the new concept on new initial states they were not demonstrated from. Second, these models have compositional properties that enable compositional trajectory generation \\cite{ajay2023is}, which allow composing learned concepts with training concepts to synthesize novel behavior that was not demonstrated ({\\it e.g.}, `jumping jacks' and `walk'), see Figure~\\ref{fig:generation}. We further demonstrate that our approach addresses a unique challenge introduced in learning task concepts: we utilize plans generated by learned concepts in a closed-loop fashion.Our main contributions are (1) formulating the problem of task learning from few demonstrations as Few-Shot Task Learning through Inverse Generative Modeling (\\methodname{}), (2) adapting a method for efficient concept learning to this problem based on the new formulation, and (3) a systematic evaluation revealing the ability of our method to learn new concepts across a diverse set of domains. section{Experiments} We demonstrate results in four domains where concept representations are T5 \\cite{raffel2020exploring} embeddings of task descriptions in natural language for training, and empty string embeddings for the dummy condition. During few-shot concept learning, we are provided with three to five demonstrations of a composition of training concepts or of a novel concept that is not an explicit composition of training tasks in natural language symbolic space. We ask a model to learn the concept from these demonstrations. {Learning concepts describing goals that are spatial relations between objects.} Object rearrangement is a common task in robotics \\cite{shah2018bayesian, yan2020robotic, rowe2019desk} and embodied artificial intelligence (AI) \\cite{puig2020watch,srivastava2022behavior}, serving as a foundation for a broader range of tasks such as housekeeping and manufacturing. Here, we use a 2D object rearrangement domain to evaluate the ability of our method to learn task specification concepts. Given a concept representing a relation between objects, we generate a single state describing that relation. The concept in a training example describes the relation (either `right of' or `above') between only one pair of objects (out of three objects) in the environment. Then, a model must learn compositions of these pairwise relations and new concepts such as `diagonal' and `circle' (see Figure~\\ref{fig:shapes}). The results in Figures~\\ref{fig:shapes-new-concept-compos-qual} and~\\<PIC> demonstrate that our method learns unseen compositions of training concepts and new concepts. They further demonstrate how our method composes new concepts with learned concepts.", "images": ["2411_04987v1_2"], "tokens": 747}, {"chunk_id": 4, "text": "For additional qualitative results, please refer to Appendix~\\ref{appendix:add-res}. While successful in most cases, there are also a few failure examples. The accuracy for the new `circle' concept is low ($0.44$) compared to the mean over task types in Figure~\\<PIC> Object Rearrangement New Concept ($0.82\\pm0.09$). This is most likely due to this concept lying far out of the training distribution. The task `square right of circle $\\land$ triangle above circle' has low accuracy for 2 concepts ($0.32$) compared to the mean in Table~\\ref{tab:ablation} Object Rearrangement Training Composition ($0.75\\pm0.11$). This may arise from the combined concept-weight optimization process -- as there is no explicit regularization on weights, they may converge to 0 or diverge. In Figure~\\ref{fig:shapes-tab4-qual}, we show that concept components ma", "images": ["2411_04987v1_2"], "tokens": 232}, {"chunk_id": 5, "text": "y or may not capture new concept relations. textbf{Learning concepts describing goals based on attributes of target objects.} We test our method in a goal-oriented navigation domain adapted from the AGENT dataset \\cite{shu2021agent}, where an agent navigates to one of two potential targets. Conditioned on a concept representing the attributes of the desired target object and initial state, we generate a state-based trajectory describing an agent navigating to the target. Each object has a color and a shape out of four possible colors and four shapes. During training, we provide 16 target-distractor combinations that include all colors and shapes (but not all combinations), and a concept is conditioned on one of the target's attributes ({\\it e.g.}, color). We introduce new concepts defined by both target attributes, including (1) unseen color-shape target combinations and (2) new target-distractor combinations. Figure~\\ref{fig:AGENT} shows an example. In training, we see bowl and red object targets. A new concept includes a novel composition as the target -- red bowl. The new concept distractor objects (green bowl and red sphere) were introduced during training, but they were not paired with a red bowl as the target.As Figure~\\ref{fig:AGENT-open} shows, our method successfully learns concepts where targets are new compositions of target attributes in settings with new target-distractor pairs and generalizes to new initial object states. We further evaluate our model and baselines in closed loop (Figure~\\<PIC>) by making an additional assumption that a planner is provided. The planner produces an action given a current state and a future desired state predicted by a model.", "images": ["2411_04987v1_2"], "tokens": 406}]}
{"id": 60086, "dataset": "arxiv", "images": ["2411_04965v1_0", "2411_04965v1_1"], "chunks": [{"chunk_id": 1, "text": "IntroductionRecent works~\\cite{bitnet2} have shown that 1-bit LLMs can match the full-precision models given the same amount of parameters and training tokens while being significantly cost-effective in terms of latency, memory, throughput, and energy consumption. With model weights represented in 1.58-bit (i.e., \\{-1, 0, 1\\}), the bottleneck of inference has shifted from the limited memory bandwidth to high computational cost. Low-bit or sparse activations in LLMs served as a promising approach to further reduce the computational budget while maintaining performance on downstream tasks. One common approach is to utilize activation sparsity~\\cite{dejavu, turbosparse, teal}, which reduces the inference FLOPs and the I/O of weight by pruning the activation entries with smaller magnitudes. Sparsification is particularly well-suited for handling activations that exhibit highly imbalanced long-tailed distributions. Recent works~\\cite{qsparse} have demonstrated that LLMs with fully sparsely-activated activations can achieve results comparable to dense models while having much less active parameters. In addition to sparsification, activation quantization is another approach to accelerate the matrix multiplication.However, the optimization of neural networks with low-bit activations is challenging due to the emergence of outlier dimensions as the training progresses and the model size grows. Despite these outliers only account for a very small portion of the activations~\\cite{llmint8, smoothquant}, they have much larger magnitude, which leads to significant quantization errors and performance degradation on downstream tasks. Previous works~\\cite{int4, quarot, spinquant, duquant} mostly utilize Hadamard or learnable rotary transformation to amortize the outlier features into other entries. However, they are mostly designed for the LLMs of higher precision (e.g., 4-bit). For 1-bit LLMs, the extremely low bit-width of the weights makes it challenging to absorb these transformation matrices directly into the weights, while leaving them as online transformations introduces additional computational overhead and limits overall inference performance. In this work, we introduce \\textbf{BitNet a4.8}, a hybrid quantization and sparsification strategy that enables 4-bit activations for 1-bit LLMs. By carefully analyzing the activation distribution of 1-bit LLMs, we selectively apply 4-bit quantization or sparsification based on the distribution patterns of these activations. Specifically, as shown in Figure~\\<PIC>, BitNet a4.8 employs 4-bit activations for the inputs to attention and FFN, while utilizing sparsification with 8 bits for intermediate states. To improve the training efficiency, BitNet a4.8 is trained from 8-bit to 4-bit activations with a two-stage recipe, which requires only a few training tokens to adapt \\bitnet{} to the low-bit activations at the end of training. Extensive experiments demonstrate that BitNet a4.8 achieves competitive performance to \\bitnet{} with the same training cost while being significantly more efficient at inference time.", "images": ["2411_04965v1_0"], "tokens": 746}, {"chunk_id": 2, "text": "Furthermore, BitNet a4.8 has only 55\\% activated parameters and supports 3-bit KV cache, which further enhances the efficiency of LLM deployment. paragraph{Down projection of FFN.} We compared 1.3B BitNet a4.8 with different quantization or activation function for the down projection of FFN. All models were trained with the first-stage scheduling for 50B tokens from the RedPajama dataset. To ensure a fair comparison, we leave the other activations at 8-bits. We adopt the absmax quantizer for INT8 quantization and MinMax quantizer for FP4 quantization. The $\\beta$ of absmean quantizer is set as $2 \\text{mean}(|X|)$. Figure~\\<PIC> shows the training loss curves of these models. Squared ReLU achieves slightly better training perplexity than Swish while enabling higher sparsity. Furthermore, applying FP4 quantization for the inputs to the down projection leads to a significant performance degradation, while using INT4 activations with STE causes divergence.", "images": ["2411_04965v1_1"], "tokens": 264}]}
{"id": 60088, "dataset": "arxiv", "images": ["2411_05564v1_0", "2411_05564v1_1"], "chunks": [{"chunk_id": 1, "text": "{Pseudo-labeling with Self-supervised ViT} In order to improve OW-DETR, by leveraging pre-trained ViT features, a first step is to replace Resnet activation maps used for pseudo-labeling by DINOv2 \\cite{dinov2} activation maps. We call this variant OW-DETR$^{+}$. Note that DINOv2 is only used for the activation map extraction for the unknown pseudo-labeling while ResNet continues to be the backbone of the detection method. To further improve {OW-DETR$^{+}$}, we propose the novel pseudo-labeling pipeline illustrated in fig. \\<PIC>. We first propose to filter out the background. Given an input image passed through the DINOv2 backbone, we obtain a feature map $F \\in \\mathbb{R}^{H \\times W \\times d}$, where each pixel is represented by a $d$-dimensional feature vector. Subsequently, we apply DBSCAN \\cite{dbscan} with predefined $eps$ and $min\\_samples$ on these features since it can automatically assign the number of clusters.", "images": ["2411_05564v1_0"], "tokens": 289}, {"chunk_id": 2, "text": "This algorithm provides a great ability to separate between the foreground and the background regions. Hence, we filter out the biggest cluster, likely to be the background, especially for OD benchmarks where usually single objects are not omnipresent in the image. Second, we aim to find different objects in the image by localising the regions having similar and homogeneous semantics. However, DBSCAN offers very coarse semantic clusters. Thus, we apply agglomerative clustering on the same features, refine the clusters boundaries and minimize noisy representations using morphological operations such as erosion and dilation. The feature space is then segmented into $AC$ distinct clusters, according to a distance threshold $d_{th}$. This clustering helps in determining coherent object-like structures. Note that each cluster represents potentially a class, as different objects from the same class have usually similar features. We aim to keep only regions having high objectness by using the DINOv2 activation maps. In fact, $h_a$ attention maps $A_i =\\{a_{h,w}, 1 \\leq h \\leq H, 1 \\leq w \\leq W \\}$ are derived from the ViT backbone, where $h_a$ represents the number of attention heads. Each pixel value $a_{h,w}$ for each attention map is normalized to prevent outlier activations: $a_{h,w} = \\frac{a_{h,w}}{\\sum_{l=1}^{H} \\sum_{k=1}^{W} a_{l,k}}$. These maps are averaged to obtain: $A_{avg} = \\frac{1}{h_a} \\sum_{i=1}^{h_a} A_i$. Using $A_{avg}$, we compute the average attention activation $A_{ac}$ for each cluster $ac$. Intuitively, higher activations correspond to regions containing objects. Given this insight, we only keep clusters having an average activation greater than the mean average attention of the whole image ($A_{ac}>mean(A_{avg})$).We then search for spatially isolated instances in these clusters and surround the largest $N$ regions with bounding boxes. Finally, we apply Non-Maximum Suppression (NMS) to the generated bounding boxes and filter out boxes that overlap with ground truth boxes ($IoU>T$). Note that in our case, we can have zero unknown object if top activated regions overlap with ground truth objects. We can also have a large number of unknown objects, if different clusters are largely activated. Following the aforementioned method, pseudo-labels for unknown objects are pre-computed in an offline manner for each image of the training dataset. It is important to note that this is a lengthy process. The D-DETR architecture is then trained end-to-end to detect objects from $|\\mathcal{K}|+1$ classes by predicting $|\\mathcal{K}|+1$ softmax probabilities ($|\\mathcal{K}|$ known class and one unknown class). {OpenImagesRoad benchmark} We compare in tab.\\ref{tab:openimages} performances of OpenDet \\cite{OpenDet}, RandBox \\cite{randbox} and OW-DETR \\cite{OW-DETR} alongside its different improvements. ORE is unreported as it shows poor performance in unknown detection in tab. \\ref{tab:voc_coco}. For fairness, ORE (MAVL \\cite{MAVL}) is also unreported as it uses external supervision to extract pseudo-labels. Interesting performance trends emerge depending on the nature of each method. Indeed, pseudo-labeling methods such as RandBox, OW-DETR, OW-DETR, OW-DETR$^{+}$ and OW-DETR$^{++}$ all have lower $AP_{u}$ than the contrastive method OpenDet which indicates that contrastive methods seem to learn a broader unknown representation which is good for classification. On the other hand, OW-DETR$^{++}$ have higher $AP_{all}$ and $AP_{sc}$ than OpenDet, demonstrating that this pseudo-labeling based method seems more apt for object localization. The higher $AP_{sc}$ also indicates a good ability of pseudo-labeling methods to understand high granularity classification but at the same time, an inability to understand smaller grained differences between known and unknown as shown by the lower $AP_{u}$. The surprising drop between $AP_{u}$ and $AP_{all}$ for OpenDet can be explained by frequent double detections of unknown objects: as unknown and known objects. Fig.method can be driven by the envisaged use-case. If unknown objects should be both correctly detected and classified as unknown, the contrastive learning based OpenDet detector appears to be an appropriate choice, especially for such $S_{unseen}$ scenario. Conversely, if the correct localisation of unknown objects is paramount, and coarse classification is sufficient, then OW-DETR$^{++}$ seems best indicated. It is also worth noting that pseudo-labeling techniques are expected to perform better in the $S_{unlabeled}$ scenario where unknown objects are potentially seen and pseudo-labeled during training. Indeed, we remark that in tab. \\ref{tab:voc_coco} (VOC-COCO/$S_{unlabeled}$), OW-DETR$^{++}$ seems to be relatively more preferment compared to Opendet, than in tab. \\ref{tab:openimages} (OpenImagesRoad/$S_{unseen}$).", "images": ["2411_05564v1_1"], "tokens": 1442}]}
{"id": 60089, "dataset": "arxiv", "images": ["2411_05443v1_0", "2411_05443v1_1"], "chunks": [{"chunk_id": 1, "text": "{Introduction} {I think this paper will benefit from more motivation provided in this section. Much more could be said why such a visualisation is important providing specific examples from different applications} High-throughput experiments are becoming extremely common in applied sciences. Now more than ever, large high-dimensional datasets are generated in almost every laboratory calling for an automated and reliable way to extract new knowledge from them. Let us fix a dataset $X$, usually embedded in a high dimensional space. Standard dimension reduction techniques, including PCA~\\cite{pca}, t-SNE~\\cite{tsne}, \\textsc{UMAP}~\\cite{umap} and \\textsc{PHATE}~\\cite{phate} aim to find a low dimensional embedding of $X$ so that points that are close in $X$, are also close in the embedding. However, preservation of the global organization of $X$ in general, and information about distances of far away points in particular, is a challenge for this methods. Clustering techniques~\\cite{statLearning, SAXENA2017664} on the other hand, based on a fixed similarity measure, provide a partition of the input dataset $X$.However, clustering itself does not provide information about either intra-- or inter-- cluster organization of points, therefore is not used to asses the global structure of the data. The aim of this work is two bridge this two approaches by enriching the output of a clustering algorithm with additional information on the data's global organization. The first contribution of this paper is the construction of a \\emph{ClusterGraph}; a graph--based structure on a top of a partition $\\mathcal{C}(X)$ of the data obtained from a clustering algorithm $\\mathcal{C}$ applied to $X$. In the ClusterGraph $G = (V,E)$, each vertex corresponds to a single cluster from $\\mathcal{C}(X)$. Two vertices $u,v \\in V$ are connected by an edge whose length corresponds to the distance between their respective clusters in $\\mathcal{C}(X)$. For the purpose of this construction, a number ofinter--cluster distances defined in the ambient space are considered. ClusterGraph has a number of advantages compared to alternative dimension reduction methods. One of them is based on the fact that the distances, computed in the ambient space, are represented by labels on edges and not subjected to distortions made by standard dimension reduction techniques that force the projected data points to be embedded in an Euclidean space. This allows to visualize the \\emph{global} distances in the dataset. This is important as many datasets cannot be embedded into low dimensional Euclidean spaces without perturbing the distances between points. As an example of such a situation consider a collection of points in four clusters: $0$, $1$, $2$ and $3$. Points in each cluster are infinitesimally close. Distance between cluster $0$ and clusters $1$, $2$ and $3$ are $1$, while distance between clusters $1$, $2$ and $3$ is $2$. It is well known~\\cite{morgan_embedding_1974, bourgain_lipschitz_1985} that such a graph cannot be isometrically embedded to any Euclidean space $\\mathbb{R}^n$ for any $n$.As a consequence, all dimension reduction techniques will distort the distances between clusters, as can be observed in Fig~\\<PIC>. ClusterGraph, on the contrary, provides the correct graph even in this case. {Metric distortion} Given a dataset $X$, different choices of the clustering algorithm $\\mathcal{C}$, as well as the metrics $d_X$ and $d_\\mathcal{C}$ can lead to significantly different ClusterGraphs. The aim of this section is to introduce a score to assess the quality of a given ClusterGraph $G$ by comparing it to the underlying geometric structure of the dataset $X$. For this purpose let us assume that the considered point cloud $X$ is sampled from a compact and connected manifold $\\mathcal{M}$ equipped with an \\emph{intrinsic distance} $d_{\\mathcal{M}}$. Informally, the intrinsic distance between two points $x,y \\in \\mathcal{M}$ is defined to be the infimum of the lengt", "images": ["2411_05443v1_0"], "tokens": 1076}, {"chunk_id": 2, "text": "h of a curve $\\gamma \\subset \\mathcal{M}$ joining $x$ and $y$, this is also known as \\emph{geodesic} distance. In most applications, the underlying manifold is not known. Consequently, the intrinsic distance needs to be estimated from the point cloud. This is a well-studied problem in computational geometry and computer graphics, and multiple methods have been proposed~\\cite{isomap, klein_point_2004, ruggeri_approximating_2006,yu_geodesics_2014}. Below, we follow the approach of~\\cite{isomap, Bernstein2000GraphAT} using the shortest path in the $k$-nearest neighbor graph as estimator. Note that any other estimator of intrinsic distance can be also used in the proposed construction.Let $G_{knn}(X)$ be the $k$-nearest neighbor graph on $X$ constructed in the following way: each point of $X$ corresponds to a vertex of $G_{knn}(X)$; it is connected to its $k$-nearest neighbors (in the chosen distance $d_X$, typically Euclidean), with $k$ being a parameter of the method. Weights corresponding to the distance between endpoints are assigned to the edges of $G_{knn}(X)$. We define a distance $d_X^k$ on $G_{knn}(X)$, estimating the intrinsic distance on $X$, as It may happen that $G_{knn}(X)$ is not connected. There are two possible reason for this, depicted in Fig.~\\<PIC>. There are two possible reason for this. In the first case points are indeed sampled from a compact and connected manifold but the parameter $k$ is too low. This can be easily solved by increasing $k$. In the second case the underlying manifold is not connected.ph, which we will discuss in Section~\\ref{sec:pruning}. This notion of $d_{CG}$ is well defined only when $\\mathcal{C}(X)$ is a partition. In the more general case of a division, when a point can belong to more than one cluster, we take $d_{CG}(x,y)$ to be the length of the shortest path between any cluster containing $x$ and any cluster containing $y$.", "images": ["2411_05443v1_1"], "tokens": 588}]}
{"id": 60090, "dataset": "arxiv", "images": ["2106_09282v1_1", "2106_09282v1_0"], "chunks": [{"chunk_id": 1, "text": "The main issues that may easily lead to smart contract vulnerabilities are twofold. First, the programming languages and tools are still new and crude, which leaves plenty of rooms for misunderstandings in the built-in functions and tools [Luu et al., 2016]. Second, due to the immutable nature of smart contracts, developers are required to anticipate all possible states (e.g., stack status) and environments that the code may encounter in the future, which is obviously difficult. Existing methods on smart contract vulnerability detection can be roughly cast into two categories. The first line of work [Luu et al., 2016; Tsankov et al., 2018; Jiang et al., 2018] utilized classical static analysis and dynamic execution techniques to identify vulnerabilities. Unfortunately, they fundamentally rely on several fixed expert rules, while the manually defined patterns bear the inherent risk of being errorprone and some complex patterns are non-trivial to be covered. Meanwhile, crafty attackers may easily bypass the fixed patterns using small tricks. Another line of work [Tann et al., 2018; Zhuang et al., 2020] explored using deep learning models to deal with complex contract data, having achieved much improved accuracy.Due to the black-box nature, however, they fail to encode useful expert knowledge and mostly have poor interpretability. This motivates us to consider whether we could combine neural networks with classical expert patterns, where neural networks contribute their ability to handle the complex code semantic graph while expert patterns contribute precise and valuable local information. More importantly, we seek an explainable solution which could tell the weights of different features. In this paper, we propose a new system beyond pure neural networks that can automatically detect vulnerabilities and incorporate expert patterns into networks in an explainable fashion. In particular, (1) we develop automatic tools to extract vulnerability-specific expert patterns.(2) Then, we exploit a graph structure to frame the rich control-flow and data-flow semantics of the function code. Upon the graph, a graph neural network is employed to extract the deep graph feature. (3) Finally, we propose an attentive multi-encoder network to interpretably fuse the global graph feature and local expert patterns. Extensive experiments are conducted on all the 40k contracts in two benchmark datasets, demonstrating significant improvements over state-of-the-art: accuracy from 84% to 90%, 83% to 87%, 75% to 80% on three types of vulnerabilities respectively. More importantly, our model is able to explain its label prediction, give warnings of high weighted local patterns, and provide a grand picture of the significance of different features. <PIC>: The attentive multi-encoder network, consisting of a self-attention mechanism and a cross-attention mechanism. It combines local pattern features and the global graph feature for vulnerability detection, and outputs interpretable weights for all features. Contributions. The key contributions of this work are: 1) We investigate combining vulnerability-specific expert patterns with neural networks in an explainable way. To the best of our knowledge, we are the first to prob the combination interpretably.", "images": ["2106_09282v1_1"], "tokens": 739}, {"chunk_id": 2, "text": "2) In the method, we present a simple but effective multi-encoder network for feature fusion. 3) Our method sets the new state-of-the-art and provides novel insights. To facilitate future research, our implementations are released at https://github.com/Messi-Q/AMEVulDetector. We would like to point out that different from [Liu et al., 2021], this work focuses mainly on the explainability of the expert pattern and deep graph feature combination, and offers a grand picture on the importance of different features. The overview of the proposed system is illustrated in <PIC> which consists of three components: 1) a local expert pattern extraction tool, which extracts expert patterns of a specific vulnerability ey invocations as core nodes, we propose to further extract key variables as core nodes, given that they are undoubtedly important in detecting vulnerabilities. To characterize rich connections between different nodes, we construct three categories of semantic edges, namely control flow, data flow, and fallback edges. Graph normalization. It is worth mentioning that different functions corresponding to distinct code semantic graphs, bringing difficulties in training a graph neural network. Moreover, current graph neural networks are inherently flat when propagating information, ignoring that different nodes are not of equal importance. Therefore, we propose to normalize the graph following that of [Zhuang et al., 2020] to remove all normal nodes and merge their features to the nearest core nodes. A simplified example for graph construction and normalization is given in Fig. 1(b).", "images": ["2106_09282v1_0"], "tokens": 371}]}
{"id": 60092, "dataset": "arxiv", "images": ["2310_13023v3_0", "2310_13023v3_1"], "chunks": [{"chunk_id": 1, "text": "<PIC>: The overall architecture of our proposed GraphGPT with graph instruction tuning paradigm. Contributions. To address these challenges, we propose a novel framework called GraphGPT, which aims to align Large Language Models (LLMs) with Graphs using a carefully designed graph instruction tuning paradigm. (C1) Our framework introduces a textgraph grounding paradigm as the initial step to align the encoding of graph structures with the natural language space. By incorporating textual information in a contrastive manner, we enable effective alignment of graph structure information within language models. (C2) In our proposed dual-stage graph instruction tuning paradigm, we leverage self-supervised signals through the graph matching task, which is derived from unlabeled graph structures, to serve as instructions for guiding model tuning of LLMs. By incorporating this self-supervised instruction tuning, the language model acquires domain-specific structural knowledge related to graphs, thereby enhancing its understanding of graph structures.", "images": ["2310_13023v3_0"], "tokens": 247}, {"chunk_id": 2, "text": "To further customize the LLM\u2019s reasoning behavior for diverse downstream graph learning tasks, the second stage of our graph instruction tuning paradigm involves fine-tuning the LLM with task-specific graph instructions, to improve the model\u2019s adaptability. (C3) By incorporating the Chain-of-Thought (COT) distillation into our framework, GraphGPT enhances its step-by-step reasoning abilities and improves its performance in the face of distribution shift. In summary, our work makes the following contributions: \u2022 This work aims to align graph domain-specific structural knowledge with the reasoning ability of Large Language Models (LLMs) to improve the generalization of graph learning. \u2022 Our approach aims to align LLMs with Graphs through a graph instruction tuning paradigm. This paradigm incorporates selfsupervised instruction tuning, enhancing the LLM\u2019s comprehension of graph structural knowledge and its reasoning capabilities. Additionally, we introduce task-specific instruction tuning to improve the model\u2019s adaptability across diverse graph tasks. \u2022 We evaluate our proposed GraphGPT on supervised and zeroshot graph learning tasks. We conduct thorough analyses of its component-wise effects and generalization ability. By comparing it with state-of-the-art baselines, we demonstrate the superior generalization power of our approach across various settings. To improve the understanding of graph structural information by large language models, our framework focuses on aligning the encoding of graph structures with the natural language space. This alignment enables language models to effectively comprehend the graph\u2019s structural elements using their language understanding capabilities. To achieve this, we introduce a text-graph grounding paradigm that generates prompts preserving the graph\u2019s structural context for language models. This paradigm acts as a bridge, connecting the semantic understanding of textual information with the inherent structural relationships in the graph. In our GraphGPT, we design the graph encoder to be highly flexible, allowing it to leverage a wide range of backbone GNN architectures obtained from diverse graph pre-training paradigms.We incorporate a message-passing neural network architecture, which can be a graph transformer [60] or a graph convolutional network [17], as the structure-level pre-trained graph model. In each message-passing step, the graph encoder aggregates information from neighboring nodes, considering their relationships: Text-Structure Alignment. To enhance the alignment of graph structure information with Language Models (LLMs), our focus is on exploring effective encoding methods that can collaborate seamlessly with LLMs. Building upon previous works [30, 49], we adopt a contrastive approach by incorporating textual information into the graph structure encoding process. We directly integrate a pre-trained graph encoder into our GraphGPT framework, enabling the seamless utilization of its capabilities. <PIC>: Workflow of text-structure alignment. The dual-stage graph instruction tuning paradigm proposed in this work builds upon the concept of instruction tuning, which has been recently introduced to enhance the adaptability of language models for specific domains [45]. In this paradigm, we aim to align the language capacity of the model with the nuances of graph learning tasks, enabling the language model to generate more accurate and contextually appropriate responses for graph-structured data. Self-Supervised Instruction Tuning. In the initial stage of our graph instruction tuning, we introduce self-supervised instruction tuning. This mechanism enhances the language model\u2019s reasoning abilities by incorporating graph domain-specific structural knowledge and effectively understanding contextual information within the graph\u2019s structure. To achieve this, we utilize self-supervised signals derived from unlabeled graph structures as instructions for model tuning. Specifically, we design a structureaware graph matching task that guides the language model in differentiating between graph tokens using language tokens.We further validate the performance of the combined Arxiv and PubMed instruction data on the original Arxiv data, as demonstrated in the \"(Arxiv + PubMed)- Arxiv\" column in Table 1. The results indicate that most traditional GNN-based approaches experience a significant decline in performance on Arxiv after iterative training. In contrast, our model exhibits improved performance. We attribute this phenomenon to the occurrence of catastrophic forgetting in GNN-based models, where the structural modeling competence of the model trained solely on the smaller PubMed dataset is compromised. However, our model effectively mitigates this issue through our unified graph instruction tuning paradigm. This enables our model to maintain and even enhance its performance by retaining the generalized graph structure patterns despite incorporating additional data.", "images": ["2310_13023v3_1"], "tokens": 1151}]}
{"id": 60093, "dataset": "arxiv", "images": ["2404_17839v1_0", "2404_17839v1_1"], "chunks": [{"chunk_id": 1, "text": "For the CL module, we incorporate correlation labels to guide the training process. These labels are constructed based on the relationships between sampled contracts. Therefore, employing a suitable sampling strategy is crucial as it can greatly enhance the performance by ensuring the utilization of high-quality sample pairs for training, while minimizing the introduction of bias into the learning process. Motivated by this, our sampling strategy is as follows: There are three types of relationships for contract pairs, i.e. \"V-V\", \"N-N\", and \"V-N\", where V and N denote Vulnerable and Non-vulnerable contracts, respectively. Our intuition is that the relationships of \"V-V\" and \"V-N\" are more important, because we would like to discover the commonality in \"V-V\" and differences in \"V-N\" by CL. In contrast, the \"N-N\" is not substantially helpful in identifying SCV. Therefore, our sampling strategy is to extract all vulnerable contracts from the original dataset and create a new set called the POS set. Then, for each contract in the original dataset, we randomly select a contract from the POS set to form a pair of contracts as input for the CL model.It should be noted that this sampling strategy does not have 'N-N' relationship. Finally, the correlation labels LCL of the contract pairs are constructed to guide the training of the CL model. <PIC>: Overview of Clear, which encompasses both the CL process, depicted by solid lines indicating the data flow, and the subsequent vulnerability detection process, represented by dotted arrows indicating the data flow. Finally, in Table 1, line 13 reports the performance of the state-of-the-art general method, LineVul, which uses CodeBERT to detect vulnerabilities. The quantitative results suggest that simple migration of general methods to the SCVD field may not yield satisfactory results. Even the LineVul, which performs the best among the general methods, only achieves an average precision, recall and F1-score of 82.92%, 71.", "images": ["2404_17839v1_0"], "tokens": 487}, {"chunk_id": 2, "text": "56%, and 76.57% for the three vulnerability detection scenarios, respectively. In comparison, Clear outperforms LineVul across all three metrics, surpassing LineVul by more than 12.93%, 33.37%, and 23.44%, respectively. Answer to RQ1: The proposed Clear outperforms the state-of-the-art methods across all metrics. On average, Clear achieves an F1-score of 94.52%, showcasing a 9.73% increase in F1-score compared to the existing best-performing method. 5.2 RQ2: Impact of Different Modules To answer RQ2, we conduct comprehensive ablation tests to examine and understand the impact of different modules on Clear\u2019s overall effectiveness. In Section 3.1, we have described that Clear consists of three stages, and therefore, we have specifically designed distinct ablation tests for each of these stages. The results of all ablation tests are presented in Table 2, in which the metrics P, R, and F represent precision, recall, and F1-score, respectively. To begin with, for stage 1, we focus our data sampling strategy on two specific types of contract relationships, namely \"V-V\" and \"V-N\".We selectively mask these relationships in order to evaluate the influence of the labels generated by these two relationships on the overall effectiveness of vulnerability detection. The \"Clear-MVV\" indicates the masked \"V-V\" relationship and \"Clear-MVN\" indicates the masked \"V-N\". As shown in Table 2, Clear-MVN and ClearMVV achieve 81.18% and 84.76% average F1-score, respectively. In comparison, Clear outperforms both of them and has an F1-score of 94.52%. That is to say, learning only one of the contract relations within the CL stage does not yield satisfactory results. It is only by simultaneously learning both relations that we observe a significant improvement in the performance of SCVD. Moving on to stage 2, we intentionally remove the MLM module that is integrated into the CL stage. This allows us to analyze the overall effectiveness of the CL stage without the presence of the MLM module. This particular test is referred to as \"Clear-RMLM\". We observe that the MLM module has a substantial impact on the effectiveness of the CL module. Specifically, when the MLM module is removed, there is an average decrease in precision, recall, and F1-score by 4.15%, 6.13%, and 5.14% respectively. Therefore, we believe that the MLM module can enhance the performance of Clear and is an essential component. Lastly, for stage 3, we remove the CL stage altogether and directly performed the vulnerability detection stage. This test, known as \"Clear-RCL\", enables us to evaluate the performance of vulnerability detection in the absence of the CL stage. In comparison to Clear-RCL, we observe a significant improvement in performance for all three types of vulnerability detection tasks with the addition of the CL module. The F-score increased by 35.47% for RE, 34.21% for TD, and 19.00% for IO. This notable improvement can be attributed to the synergistic effects of the CL stage itself and our unique sampling strategy. Specifically, the CL module facilitates the convergence of dispersed vulnerability samples in the feature space, resulting in increased proximity among them.By utilizing our unique sampling strategy, we further reinforce the correlation among samples belonging to the same vulnerability category, thereby promoting their clustering behavior. This process enables the model to more effortlessly identify and discover potential SCVs, leading to a significant improvement in the performance of SCVD. To substantiate our assertion, we thoroughly examine the derivation process of the sample distribution during the CL stage. In particular, we analyze the evolution of the output of the CL stage (denoted as in Eq. 7) at each epoch and employ principal component analysis [29] to project each output onto a two-dimensional space. Subsequently, these outputs are visualized as scatter plots and displayed in <PIC>, where the horizontal and vertical axes represent linear combinations of the vectors v obtained through PCA. Each point denotes a contract sample, with purple indicating vulnerability samples and yellow representing non-vulnerability samples. The figure clearly depicts the progression of smart contract sample distribution throughout the CL stage and yields the following finding. First, during the training process of the CL module, the samples of vulnerability contracts exhibit a tendency to cluster together, while being distinctly separated from non-vulnerability samples, indicating a clear distinction between the two groups. Second, this distribution enhances the ability to differentiate and detect SCVs. Clear exhibits a higher proficiency in recognizing this particular cluster and accurately classifying contracts within its proximity as vulnerable. This leads to an improved capability for identifying SCVs.", "images": ["2404_17839v1_1"], "tokens": 1150}]}
{"id": 60094, "dataset": "arxiv", "images": ["2211_14582v1_0", "2211_14582v1_1"], "chunks": [{"chunk_id": 1, "text": "With the continuous growth of bitcoin addresses, an accurate yet efficient method for address classification is much coveted. Existing methods for bitcoin address classification can be roughly cast into two categories, i.e., off-chain informationbased method and on-chain behavior-based method. The offchain information-based method focuses on tagging addresses by gathering real-world data from address users. For example, Ermilov et al. [18] crawl the user profiles from relevant forums (e.g., Bitcointalk.com, Twitter, and Reddit) and darknet markets (e.g., Silkroad, Hub Marketplace, and Alphabay) to obtain the association between some bitcoin addresses and users, and then analyze the behavior of other addresses using a clustering method. Kang et al. [19] obtain the IP address by receiving bitcoin protocol message packets, and then use static analysis to infer other addresses which the current IP address may be associated with, realizing the mapping between bitcoin addresses and IP addresses. Unfortunately, these solutions are highly dependent on off-chain collected information and human experience, and hence cannot be used for all bitcoin addresses. Additionally, there are usually many mistakes in off-chain information, which might lead to low accuracy of the address behavior analysis.The on-chain behavior-based method concentrates on extracting transaction characteristics of bitcoin addresses and analyzing their behavior with the guidance of machine learning. For instance, [20]\u2013[25] directly extract transaction features from bitcoin addresses and then feed them into models, e.g., Random Forest, SVM, and LightGBM. Such methods do not need to rely on off-chain information, thus avoiding potential problems caused by missing off-chain information. However, they also suffer from inherent drawbacks. On one hand, direct feature extraction from transactions in the addresses may lead to large deviations. On the other hand, the traditional classification model has difficulties in utilizing the temporal feature and topology of transactions in the addresses. These problems impose a significant impact on the accuracy of address behavior analysis, which motivates us to design a novel and effective address behavior analysis model. In this paper, we present BAClassifier, a fully automatic framework for bitcoin address classification. In particular, BAClassifier consists of three key components. (1) Address Graph Construction. For each given bitcoin address, BAClassifier constructs a chronological transaction graph that reflects the behavior of that address. Specifically, we engage in a graph node compression technique and a graph structure augmentation method to transfer the transactions of each bitcoin address into a unified graph representation. (2) Graph Representation Learning. BAClassifier utilizes graph neural networks to learn the graph representation of each address and generate the graph embeddings. (3) Address Classification. BAClassifier aggregates all graph embeddings of each address into the address-level representations, and engages in a classification model to give the final address behavior classification. \u2022 We propose BAClassifier, a tool that can automatically classify bitcoin address behaviors. Particularly, we investigate a scalable and generic manner of analyzing bitcoin address behaviors using graph neural networks.\u2022 Within this system, we come up with three key components, i.e., address graph construction, graph representation learning, and address classification. Specifically, we introduce graph node compression and graph structure augmentation techniques to translate bitcoin address transactions into unified graph representations. Furthermore, we adopt graph feature networks to extract address graph features and select the combination of LSTM+MLP as the behavior classification model. \u2022 We construct a large-scale labeled dataset that consists of over 2 million bitcoin addresses as well as their transactions and concerns 4 types of address behaviors, which can serve as a benchmark for evaluating bitcoin address classification methods. \u2022 Extensive experiments on the collected dataset show that our proposed system is indeed useful in identifying bitcoin address behaviors. BAClassifier surpasses stateof-the-art address classifiers and overall provides interesting insights. We have released our code and dataset at https://github.com/AwesomeHuang/BAClassifier, hoping to facilitate future research. Method Overview. The overall architecture of BAClassifier is outlined in <PIC>. Generally, BAClassifier consists of three key components: \u2022 Address Graph Construction: For each given bitcoin address, BAClassifier will construct a chronological transaction graph that reflects the behavior of that address. \u2022 Graph Representation Learning: BAClassifier utilizes a graph neural network to learn the graph representation of each address and generate the graph embeddings. \u2022 Address Classification: BAClassifier aggregates all graph embeddings to produce the address-level representations, and confirms the classification model to output the predictions of address classifications. In what follows, we will elaborate on the details of these components one by one. The first step in BAClassifier is to transfer the address transactions into graph structures.", "images": ["2211_14582v1_0"], "tokens": 1160}, {"chunk_id": 2, "text": "To obtain a unified address graph, we have to solve three key problems. (1) Different bitcoin addresses have a distinct number of transactions, thus yielding different sizes of graphs. Moreover, the transactions of one address are performed in temporal order. Therefore, we must guarantee that the generated graphs have a unified structure while still preserving the temporal order of transactions. (2) There is a significant disparity in the number of addresses involved in various transactions. For instance, a transaction issued by certain exchanges may only have several associated addresses, while a transaction generated by a mining pool may have thousands of associated addresses. Hence, the graph size must be limited. (3) Since bitcoin transactions provide an insufficient amount of information, we thus need to go deeper into the global graph structural feature to elicit further information. To tackle the above problems, BAClassifier incorporates three key modules into the address graph construction, namely original graph extraction, graph node compression, and graph structure augmentation. When a transaction occurs, the bitcoin wallet will zero off the balance in the original address, and transfer any leftover funds to a new address. Naturally, the address that receives the change could be set to the original address of the originating transaction. Then, the bitcoin wallet automatically generates a new address to receive the change after the transferred amount and the fee are deducted. Such a mechanism safeguards the user\u2019s privacy by ensuring that no one other than the user could know which address is the change address and which is the receipt address. However, all these facts make the analysis of bitcoin address behavior more difficult and challenging than that of traditional account models. Single-Transaction Address Compression. The single-transaction address compression is proposed to merge the address nodes that have only a single transaction, reducing the number of address nodes in the graph. <PIC> illustrates the specific procedure.espectively.", "images": ["2211_14582v1_1"], "tokens": 443}]}
{"id": 60095, "dataset": "arxiv", "images": ["2404_07382v3_0", "2404_07382v3_2", "2404_07382v3_1"], "chunks": [{"chunk_id": 1, "text": "TRIALMASTER conducts inference on itself without any help from a backtracking system like DFS or BFS. It outputs two kinds of tactics: tactics in Lean and backtrack instructions. When TRIALMASTER is doing a proof search on the test set, it is prompted with all history paths, including previous tactics, states, the backtracking it made before, and the failed search path. It then outputs the entire proof path after. Nonetheless, we only utilize the first tactic in the output and employ Lean as a calculator to determine the next state, thereby ensuring the correctness of the state following the tactic. If the tactic outputted by TRIALMASTER is a backtrack instruction, it is then prompted with all the proof search history including the backtrack instruction and the state that the backtrack instruction says to return to. If that tactic is not a backtrack instruction, the tactic and the current state will be fed into Lean for producing the state after. TRIALMASTER is then prompted with the entire proof tree including the state that Lean calculated, and it should output a tactic again. This process is repeated until Lean identifies that the proof is complete or any Lean error occurs. We also note that TRIALMASTER only outputs one tactic at each state using greedy search.<PIC>: Method comparison. (a) A conventional system: The tactic generator (i.e., LLM) is fine-tuned on correct proof paths only. During inference, the trained tactic generator produces Nsampled (e.g., 2 in the example) tactics at a time. If Lean decides that the current tactic is wrong, the system backtracks to the last valid state and tries other candidate tactics. (b) Our methodology: The tactic generator is fine-tuned on proofs with trial-and-error. During inference, we take the first tactic it generates and feed that into Lean for state checking at each step. Why do we choose DFS over BFS? While the breadth-first-search (BFS) system is also popular for building neural provers in Automated Theorem", "images": ["2404_07382v3_0"], "tokens": 494}, {"chunk_id": 2, "text": "Proving, we have opted for DFS as our baseline over BFS in the context of propositional logic theorem proving. This is due to the finite number (around 20) of tactics available at any step for the search process of intuitionistic propositional logic theorems, making DFS more efficient than BFS without compromising the success rate. 5.3 Results and Analysis TRIALMASTER outperforms conventional DFS system. We begin by evaluating the methods of the in-distribution test set. Table 2 illustrates that both our method and the DFS system perform exceptionally well, achieving a success rate of nearly 100% in most configurations. This suggests that Llama-7b effectively masters in-distribution intuitionistic propositional logic theorems. Then, we compare the performance of the methods on the out-of-distribution task. The results are presented in <PIC>. Our method with trial-and-error significantly outperforms the DFS system across various hyperparameter configurations. Additionally, we observe that feeding more proofs without trial-and-error for LLM fine-tuning does not further improve the performance. Model learns backtracking capability from trial-and-error data.", "images": ["2404_07382v3_2"], "tokens": 274}, {"chunk_id": 3, "text": "In the experiments, we find out that our TRIALMASTER successfully acquires the backtracking capability from proofs with trial-anderror information. This is evidenced by the fact that during TRIALMASTER\u2019s proof search for theorems in the test set, all backtracking instructions produced by the LLM adhere to the correct format and point to existing state numbers. Including failed search paths helps TRIALMASTER to learn. The following experiment shows that adding failed search paths to the training data for TRIALMASTER results in an overall gain. In this experiment, the model is only trained to learn the correct search paths and the backtracking instructions. The model is not trained to learn the failed search paths (we don\u2019t compute the loss for the failed search paths during the training stage in this case). The proof success rate in this case is 75.6%, which is lower than TRIALMASTER\u2019s proof success rate of 88.7%. The NLean for the model is 13,600, which is lower than that of TRIALMASTER. This is expected since the model does not learn to predict failed search paths. Our explanation for why TRIALMASTER has a higher proof search success rate than the model trained in the previously mentioned experiment is that the failed search paths also contribute to improving the proof search success rate of the model. TRIALMASTER strategically tried some potentially failed search paths to gain a more comprehensive view of the problem, which then led to the correct search paths. 5.4 Ablation Study To evaluate the effectiveness of training with trial-and-error, we craft an ablated version of our method where the LLM is fined-tuned with data of the correct path only and do inference in the same way as our method (i.e., producing one tactic at a time and applying Lean for state checking). We denote the ablated version as Model - proof w/o t.a.e.. For both methods, we mark the search attempt as failed if the tactic induces a Lean error, or the search exceeds the 1500-word limit. The result is shown in the Table 3. The difference between the success rates of the two models is 29.4%, which is significant. This clearly shows that failed search states and trial-and-error information tremendously enhance the model\u2019s capability to solve theorem-proving tasks. 5.5 Exploratory Study: Effect of Training Proof Length on Model Performance Since the FPS algorithm of PropL dataset can generate multiple proofs with variable length, we conduct an exploratory study to assess the impact of proof length on model performance. We fine-tune two models using proofs with different lengths of trial-and-error information. For the first model, which is our TRIALMASTER, the training data is derived by randomly selecting two out of the shortest four proofs from the ten available proofs for each theorem in PropL. We denote it as Model - short proof w/ t.a.e. In contrast, the training data of the second model is formed by randomly selecting two proofs from the ten available for each theorem, irrespective of their lengths. We denote it as Model - long proof w/ t.a.e. For both models, we use greedy search to let them generate one tactic for each state. We evaluate the models on our 1000 OOD test set. The results are shown in the <PIC>. A higher success rate is observed in the model trained with shorter proofs. This can be attributed to the fact that as the proof with trial-and-error information becomes longer, there is too much trial-and-error information that may detrimentally affect the model's performance, as too many failed search paths may lower the quality of the training data. 6 Conclusion and Future Work In this paper, we study Automated Theorem Proving in formalized environments. We create a complete, scalable, and representative dataset of intuitionistic propositional logic theorems in Lean. We demonstrate that leveraging information from failed search states and backtracking not only teaches models how to backtrack effectively, but also helps in developing better tactics than those generated by models trained without access to backtracking insights.We release our datasets on GitHub and Huggingface.", "images": ["2404_07382v3_1"], "tokens": 963}]}
{"id": 60096, "dataset": "arxiv", "images": ["2312_04512v2_0", "2312_04512v2_1", "2312_04512v2_2"], "chunks": [{"chunk_id": 1, "text": "In this paper, we shed light on smart contract fuzzing by employing a sequence-aware mutation and seed mask guidance strategy. In particular, we first utilize data-flow-based feedback to determine transaction orders in a meaningful way and further introduce a sequence-aware mutation technique to explore deeper states. Thereafter, we design a mask-guided seed mutation strategy that biases the generated transaction inputs to hit target branches. In addition, we develop a dynamic-adaptive energy adjustment paradigm that balances the fuzzing resource allocation during a fuzzing campaign. <PIC>. A high-level architecture and analysis pipeline of MuFuzz. We design and implement MuFuzz, a novel fuzzing framework for smart contracts, which consists of a sequence-aware mutation module, a seed mask guidance module, and a dynamic energy adjustment module. In summary, we make the following key contributions: \u2022 We design a sequence-aware mutation and seed mask guidance strategy for blockchain smart contract fuzzing. \u2022 We propose a new fuzzing framework for smart contracts that consists of three key components: a sequence-aware mutation, a mask-guided seed mutation, and a dynamic adaptive energy adjustment, which increases the probabil ity of exploring deep contract states.", "images": ["2312_04512v2_0"], "tokens": 299}, {"chunk_id": 2, "text": "The proposed mod ules hold the potential to be transferable to the fuzzing of smart contracts on alternative blockchain platforms. \u2022 We implement MuFuzz1 and conduct extensive exper iments on three benchmarks. Not only does MuFuzz outperform state-of-the-art smart contract fuzzers in terms of both coverage and runtime, but it is also able to detect more vulnerabilities than existing bug detection tools. \u2022 We release both our system and benchmarks to facilitate future research, in the spirit of open science. II. BACKGROUND A. Ethereum Smart Contract The notion of smart contracts was originally put forth in 1994 by Nick Szabo [42], who described the concept of a trustless system containing self-executing computer programs. However, the concept did not become a reality until the emergence of the Ethereum in 2014 [43]. Ethereum is currently the most popular blockchain, which employs the Ethereum Virtual Machine (EVM) to execute smart contracts. Smart contracts maintain Storage, which is organized as a key value store to persistently hold state variables [18]. Since smart contracts are stored on the blockchain, they inherit certain properties. The immutability of the blockchain ensures that the dapp/program execution strictly adheres to the rules defined in the smart contract, which are unalterable once deployed on chain. Attributed to the decentralized nature, smart contracts allow transactions to transpire between anonymous parties without relying on a trusted third party. To date, more than 60 million smart contracts have been created on Ethereum [4], giving rise to a variety of decentralized applications, such as decentralized finance (DeFi) [44], non-fungible tokens (NFT) [45], Internet of Things (IoT) [46], healthcare [47], crowdsale [48], and many others [1], [2], [49]. B. Smart Contract Vulnerability Smart contracts on the Ethereum blockchain have been subject to numerous destructive attacks.The most notable ones include the DAO attack in 2016 [50], the Parity Multisig Wallet attack in 2017 [5], the Beauty Chain attack in 2018 [51], the Cream Finance attack in 2021 [9], and the Rari Fuse Pool attack in 2022 [52], which together resulted in huge financial losses. Prior works such as [53], [18] have studied and defined various defects in Ethereum smart contracts. Here, we mainly focus on detecting nine types of vulnerabilities, which are summarized in Table I. The reasons for considering these bug classes are: (1) A large body of previous research has demonstrated that these vulnerabilities account for a significant portion of the existing bugs in the Ethereum ecosystem [12], [13]. (2) Over 90% of the financial losses in Ethereum smart contracts are caused by these vulnerabilities [6], [30], [54]. (3) We have investigated the bug classes handled by existing smart contract security tools and found that these vulnerabilities are of the highest concern [18], [15], [19], [33]. <PIC> shows the experimental results, where each bar represents the percentage of achieved coverage or detected vulnerabilities compared to the results when all three com ponents were enabled (i.e., the gray bar in the back). It can be seen from the figure that each component is essential to the performance improvement of MuFuzz. In particular, we can observe that generating a meaningful transaction sequence plays the most critical role in achieving branch coverage and detecting more vulnerabilities. Quantitatively, without the three components, the achieved coverage decreases by 18%, 9%, 10% on small contracts and 26%, 19%, 25% on large contracts, respectively. Moreover, MuFuzz can discover 14%, 6%, 11% more bugs on small contracts and 27%, 22%, 24% more bugs on large contracts than without the three components, respectively. E. Real-World Case Study (Answering RQ4) We also pay attention to the scalability and practicality of MuFuzz. We randomly select 100 contracts from D3, where each contract contains more than 30,000 transactions in Ethereum.", "images": ["2312_04512v2_1"], "tokens": 940}, {"chunk_id": 3, "text": "We manually check the bug detection results and classify them into true positives (TP) and false positives (FP). In addition, we present the overall branch coverage (i.e., the average of the 100 contract runs) of MuFuzz. Table IV summarizes the experimental results. From the table, we can see that: (1) MuFuzz reports a total of 86 bug alarms. Out of the 100 contracts, 39 contracts are flagged as having at least one of these alarms. We manually verify the alarms and con firm that 94% of them are true positives. (2) MuFuzz achieves an average coverage of 80.71% on the 100 contracts, showing inspiring practicality in testing real-world large contracts. Case Study. To provide further insight on how MuFuzz achieves high coverage and detects bugs, we illustrate the fuzzing strategy of MuFuzz by through its workflow. <PIC> shows a contract where MuFuzz achieves 100% coverage while sFuzz and ConFuzzius achieve only 50%. Function withdraw has an if condition at line 30. If the condition cannot be satisfied, the bug inside the condition cannot be exposed. To cover line 31, one must call function invest twice to set the phase = 1. sFuzz and ConFuzzius fail to cover line 31 because they cannot generate a sequence that runs invest twice. MuFuzz, instead, incorporates a sequence aware mutation strategy to create a sequence that can handle such conditions. Particularly, MuFuzz works in four main steps: 1) MuFuzz parses a contract source code into bytecode, ABI, and AST. By analyzing the ABI and AST, MuFuzz identifies which state variables are defined. 2) MuFuzz derives the dataflow dependencies of state variables and formulates a sequence of transactions. 3) MuFuzz determines the test inputs for each transaction in the sequence and activates the mutation masking strategy to evolve the seed inputs.4) MuFuzz analyzes the fuzzing log and identifies if there is a vulnerability by matching it against the bug oracles. The vulnerability in the crowdsale case can be exposed by MuFuzz with a sequence of transactions:", "images": ["2312_04512v2_2"], "tokens": 508}]}
{"id": 60098, "dataset": "arxiv", "images": ["2202_10938v1_0", "2202_10938v1_1"], "chunks": [{"chunk_id": 1, "text": "One of the most critical problems in BCFL is the resource allocation on local devices. Firstly, local devices with heterogeneous computational power usually have their own tasks to finish, so a universal resource allocation scheme for all the mobile devices is not practical. In addition, the whole system may not work effectively and sustainably if there are no reasonable rewards allocated to clients. Furthermore, both training and mining in the framework of BCFL consume significant amount of resources and time, and thus it is difficult for clients to appropriately allocate their limited resources to ensure the performance of the global model during the required time period. Lastly, since the system may not be aware of the amount of training data that each client owns, it can be challenging for the model owner (MO), i.e., the BCFL task publisher, to make proper decisions regarding the reward distribution. Inspired by [20], we consider a fully coupled BCFL system, which runs FL on a consortium blockchain network. In such a decentralized BCFL system, the participants in FL work as the blockchain nodes as well. Specifically, there are multiple local devices, termed as clients, working collaboratively to train a machine learning model, i.e., the global model.The set of clients can be denoted as N = {1, \u00b7 \u00b7 \u00b7, i, \u00b7 \u00b7 \u00b7, N} with N representing the total number of clients in the BCFL system. For simplicity, we refer to the work done by the blockchain for FL as mining in a uniform way, which does not imply that clients perform mining jobs consuming excessive amount of computing power like Proof of Work (PoW) [21]. In our system, we consider that lightweight consensus algorithms, such as Practical Byzantine Fault Tolerance (PBFT) [22] and Delegated Proof of Stake (DPoS) [23], are utilized in the consortium blockchain system. In our considered BCFL system, each client should be responsible for both training and mining. Since the workflow of the fully coupled BCFL is that mining starts only after the training is completed, we assume that training and mining are not parallel in this paper. Once the training is finished, all the clients will upload their local model updates to the blockchain network so as to be recorded on the blockchain. Here we define one round of BCFL as finishing both the training and mining processes. Since mobile devices usually have their own tasks to finish rather than only contributing to BCFL, we assume that they will not use all of their computation resources. In other words, CPU cycles per second for training and mining can be adjusted strategically in each round of BCFL. The MO is the requester of the FL task, aiming at to receive a well-trained final global model from the BCFL system. After the FL task is published on the blockchain, clients start to train their local models and then broadcast the obtained model updates to the blockchain network once the local training process is finished. By this means, the MO can only access the model updates from all clients rather than raw data of devices, thus preventing the leakage of private information for participants. An illustration of our system model is shown in <PIC>. The detailed workflow to finish one BCFL task is as below: \u2022 The MO publishes an FL task, with the rewards for training and mining. \u2022 Clients determine the computational resources, i.e.", "images": ["2202_10938v1_0"], "tokens": 757}, {"chunk_id": 2, "text": ", CPU cycles per second, used to train the model and mine for the blockchain based on the rewards provided by the MO. \u2022 Each client trains the local model, and then broadcasts the model updates to the blockchain network. Then, clients start to mine the block. \u2022 Once the block is generated, the model updates are stored on the blockchain, and the rewards will be delivered to each client. \u2022 Clients calculate the global model with the verified model updates on chain. As long as the expected performance of the global modal is not reached, clients will start the next round of training based on the aggregated global model. Before we formulate the game model, we should clarify the fair reward allocation scheme first. In our system, we consider that each client has an equal chance to participate in both the training and mining processes with fair rewards. And since the allocation of rewards to each client in training and mining has a significant impact on the system fairness and further the participation willingness, we need to design a fair reward allocation scheme. Although we have already defined the payoff of each client during the training and mining processes in the above section, it is necessary to investigate their upper bounds based on the MO\u2019s rewards budget. And the rewards distribution should not only be associated with the computing power of the device, but also take into account the performance of its work. On the one hand, the reward budget of the MO and the rewards that each client can get are limited; on the other hand, if the resources are allocated only based on the computing power devoted, it could lead to the situation where devices with sufficient computing power take most of the rewards, while devices with less power cannot get enough rewards, making the system unstable and unsustainable. In this section, we will conduct numerical experiments to verify and support our designed mechanism. We first clarify the experimental settings and then illustrate the results.We implement the simulations using Matlab 2019b in macOS 11.0.1 running on Intel i7 processor with 32 GB RAM and 1 TB SSD. First, we prove the correctness of the optimal strategies derived from our models. We assume there are 50 clients in total and each client has the same ata size, so we set \u03bci = 10. In our experiments, for clients and the MO, there are four strategy combinations, i.e., both sides choose the optimal strategies, one chooses the random strategies while the other chooses optimal strategies, and both choose the random strategies. For example, we define the strategy combination Random vs. Optimal as the clients choose the random strategies and the MO chooses the optimal strategy. We compare the utilities of clients and the MO with random strategies and optimal strategies, respectively. The results in <PIC> illustrate that clients and the MO can obtain the higher utilities than all other strategies when they both choose the optimal strategies, proving the validity of our proposed optimal strategies.", "images": ["2202_10938v1_1"], "tokens": 655}]}
{"id": 60099, "dataset": "arxiv", "images": ["2403_14280v4_0", "2403_14280v4_1"], "chunks": [{"chunk_id": 1, "text": "Abstract Large Language Models (LLMs) have emerged as powerful tools across var ious domains within cyber security. Notably, recent studies are increasingly exploring LLMs applied to the context of blockchain security (BS). How ever, there remains a gap in a comprehensive understanding regarding the full scope of applications, impacts, and potential constraints of LLMs on blockchain security. To fill this gap, we undertake a literature review focus ing on the studies that apply LLMs in blockchain security (LLM4BS). Our study aims to comprehensively analyze and understand existing re search, and elucidate how LLMs contribute to enhancing the security of blockchain systems. Through a thorough examination of existing literature, we delve into the integration of LLMs into various aspects of blockchain secu rity. We explore the mechanisms through which LLMs can bolster blockchain security, including their applications in smart contract auditing, transaction anomaly detection, vulnerability repair, program analysis of smart contracts, and serving as participants in the cryptocurrency community. Furthermore, we assess the challenges and limitations associated with leveraging LLMs for enhancing blockchain security, considering factors such as scalability, pri vacy concerns, and ethical concerns.Our thorough review sheds light on the opportunities and potential risks of tasks on LLM4BS, providing valuable insights for researchers, practitioners, and policymakers alike. In this section, we introduce a thematic taxonomy devised to systematically categorize the body of literature about tasks associated with large language models for blockchain security (LLM4BS), emphasizing the function of the LLM within these contexts. <PIC> depicts the five applications of LLM4BS task, involving code audit of smart contracts \u00a73.1, analysis of abnormal transactions \u00a73.2, dynamic analysis of smart contracts \u00a73.3, development of smart contracts \u00a73.4, participants of cryptocurrency communi", "images": ["2403_14280v4_0"], "tokens": 430}, {"chunk_id": 2, "text": "ty \u00a73.5, and other potential directions \u00a73.6. Governance emerges as a major theme, as LLMs could contribute signifi cantly to the structuring and transparency of this largely unregulated space. The first document outlines the broader governance challenges faced by AI systems, suggesting blockchain as a viable solution to introduce verifiability and accountability. On the other hand, the limitations of LLMs in capturing the complexities of legal reasoning are highlighted, a concern that is echoed across the three studies to varying degrees. The practical applications of these models in legal settings, specifically detailed in the second and third documents, emphasize their innovative role in drafting legal complaints. This development is promising for the future of legal work related to cryptocurrency regulations and litigation, as it sug gests that LLMs could alleviate some of the workload from human experts, although the need for human oversight remains. While governance and legal assistance dominate the discourse, there\u2019s a tone of cautious optimism throughout the texts. There is recognition of the transformative potential of LLMs in the cryptocurrency sector, but also a clear acknowledgment of the need for further advancement in AI technology to fully integrate into complex decision-making processes where legal and ethical considerations are paramount.In essence, the collective narrative from the three documents converges on the premise that LLMs hold transformative potential for the cryptocurrency community\u2019s governance and legal sectors but must overcome challenges in understanding before they can be fully trusted in autonomous roles. 3.6. Miscellaneous As displayed in Table 7, LLM is also used in other blockchain security fields, involving smart contract compilers [57], zero-knowledge proofs [58], model training [20, 59], NFT generation [60]. We will introduce their appli cations in detail in the future. 4. Case study of LLM4BS In this section, we engage in an in-depth examination through three dis tinct case studies, each serving to illustrate and shed light on the diverse and concrete applications of Large Language Models for Blockchain Systems (LLM4BS). These cases in Table.8 have been meticulously selected to encom pass a broad range of scenarios, comprising LLM4FUZZ [44] \u00a74.1, SMART INV [29] \u00a74.2, BLOCKGPT [26] \u00a74.3. LLM4FUZZ emerges as an innovative technique in the cybersecurity landscape, specifically in the niche of smart contract security within blockchain networks. It intricately combines the prowess of Large Language Models (LLMs) with fuzz testing methodologies to proactively unearth vulnerabilities that could potentially compromise the integrity of smart contracts. LLMs are highly sophisticated AI models that have made significant strides in understanding and generating human-like text, and more recently, they have proven to be adept at comprehending programming languages and code structure. LLM4FUZZ exploits this capacity by deploying LLMs to guide fuzzing processes intelligently. This results in a more incisive and nuanced exploration of smart contracts, focusing testing efforts on areas that LLMs determine to be most likely to contain security flaws.By doing so, LLM4FUZZ succeeds in not only streamlining the anomaly detection process but also in enhancing its accuracy and depth. <PIC>: The architecture of LLM4FUZZ. In the world of blockchain technology, where smart contracts serve as immutable agreements that execute automatically based on coded conditions, the potential negative impact of a security breach is heightened. Smart contracts control significant digital assets and are essential to the functioning of distributed applications (dApps). The immutable nature of blockchain adds a layer of complexity as deployed smart contracts, once committed to the blockchain, cannot be altered. Therefore, preemptive security assurances become crucial to ensuring their reliability and safeguarding theThrough its lens, we catch a glimpse of the future of smart contract security \u2013 a future where AI-driven tools not only anticipate but actively engage in the continuous battle against cyber threats.", "images": ["2403_14280v4_1"], "tokens": 958}]}
